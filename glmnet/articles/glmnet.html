<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>An Introduction to `glmnet` • glmnet</title>
<!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js" integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8=" crossorigin="anonymous"></script><!-- Bootstrap --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha256-916EbMg70RQy9LHiGkXzG8hSg9EdNy97GazNG/aiY1w=" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha256-U5ZEeKfGNOja007MMD3YBI0A3OSZOQbeG6z2f2Y0hu8=" crossorigin="anonymous"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.7.1/css/all.min.css" integrity="sha256-nAmazAk6vS34Xqo0BSrTb+abbtFlgsFK7NKSi6o7Y78=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.7.1/css/v4-shims.min.css" integrity="sha256-6qHlizsOWFskGlwVOKuns+D1nB6ssZrHQrNj1wGplHc=" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.4/clipboard.min.js" integrity="sha256-FiZwavyI2V6+EXO1U+xzLG3IKldpiTFf3153ea9zikQ=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.9.4/headroom.min.js" integrity="sha256-DJFC1kqIhelURkuza0AvYal5RxMtpzLjFhsnVIeuk+U=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.9.4/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../pkgdown.js"></script><meta property="og:title" content="An Introduction to `glmnet`">
<meta property="og:description" content="">
<meta name="twitter:card" content="summary">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <div class="container template-article">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">glmnet</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="Released version">3.0.1</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../index.html">
    <span class="fas fa fas fa-home fa-lg"></span>
     
  </a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    <span class="fas fa fas fa-archive fa-lg"></span>
     
    R Packages
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../../boot/index.html">boot</a>
    </li>
    <li>
      <a href="../../broom/index.html">broom</a>
    </li>
    <li>
      <a href="../../caret/index.html">caret</a>
    </li>
    <li>
      <a href="../../cluster/index.html">cluster</a>
    </li>
    <li>
      <a href="../../coefplot/index.html">coefplot</a>
    </li>
    <li>
      <a href="../../data.table/index.html">data.table</a>
    </li>
    <li>
      <a href="../../devtools/index.html">devtools</a>
    </li>
    <li>
      <a href="../../dplyr/index.html">dplyr</a>
    </li>
    <li>
      <a href="../../e1071/index.html">e1071</a>
    </li>
    <li>
      <a href="../../forcats/index.html">forcats</a>
    </li>
    <li>
      <a href="../../gbm/index.html">gbm</a>
    </li>
    <li>
      <a href="../../ggplot2/index.html">ggplot2</a>
    </li>
    <li>
      <a href="../../glmnet/index.html">glmnet</a>
    </li>
    <li>
      <a href="../../gridExtra/index.html">gridExtra</a>
    </li>
    <li>
      <a href="../../ISLR/index.html">ISLR</a>
    </li>
    <li>
      <a href="../../MASS/index.html">MASS</a>
    </li>
    <li>
      <a href="../../pdp/index.html">pdp</a>
    </li>
    <li>
      <a href="../../pls/index.html">pls</a>
    </li>
    <li>
      <a href="../../plyr/index.html">plyr</a>
    </li>
    <li>
      <a href="../../pROC/index.html">pROC</a>
    </li>
    <li>
      <a href="../../purrr/index.html">purrr</a>
    </li>
    <li>
      <a href="../../randomForest/index.html">randomForest</a>
    </li>
    <li>
      <a href="../../readr/index.html">readr</a>
    </li>
    <li>
      <a href="../../rpart/index.html">rpart</a>
    </li>
    <li>
      <a href="../../rpart.plot/index.html">rpart.plot</a>
    </li>
    <li>
      <a href="../../stringr/index.html">stringr</a>
    </li>
    <li>
      <a href="../../tibble/index.html">tibble</a>
    </li>
    <li>
      <a href="../../tidyr/index.html">tidyr</a>
    </li>
    <li>
      <a href="../../xgboost/index.html">xgboost</a>
    </li>
  </ul>
</li>
<li>
  <a href="../articles/glmnet.html">Get started</a>
</li>
<li>
  <a href="../reference/index.html">Reference</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../articles/Coxnet.html">Coxnet: Regularized Cox Regression</a>
    </li>
    <li>
      <a href="../articles/relax.html">Relaxed fits and other additions in `glmnet` 3.0</a>
    </li>
  </ul>
</li>
<li>
  <a href="../news/index.html">Changelog</a>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right"></ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      

      </header><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1>An Introduction to <code>glmnet</code>
</h1>
                        <h4 class="author">Trevor Hastie and Junyang Qian</h4>
            
            <h4 class="date">September 13, 2016</h4>
      
      
      <div class="hidden name"><code>glmnet.Rmd</code></div>

    </div>

    
    
<div id="introduction" class="section level2">
<h2 class="hasAnchor">
<a href="#introduction" class="anchor"></a>Introduction</h2>
<p>Glmnet is a package that fits a generalized linear model via penalized maximum likelihood. The regularization path is computed for the lasso or elasticnet penalty at a grid of values for the regularization parameter lambda. The algorithm is extremely fast, and can exploit sparsity in the input matrix <code>x</code>. It fits linear, logistic and multinomial, poisson, and Cox regression models. A variety of predictions can be made from the fitted models. It can also fit multi-response linear regression.</p>
<p>The authors of glmnet are Jerome Friedman, Trevor Hastie, Rob Tibshirani and Noah Simon, and the R package is maintained by Trevor Hastie. The matlab version of glmnet is maintained by Junyang Qian. This vignette describes the usage of glmnet in R. There is an additional vignette for the new <code>relaxed</code> features in <code>glmnet</code>, along with some new capabilities. There is as well a vignette devoted to Cox models in <code>glmnet</code>.</p>
<p><code>glmnet</code> solves the following problem</p>
<p><span class="math display">\[
\min_{\beta_0,\beta} \frac{1}{N} \sum_{i=1}^{N} w_i l(y_i,\beta_0+\beta^T x_i) + \lambda\left[(1-\alpha)||\beta||_2^2/2 + \alpha ||\beta||_1\right],
\]</span></p>
<p>over a grid of values of <span class="math inline">\(\lambda\)</span> covering the entire range. Here <span class="math inline">\(l(y,\eta)\)</span> is the negative log-likelihood contribution for observation <span class="math inline">\(i\)</span>; e.g. for the Gaussian case it is <span class="math inline">\(\frac{1}{2}(y-\eta)^2\)</span>. The <em>elastic-net</em> penalty is controlled by <span class="math inline">\(\alpha\)</span>, and bridges the gap between lasso (<span class="math inline">\(\alpha=1\)</span>, the default) and ridge (<span class="math inline">\(\alpha=0\)</span>). The tuning parameter <span class="math inline">\(\lambda\)</span> controls the overall strength of the penalty.</p>
<p>It is known that the ridge penalty shrinks the coefficients of correlated predictors towards each other while the lasso tends to pick one of them and discard the others. The elastic-net penalty mixes these two; if predictors are correlated in groups, an <span class="math inline">\(\alpha=0.5\)</span> tends to select the groups in or out together. This is a higher level parameter, and users might pick a value upfront, else experiment with a few different values. One use of <span class="math inline">\(\alpha\)</span> is for numerical stability; for example, the elastic net with <span class="math inline">\(\alpha = 1 - \epsilon\)</span> for some small <span class="math inline">\(\epsilon &gt; 0\)</span> performs much like the lasso, but removes any degeneracies and wild behavior caused by extreme correlations.</p>
<p>The <code>glmnet</code> algorithms use cyclical coordinate descent, which successively optimizes the objective function over each parameter with others fixed, and cycles repeatedly until convergence. The package also makes use of the strong rules for efficient restriction of the active set. Due to highly efficient updates and techniques such as warm starts and active-set convergence, our algorithms can compute the solution path very fast.</p>
<p>The code can handle sparse input-matrix formats, as well as range constraints on coefficients. The core of <code>glmnet</code> is a set of fortran subroutines, which make for very fast execution.</p>
<p>The package also includes methods for prediction and plotting, and a function that performs K-fold cross-validation.</p>
<p>The theory and algorithms in this implementation are described in <span class="citation">Friedman, Hastie, and Tibshirani (<a href="#ref-glmnet">2010</a>)</span>, <span class="citation">Simon et al. (<a href="#ref-coxnet">2011</a>)</span>, <span class="citation">Tibshirani et al. (<a href="#ref-strongrules">2012</a>)</span> and <span class="citation">Simon, Friedman, and Hastie (<a href="#ref-block">2013</a>)</span> .</p>
</div>
<div id="installation" class="section level2">
<h2 class="hasAnchor">
<a href="#installation" class="anchor"></a>Installation</h2>
<p>Like many other R packages, the simplest way to obtain <code>glmnet</code> is to install it directly from CRAN. Type the following command in R console:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1-1" data-line-number="1"><span class="kw"><a href="https://rdrr.io/r/utils/install.packages.html">install.packages</a></span>(<span class="st">"glmnet"</span>, <span class="dt">repos =</span> <span class="st">"https://cran.us.r-project.org"</span>)</a></code></pre></div>
<p>Users may change the <code>repos</code> options depending on their locations and preferences. Other options such as the directories where to install the packages can be altered in the command. For more details, see <code><a href="https://rdrr.io/r/utils/help.html">help(install.packages)</a></code>.</p>
<p>Here the R package has been downloaded and installed to the default directories.</p>
<p>Alternatively, users can download the package source from <a href="https://cran.r-project.org/package=glmnet">CRAN</a> and type Unix commands to install it to the desired location.</p>
</div>
<div id="quick-start" class="section level2">
<h2 class="hasAnchor">
<a href="#quick-start" class="anchor"></a>Quick Start</h2>
<p>The purpose of this section is to give users a general sense of the package, including the components, what they do and some basic usage. We will briefly go over the main functions, see the basic operations and have a look at the outputs. Users may have a better idea after this section what functions are available, which one to choose, or at least where to seek help. More details are given in later sections.</p>
<p>First, we load the <code>glmnet</code> package:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2-1" data-line-number="1"><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span>(glmnet)</a></code></pre></div>
<pre><code>## Loading required package: Matrix</code></pre>
<pre><code>## Loaded glmnet 3.0-1</code></pre>
<p>The default model used in the package is the Guassian linear model or “least squares”, which we will demonstrate in this section. We load a set of data created beforehand for illustration. Users can either load their own data or use those saved in the workspace.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb5-1" data-line-number="1"><span class="kw"><a href="https://rdrr.io/r/utils/data.html">data</a></span>(QuickStartExample)</a></code></pre></div>
<p>The command loads an input matrix <code>x</code> and a response vector <code>y</code> from this saved R data archive.</p>
<p>We fit the model using the most basic call to <code>glmnet</code>.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb6-1" data-line-number="1">fit =<span class="st"> </span><span class="kw"><a href="../reference/glmnet.html">glmnet</a></span>(x, y)</a></code></pre></div>
<p>“fit” is an object of class <code>glmnet</code> that contains all the relevant information of the fitted model for further use. We do not encourage users to extract the components directly. Instead, various methods are provided for the object such as <code>plot</code>, <code>print</code>, <code>coef</code> and <code>predict</code> that enable us to execute those tasks more elegantly.</p>
<p>We can visualize the coefficients by executing the <code>plot</code> function:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb7-1" data-line-number="1"><span class="kw"><a href="https://rdrr.io/r/graphics/plot.html">plot</a></span>(fit)</a></code></pre></div>
<p><img src="glmnet_files/figure-html/unnamed-chunk-5-1.png" width="700"></p>
<p>Each curve corresponds to a variable. It shows the path of its coefficient against the <span class="math inline">\(\ell_1\)</span>-norm of the whole coefficient vector at as <span class="math inline">\(\lambda\)</span> varies. The axis above indicates the number of nonzero coefficients at the current <span class="math inline">\(\lambda\)</span>, which is the effective degrees of freedom (<em>df</em>) for the lasso. Users may also wish to annotate the curves; this can be done by setting <code>label = TRUE</code> in the plot command.</p>
<p>A summary of the <code>glmnet</code> path at each step is displayed if we just enter the object name or use the <code>print</code> function:</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb8-1" data-line-number="1"><span class="kw"><a href="https://rdrr.io/r/base/print.html">print</a></span>(fit)</a></code></pre></div>
<pre><code>## 
## Call:  glmnet(x = x, y = y) 
## 
##    Df    %Dev  Lambda
## 1   0 0.00000 1.63100
## 2   2 0.05528 1.48600
## 3   2 0.14590 1.35400
## 4   2 0.22110 1.23400
## 5   2 0.28360 1.12400
## 6   2 0.33540 1.02400
## 7   4 0.39040 0.93320
## 8   5 0.45600 0.85030
## 9   5 0.51540 0.77470
## 10  6 0.57350 0.70590
## 11  6 0.62550 0.64320
## 12  6 0.66870 0.58610
## 13  6 0.70460 0.53400
## 14  6 0.73440 0.48660
## 15  7 0.76210 0.44330
## 16  7 0.78570 0.40400
## 17  7 0.80530 0.36810
## 18  7 0.82150 0.33540
## 19  7 0.83500 0.30560
## 20  7 0.84620 0.27840
## 21  7 0.85550 0.25370
## 22  7 0.86330 0.23120
## 23  8 0.87060 0.21060
## 24  8 0.87690 0.19190
## 25  8 0.88210 0.17490
## 26  8 0.88650 0.15930
## 27  8 0.89010 0.14520
## 28  8 0.89310 0.13230
## 29  8 0.89560 0.12050
## 30  8 0.89760 0.10980
## 31  9 0.89940 0.10010
## 32  9 0.90100 0.09117
## 33  9 0.90230 0.08307
## 34  9 0.90340 0.07569
## 35 10 0.90430 0.06897
## 36 11 0.90530 0.06284
## 37 11 0.90620 0.05726
## 38 12 0.90700 0.05217
## 39 15 0.90780 0.04754
## 40 16 0.90860 0.04331
## 41 16 0.90930 0.03947
## 42 16 0.90980 0.03596
## 43 17 0.91030 0.03277
## 44 17 0.91070 0.02985
## 45 18 0.91110 0.02720
## 46 18 0.91140 0.02479
## 47 19 0.91170 0.02258
## 48 19 0.91200 0.02058
## 49 19 0.91220 0.01875
## 50 19 0.91240 0.01708
## 51 19 0.91250 0.01557
## 52 19 0.91260 0.01418
## 53 19 0.91270 0.01292
## 54 19 0.91280 0.01178
## 55 19 0.91290 0.01073
## 56 19 0.91290 0.00978
## 57 19 0.91300 0.00891
## 58 19 0.91300 0.00812
## 59 19 0.91310 0.00740
## 60 19 0.91310 0.00674
## 61 19 0.91310 0.00614
## 62 20 0.91310 0.00559
## 63 20 0.91310 0.00510
## 64 20 0.91310 0.00464
## 65 20 0.91320 0.00423
## 66 20 0.91320 0.00386
## 67 20 0.91320 0.00351</code></pre>
<p>It shows from left to right the number of nonzero coefficients (<code>Df</code>), the percent (of null) deviance explained (<code>%dev</code>) and the value of <span class="math inline">\(\lambda\)</span> (<code>Lambda</code>). Although by default <code>glmnet</code> calls for 100 values of <code>lambda</code> the program stops early if `%dev% does not change sufficently from one lambda to the next (typically near the end of the path.)</p>
<p>We can obtain the actual coefficients at one or more <span class="math inline">\(\lambda\)</span>’s within the range of the sequence:</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb10-1" data-line-number="1"><span class="kw"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span>(fit,<span class="dt">s=</span><span class="fl">0.1</span>)</a></code></pre></div>
<pre><code>## 21 x 1 sparse Matrix of class "dgCMatrix"
##                        1
## (Intercept)  0.150928072
## V1           1.320597195
## V2           .          
## V3           0.675110234
## V4           .          
## V5          -0.817411518
## V6           0.521436671
## V7           0.004829335
## V8           0.319415917
## V9           .          
## V10          .          
## V11          0.142498519
## V12          .          
## V13          .          
## V14         -1.059978702
## V15          .          
## V16          .          
## V17          .          
## V18          .          
## V19          .          
## V20         -1.021873704</code></pre>
<p>(why <code>s</code> and not <code>lambda</code>? In case later we want to allow one to specify the model size in other ways.) Users can also make predictions at specific <span class="math inline">\(\lambda\)</span>’s with new input data:</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb12-1" data-line-number="1"><span class="kw"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span>(<span class="dv">29</span>)</a>
<a class="sourceLine" id="cb12-2" data-line-number="2">nx =<span class="st"> </span><span class="kw"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span>(<span class="kw"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span>(<span class="dv">10</span><span class="op">*</span><span class="dv">20</span>),<span class="dv">10</span>,<span class="dv">20</span>)</a>
<a class="sourceLine" id="cb12-3" data-line-number="3"><span class="kw"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span>(fit,<span class="dt">newx=</span>nx,<span class="dt">s=</span><span class="kw"><a href="https://rdrr.io/r/base/c.html">c</a></span>(<span class="fl">0.1</span>,<span class="fl">0.05</span>))</a></code></pre></div>
<pre><code>##                1          2
##  [1,] -2.6025731 -2.7138626
##  [2,]  1.6348323  1.7919846
##  [3,] -3.1014967 -3.4777129
##  [4,] -0.3540413 -0.5789109
##  [5,] -2.3377144 -2.4553137
##  [6,]  3.7860636  4.0486134
##  [7,] -0.4757211 -0.3753130
##  [8,] -2.6844679 -2.8196771
##  [9,]  1.2276234  1.0480499
## [10,]  0.8598950  0.8757991</code></pre>
<p>The function <code>glmnet</code> returns a sequence of models for the users to choose from. In many cases, users may prefer the software to select one of them. Cross-validation is perhaps the simplest and most widely used method for that task.</p>
<p><code>cv.glmnet</code> is the main function to do cross-validation here, along with various supporting methods such as plotting and prediction. We still act on the sample data loaded before.</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb14-1" data-line-number="1">cvfit =<span class="st"> </span><span class="kw"><a href="../reference/cv.glmnet.html">cv.glmnet</a></span>(x, y)</a></code></pre></div>
<p><code>cv.glmnet</code> returns a <code>cv.glmnet</code> object, which is “cvfit” here, a list with all the ingredients of the cross-validation fit. As for <code>glmnet</code>, we do not encourage users to extract the components directly except for viewing the selected values of <span class="math inline">\(\lambda\)</span>. The package provides well-designed functions for potential tasks.</p>
<p>We can plot the object.</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb15-1" data-line-number="1"><span class="kw"><a href="https://rdrr.io/r/graphics/plot.html">plot</a></span>(cvfit)</a></code></pre></div>
<p><img src="glmnet_files/figure-html/unnamed-chunk-10-1.png" width="700"></p>
<p>It includes the cross-validation curve (red dotted line), and upper and lower standard deviation curves along the <span class="math inline">\(\lambda\)</span> sequence (error bars). Two selected <span class="math inline">\(\lambda\)</span>’s are indicated by the vertical dotted lines (see below).</p>
<p>We can view the selected <span class="math inline">\(\lambda\)</span>’s and the corresponding coefficients. For example,</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb16-1" data-line-number="1">cvfit<span class="op">$</span>lambda.min</a></code></pre></div>
<pre><code>## [1] 0.08307327</code></pre>
<p><code>lambda.min</code> is the value of <span class="math inline">\(\lambda\)</span> that gives minimum mean cross-validated error. The other <span class="math inline">\(\lambda\)</span> saved is <code>lambda.1se</code>, which gives the most regularized model such that error is within one standard error of the minimum. To use that, we only need to replace <code>lambda.min</code> with <code>lambda.1se</code> above.</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb18-1" data-line-number="1"><span class="kw"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span>(cvfit, <span class="dt">s =</span> <span class="st">"lambda.min"</span>)</a></code></pre></div>
<pre><code>## 21 x 1 sparse Matrix of class "dgCMatrix"
##                       1
## (Intercept)  0.14936467
## V1           1.32975267
## V2           .         
## V3           0.69096092
## V4           .         
## V5          -0.83122558
## V6           0.53669611
## V7           0.02005438
## V8           0.33193760
## V9           .         
## V10          .         
## V11          0.16239419
## V12          .         
## V13          .         
## V14         -1.07081121
## V15          .         
## V16          .         
## V17          .         
## V18          .         
## V19          .         
## V20         -1.04340741</code></pre>
<p>Note that the coefficients are represented in the sparse matrix format. The reason is that the solutions along the regularization path are often sparse, and hence it is more efficient in time and space to use a sparse format. If you prefer non-sparse format, pipe the output through <code><a href="https://rdrr.io/r/base/matrix.html">as.matrix()</a></code>.</p>
<p>Predictions can be made based on the fitted <code>cv.glmnet</code> object. Let’s see a toy example.</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb20-1" data-line-number="1"><span class="kw"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span>(cvfit, <span class="dt">newx =</span> x[<span class="dv">1</span><span class="op">:</span><span class="dv">5</span>,], <span class="dt">s =</span> <span class="st">"lambda.min"</span>)</a></code></pre></div>
<pre><code>##               1
## [1,] -1.3647490
## [2,]  2.5686013
## [3,]  0.5705879
## [4,]  1.9682289
## [5,]  1.4964211</code></pre>
<p><code>newx</code> is for the new input matrix and <code>s</code>, as before, is the value(s) of <span class="math inline">\(\lambda\)</span> at which predictions are made.</p>
<p>That is the end of <code>glmnet</code> 101. With the tools introduced so far, users are able to fit the entire elastic net family, including ridge regression, using squared-error loss. In the package, there are many more options that give users a great deal of flexibility. To learn more, move on to later sections.</p>
</div>
<div id="linear-regression" class="section level2">
<h2 class="hasAnchor">
<a href="#linear-regression" class="anchor"></a>Linear Regression</h2>
<p>Linear regression here refers to two families of models. One is <code>gaussian</code>, the Gaussian family, and the other is <code>mgaussian</code>, the multiresponse Gaussian family. We first discuss the ordinary Gaussian and the multiresponse one after that.</p>
<div id="gaussian-family" class="section level3">
<h3 class="hasAnchor">
<a href="#gaussian-family" class="anchor"></a>Gaussian Family</h3>
<p><code>gaussian</code> is the default family option in the function <code>glmnet</code>. Suppose we have observations <span class="math inline">\(x_i \in \mathbb{R}^p\)</span> and the responses <span class="math inline">\(y_i \in \mathbb{R}, i = 1, \ldots, N\)</span>. The objective function for the Gaussian family is <span class="math display">\[
\min_{(\beta_0, \beta) \in \mathbb{R}^{p+1}}\frac{1}{2N} \sum_{i=1}^N (y_i -\beta_0-x_i^T \beta)^2+\lambda \left[ (1-\alpha)||\beta||_2^2/2 + \alpha||\beta||_1\right],
\]</span> where <span class="math inline">\(\lambda \geq 0\)</span> is a complexity parameter and <span class="math inline">\(0 \leq \alpha \leq 1\)</span> is a compromise between ridge (<span class="math inline">\(\alpha = 0\)</span>) and lasso (<span class="math inline">\(\alpha = 1\)</span>).</p>
<p>Coordinate descent is applied to solve the problem. Specifically, suppose we have current estimates <span class="math inline">\(\tilde{\beta_0}\)</span> and <span class="math inline">\(\tilde{\beta}_\ell\)</span> <span class="math inline">\(\forall j\in 1,]\ldots,p\)</span>. By computing the gradient at <span class="math inline">\(\beta_j = \tilde{\beta}_j\)</span> and simple calculus, the update is <span class="math display">\[
\tilde{\beta}_j \leftarrow \frac{S(\frac{1}{N}\sum_{i=1}^N x_{ij}(y_i-\tilde{y}_i^{(j)}),\lambda \alpha)}{1+\lambda(1-\alpha)},
\]</span> where <span class="math inline">\(\tilde{y}_i^{(j)} = \tilde{\beta}_0 + \sum_{\ell \neq j} x_{i\ell} \tilde{\beta}_\ell\)</span>, and <span class="math inline">\(S(z, \gamma)\)</span> is the soft-thresholding operator with value <span class="math inline">\(\text{sign}(z)(|z|-\gamma)_+\)</span>.</p>
<p>This formula above applies when the <code>x</code> variables are standardized to have unit variance (the default); it is slightly more complicated when they are not. Note that for “family=gaussian”, <code>glmnet</code> standardizes <span class="math inline">\(y\)</span> to have unit variance before computing its lambda sequence (and then unstandardizes the resulting coefficients); if you wish to reproduce/compare results with other software, best to supply a standardized <span class="math inline">\(y\)</span> first (Using the “1/N” variance formula).</p>
<p><code>glmnet</code> provides various options for users to customize the fit. We introduce some commonly used options here and they can be specified in the <code>glmnet</code> function.</p>
<ul>
<li><p><code>alpha</code> is for the elastic-net mixing parameter <span class="math inline">\(\alpha\)</span>, with range <span class="math inline">\(\alpha \in [0,1]\)</span>. <span class="math inline">\(\alpha = 1\)</span> is the lasso (default) and <span class="math inline">\(\alpha = 0\)</span> is the ridge.</p></li>
<li><p><code>weights</code> is for the observation weights. Default is 1 for each observation. (Note: <code>glmnet</code> rescales the weights to sum to N, the sample size.)</p></li>
<li><p><code>nlambda</code> is the number of <span class="math inline">\(\lambda\)</span> values in the sequence. Default is 100.</p></li>
<li><p><code>lambda</code> can be provided, but is typically not and the program constructs a sequence. When automatically generated, the <span class="math inline">\(\lambda\)</span> sequence is determined by <code>lambda.max</code> and <code>lambda.min.ratio</code>. The latter is the ratio of smallest value of the generated <span class="math inline">\(\lambda\)</span> sequence (say <code>lambda.min</code>) to <code>lambda.max</code>. The program then generated <code>nlambda</code> values linear on the log scale from <code>lambda.max</code> down to <code>lambda.min</code>. <code>lambda.max</code> is not given, but easily computed from the input <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>; it is the smallest value for <code>lambda</code> such that all the coefficients are zero. For <code>alpha=0</code> (ridge) <code>lambda.max</code> would be <span class="math inline">\(\infty\)</span>; hence for this case we pick a value corresponding to a small value for <code>alpha</code> close to zero.)</p></li>
<li><p><code>standardize</code> is a logical flag for <code>x</code> variable standardization, prior to fitting the model sequence. The coefficients are always returned on the original scale. Default is <code>standardize=TRUE</code>.</p></li>
</ul>
<p>For more information, type <code><a href="https://rdrr.io/r/utils/help.html">help(glmnet)</a></code> or simply <code><a href="../reference/glmnet.html">?glmnet</a></code>.</p>
<p>As an example, we set <span class="math inline">\(\alpha = 0.2\)</span> (more like a ridge regression), and give double weights to the latter half of the observations. To avoid too long a display here, we set <code>nlambda</code> to 20. In practice, however, the number of values of <span class="math inline">\(\lambda\)</span> is recommended to be 100 (default) or more. In most cases, it does not come with extra cost because of the warm-starts used in the algorithm, and for nonlinear models leads to better convergence properties.</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb22-1" data-line-number="1">fit =<span class="st"> </span><span class="kw"><a href="../reference/glmnet.html">glmnet</a></span>(x, y, <span class="dt">alpha =</span> <span class="fl">0.2</span>, <span class="dt">weights =</span> <span class="kw"><a href="https://rdrr.io/r/base/c.html">c</a></span>(<span class="kw"><a href="https://rdrr.io/r/base/rep.html">rep</a></span>(<span class="dv">1</span>,<span class="dv">50</span>),<span class="kw"><a href="https://rdrr.io/r/base/rep.html">rep</a></span>(<span class="dv">2</span>,<span class="dv">50</span>)), <span class="dt">nlambda =</span> <span class="dv">20</span>)</a></code></pre></div>
<p>We can then print the <code>glmnet</code> object.</p>
<div class="sourceCode" id="cb23"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb23-1" data-line-number="1"><span class="kw"><a href="https://rdrr.io/r/base/print.html">print</a></span>(fit)</a></code></pre></div>
<pre><code>## 
## Call:  glmnet(x = x, y = y, weights = c(rep(1, 50), rep(2, 50)), alpha = 0.2,      nlambda = 20) 
## 
##    Df   %Dev Lambda
## 1   0 0.0000 7.9390
## 2   4 0.1789 4.8890
## 3   7 0.4445 3.0110
## 4   7 0.6567 1.8540
## 5   8 0.7850 1.1420
## 6   9 0.8539 0.7033
## 7  10 0.8867 0.4331
## 8  11 0.9025 0.2667
## 9  14 0.9101 0.1643
## 10 17 0.9138 0.1012
## 11 17 0.9154 0.0623
## 12 17 0.9160 0.0384
## 13 19 0.9163 0.0236
## 14 20 0.9164 0.0146
## 15 20 0.9164 0.0090
## 16 20 0.9165 0.0055
## 17 20 0.9165 0.0034</code></pre>
<p>This displays the call that produced the object <code>fit</code> and a three-column matrix with columns <code>Df</code> (the number of nonzero coefficients), <code>%dev</code> (the percent deviance explained) and <code>Lambda</code> (the corresponding value of <span class="math inline">\(\lambda\)</span>).</p>
<p>(Note that the <code>digits</code> option can used to specify significant digits in the printout.)</p>
<p>Here the actual number of <span class="math inline">\(\lambda\)</span>’s here is less than specified in the call. The reason lies in the stopping criteria of the algorithm. According to the default internal settings, the computations stop if either the fractional change in deviance down the path is less than <span class="math inline">\(10^{-5}\)</span> or the fraction of explained deviance reaches <span class="math inline">\(0.999\)</span>. From the last few lines , we see the fraction of deviance does not change much and therefore the computation ends when meeting the stopping criteria. We can change such internal parameters. For details, see the Appendix section or type <code><a href="https://rdrr.io/r/utils/help.html">help(glmnet.control)</a></code>.</p>
<p>We can plot the fitted object as in the previous section. There are more options in the <code>plot</code> function.</p>
<p>Users can decide what is on the X-axis. <code>xvar</code> allows three measures: “norm” for the <span class="math inline">\(\ell_1\)</span>-norm of the coefficients (default), “lambda” for the log-lambda value and “dev” for %deviance explained.</p>
<p>Users can also label the curves with variable sequence numbers simply by setting <code>label = TRUE</code>.</p>
<p>Let’s plot “fit” against the log-lambda value and with each curve labeled.</p>
<div class="sourceCode" id="cb25"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb25-1" data-line-number="1"><span class="kw"><a href="https://rdrr.io/r/graphics/plot.html">plot</a></span>(fit, <span class="dt">xvar =</span> <span class="st">"lambda"</span>, <span class="dt">label =</span> <span class="ot">TRUE</span>)</a></code></pre></div>
<p><img src="glmnet_files/figure-html/unnamed-chunk-16-1.png" width="700"></p>
<p>Now when we plot against %deviance we get a very different picture. This is percent deviance explained on the training data. What we see here is that toward the end of the path this value are not changing much, but the coefficients are “blowing up” a bit. This lets us focus attention on the parts of the fit that matter. This will especially be true for other models, such as logistic regression.</p>
<div class="sourceCode" id="cb26"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb26-1" data-line-number="1"><span class="kw"><a href="https://rdrr.io/r/graphics/plot.html">plot</a></span>(fit, <span class="dt">xvar =</span> <span class="st">"dev"</span>, <span class="dt">label =</span> <span class="ot">TRUE</span>)</a></code></pre></div>
<p><img src="glmnet_files/figure-html/unnamed-chunk-17-1.png" width="700"></p>
<p>We can extract the coefficients and make predictions at certain values of <span class="math inline">\(\lambda\)</span>. Two commonly used options are:</p>
<ul>
<li><p><code>s</code> specifies the value(s) of <span class="math inline">\(\lambda\)</span> at which extraction is made.</p></li>
<li><p><code>exact</code> indicates whether the exact values of coefficients are desired or not. That is, if <code>exact = TRUE</code>, and predictions are to be made at values of <code>s</code> not included in the original fit, these values of <code>s</code> are merged with <code>object$lambda</code>, and the model is refit before predictions are made. If <code>exact=FALSE</code> (default), then the predict function uses linear interpolation to make predictions for values of <code>s</code> that do not coincide with lambdas used in the fitting algorithm.</p></li>
</ul>
<p>A simple example is:</p>
<div class="sourceCode" id="cb27"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb27-1" data-line-number="1">fit =<span class="st"> </span><span class="kw"><a href="../reference/glmnet.html">glmnet</a></span>(x, y)</a>
<a class="sourceLine" id="cb27-2" data-line-number="2"><span class="kw"><a href="https://rdrr.io/r/base/any.html">any</a></span>(fit<span class="op">$</span>lambda <span class="op">==</span><span class="st"> </span><span class="fl">0.5</span>)</a></code></pre></div>
<pre><code>## [1] FALSE</code></pre>
<div class="sourceCode" id="cb29"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb29-1" data-line-number="1">coef.apprx =<span class="st"> </span><span class="kw"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span>(fit, <span class="dt">s =</span> <span class="fl">0.5</span>, <span class="dt">exact =</span> <span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb29-2" data-line-number="2">coef.exact =<span class="st"> </span><span class="kw"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span>(fit, <span class="dt">s =</span> <span class="fl">0.5</span>, <span class="dt">exact =</span> <span class="ot">TRUE</span>, <span class="dt">x=</span>x, <span class="dt">y=</span>y)</a>
<a class="sourceLine" id="cb29-3" data-line-number="3"><span class="kw">cbind2</span>(coef.exact, coef.apprx)</a></code></pre></div>
<pre><code>## 21 x 2 sparse Matrix of class "dgCMatrix"
##                      1          1
## (Intercept)  0.2613110  0.2613110
## V1           1.0055470  1.0055473
## V2           .          .        
## V3           0.2677140  0.2677134
## V4           .          .        
## V5          -0.4476485 -0.4476475
## V6           0.2379287  0.2379283
## V7           .          .        
## V8           .          .        
## V9           .          .        
## V10          .          .        
## V11          .          .        
## V12          .          .        
## V13          .          .        
## V14         -0.8230862 -0.8230865
## V15          .          .        
## V16          .          .        
## V17          .          .        
## V18          .          .        
## V19          .          .        
## V20         -0.5553678 -0.5553675</code></pre>
<p>The left column is for <code>exact = TRUE</code> and the right for <code>FALSE</code>. We see from the above that 0.5 is not in the sequence and that hence there are some difference, though not much. Linear interpolation is mostly enough if there are no special requirements. Notice that with <code>exact=TRUE</code> we have to supply by named argument any data that was used in creating the original fit, in this case <code>x</code> and <code>y</code>.</p>
<p>Users can make predictions from the fitted object. In addition to the options in <code>coef</code>, the primary argument is <code>newx</code>, a matrix of new values for <code>x</code>. The <code>type</code> option allows users to choose the type of prediction: * “link” gives the fitted values</p>
<ul>
<li><p>“response” the sames as “link” for “gaussian” family.</p></li>
<li><p>“coefficients” computes the coefficients at values of <code>s</code></p></li>
<li><p>“nonzero” retuns a list of the indices of the nonzero coefficients for each value of <code>s</code>.</p></li>
</ul>
<p>For example,</p>
<div class="sourceCode" id="cb31"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb31-1" data-line-number="1"><span class="kw"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span>(fit, <span class="dt">newx =</span> x[<span class="dv">1</span><span class="op">:</span><span class="dv">5</span>,], <span class="dt">type =</span> <span class="st">"response"</span>, <span class="dt">s =</span> <span class="fl">0.05</span>)</a></code></pre></div>
<pre><code>##               1
## [1,] -1.3362652
## [2,]  2.5894245
## [3,]  0.5872868
## [4,]  2.0977222
## [5,]  1.6436280</code></pre>
<p>gives the fitted values for the first 5 observations at <span class="math inline">\(\lambda = 0.05\)</span>. If multiple values of <code>s</code> are supplied, a matrix of predictions is produced.</p>
<p>Users can customize K-fold cross-validation. In addition to all the <code>glmnet</code> parameters, <code>cv.glmnet</code> has its special parameters including <code>nfolds</code> (the number of folds), <code>foldid</code> (user-supplied folds), <code>type.measure</code>(the loss used for cross-validation): * “deviance” or “mse” uses squared loss</p>
<ul>
<li>“mae” uses mean absolute error</li>
</ul>
<p>As an example,</p>
<div class="sourceCode" id="cb33"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb33-1" data-line-number="1">cvfit =<span class="st"> </span><span class="kw"><a href="../reference/cv.glmnet.html">cv.glmnet</a></span>(x, y, <span class="dt">type.measure =</span> <span class="st">"mse"</span>, <span class="dt">nfolds =</span> <span class="dv">20</span>)</a></code></pre></div>
<p>does 20-fold cross-validation, based on mean squared error criterion (default though).</p>
<p>Parallel computing is also supported by <code>cv.glmnet</code>. To make it work, users must register parallel beforehand. We give a simple example of comparison here. Unfortunately, the package <code>doMC</code> is not available on Windows platforms (it is on others), so we cannot run the code here, but we make it looks as if we have.</p>
<div class="sourceCode" id="cb34"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb34-1" data-line-number="1"><span class="kw"><a href="https://rdrr.io/r/base/library.html">require</a></span>(doMC)</a>
<a class="sourceLine" id="cb34-2" data-line-number="2"><span class="kw">registerDoMC</span>(<span class="dt">cores=</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb34-3" data-line-number="3">X =<span class="st"> </span><span class="kw"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span>(<span class="kw"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span>(<span class="fl">1e4</span> <span class="op">*</span><span class="st"> </span><span class="dv">200</span>), <span class="fl">1e4</span>, <span class="dv">200</span>)</a>
<a class="sourceLine" id="cb34-4" data-line-number="4">Y =<span class="st"> </span><span class="kw"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span>(<span class="fl">1e4</span>)</a></code></pre></div>
<div class="sourceCode" id="cb35"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb35-1" data-line-number="1"><span class="kw"><a href="https://rdrr.io/r/base/system.time.html">system.time</a></span>(<span class="kw"><a href="../reference/cv.glmnet.html">cv.glmnet</a></span>(X, Y))</a></code></pre></div>
<pre><code>##    user  system elapsed 
##   2.440   0.080   2.518</code></pre>
<div class="sourceCode" id="cb37"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb37-1" data-line-number="1"><span class="kw"><a href="https://rdrr.io/r/base/system.time.html">system.time</a></span>(<span class="kw"><a href="../reference/cv.glmnet.html">cv.glmnet</a></span>(X, Y, <span class="dt">parallel =</span> <span class="ot">TRUE</span>))</a></code></pre></div>
<pre><code>##    user  system elapsed 
##   2.450   0.157   1.567</code></pre>
<p>As suggested from the above, parallel computing can significantly speed up the computation process especially for large-scale problems.</p>
<p>Functions <code>coef</code> and <code>predict</code> on cv.glmnet object are similar to those for a <code>glmnet</code> object, except that two special strings are also supported by <code>s</code> (the values of <span class="math inline">\(\lambda\)</span> requested): * “lambda.1se”: the largest <span class="math inline">\(\lambda\)</span> at which the MSE is within one standard error of the minimal MSE.</p>
<ul>
<li>“lambda.min”: the <span class="math inline">\(\lambda\)</span> at which the minimal MSE is achieved.</li>
</ul>
<div class="sourceCode" id="cb39"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb39-1" data-line-number="1">cvfit<span class="op">$</span>lambda.min</a></code></pre></div>
<pre><code>## [1] 0.07569327</code></pre>
<div class="sourceCode" id="cb41"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb41-1" data-line-number="1"><span class="kw"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span>(cvfit, <span class="dt">s =</span> <span class="st">"lambda.min"</span>)</a></code></pre></div>
<pre><code>## 21 x 1 sparse Matrix of class "dgCMatrix"
##                       1
## (Intercept)  0.14867414
## V1           1.33377821
## V2           .         
## V3           0.69787701
## V4           .         
## V5          -0.83726751
## V6           0.54334327
## V7           0.02668633
## V8           0.33741131
## V9           .         
## V10          .         
## V11          0.17105029
## V12          .         
## V13          .         
## V14         -1.07552680
## V15          .         
## V16          .         
## V17          .         
## V18          .         
## V19          .         
## V20         -1.05278699</code></pre>
<div class="sourceCode" id="cb43"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb43-1" data-line-number="1"><span class="kw"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span>(cvfit, <span class="dt">newx =</span> x[<span class="dv">1</span><span class="op">:</span><span class="dv">5</span>,], <span class="dt">s =</span> <span class="st">"lambda.min"</span>)</a></code></pre></div>
<pre><code>##               1
## [1,] -1.3638848
## [2,]  2.5713428
## [3,]  0.5729785
## [4,]  1.9881422
## [5,]  1.5179882</code></pre>
<p>Users can control the folds used. Here we use the same folds so we can also select a value for <span class="math inline">\(\alpha\)</span>.</p>
<div class="sourceCode" id="cb45"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb45-1" data-line-number="1">foldid=<span class="kw"><a href="https://rdrr.io/r/base/sample.html">sample</a></span>(<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>,<span class="dt">size=</span><span class="kw"><a href="https://rdrr.io/r/base/length.html">length</a></span>(y),<span class="dt">replace=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb45-2" data-line-number="2">cv1=<span class="kw"><a href="../reference/cv.glmnet.html">cv.glmnet</a></span>(x,y,<span class="dt">foldid=</span>foldid,<span class="dt">alpha=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb45-3" data-line-number="3">cv<span class="fl">.5</span>=<span class="kw"><a href="../reference/cv.glmnet.html">cv.glmnet</a></span>(x,y,<span class="dt">foldid=</span>foldid,<span class="dt">alpha=</span>.<span class="dv">5</span>)</a>
<a class="sourceLine" id="cb45-4" data-line-number="4">cv0=<span class="kw"><a href="../reference/cv.glmnet.html">cv.glmnet</a></span>(x,y,<span class="dt">foldid=</span>foldid,<span class="dt">alpha=</span><span class="dv">0</span>)</a></code></pre></div>
<p>There are no built-in plot functions to put them all on the same plot, so we are on our own here:</p>
<div class="sourceCode" id="cb46"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb46-1" data-line-number="1"><span class="kw"><a href="https://rdrr.io/r/graphics/par.html">par</a></span>(<span class="dt">mfrow=</span><span class="kw"><a href="https://rdrr.io/r/base/c.html">c</a></span>(<span class="dv">2</span>,<span class="dv">2</span>))</a>
<a class="sourceLine" id="cb46-2" data-line-number="2"><span class="kw"><a href="https://rdrr.io/r/graphics/plot.html">plot</a></span>(cv1);<span class="kw"><a href="https://rdrr.io/r/graphics/plot.html">plot</a></span>(cv<span class="fl">.5</span>);<span class="kw"><a href="https://rdrr.io/r/graphics/plot.html">plot</a></span>(cv0)</a>
<a class="sourceLine" id="cb46-3" data-line-number="3"><span class="kw"><a href="https://rdrr.io/r/graphics/plot.html">plot</a></span>(<span class="kw"><a href="https://rdrr.io/r/base/Log.html">log</a></span>(cv1<span class="op">$</span>lambda),cv1<span class="op">$</span>cvm,<span class="dt">pch=</span><span class="dv">19</span>,<span class="dt">col=</span><span class="st">"red"</span>,<span class="dt">xlab=</span><span class="st">"log(Lambda)"</span>,<span class="dt">ylab=</span>cv1<span class="op">$</span>name)</a>
<a class="sourceLine" id="cb46-4" data-line-number="4"><span class="kw"><a href="https://rdrr.io/r/graphics/points.html">points</a></span>(<span class="kw"><a href="https://rdrr.io/r/base/Log.html">log</a></span>(cv<span class="fl">.5</span><span class="op">$</span>lambda),cv<span class="fl">.5</span><span class="op">$</span>cvm,<span class="dt">pch=</span><span class="dv">19</span>,<span class="dt">col=</span><span class="st">"grey"</span>)</a>
<a class="sourceLine" id="cb46-5" data-line-number="5"><span class="kw"><a href="https://rdrr.io/r/graphics/points.html">points</a></span>(<span class="kw"><a href="https://rdrr.io/r/base/Log.html">log</a></span>(cv0<span class="op">$</span>lambda),cv0<span class="op">$</span>cvm,<span class="dt">pch=</span><span class="dv">19</span>,<span class="dt">col=</span><span class="st">"blue"</span>)</a>
<a class="sourceLine" id="cb46-6" data-line-number="6"><span class="kw"><a href="https://rdrr.io/r/graphics/legend.html">legend</a></span>(<span class="st">"topleft"</span>,<span class="dt">legend=</span><span class="kw"><a href="https://rdrr.io/r/base/c.html">c</a></span>(<span class="st">"alpha= 1"</span>,<span class="st">"alpha= .5"</span>,<span class="st">"alpha 0"</span>),<span class="dt">pch=</span><span class="dv">19</span>,<span class="dt">col=</span><span class="kw"><a href="https://rdrr.io/r/base/c.html">c</a></span>(<span class="st">"red"</span>,<span class="st">"grey"</span>,<span class="st">"blue"</span>))</a></code></pre></div>
<p><img src="glmnet_files/figure-html/unnamed-chunk-28-1.png" width="700"></p>
<p>We see that lasso (<code>alpha=1</code>) does about the best here. We also see that the range of lambdas used differs with alpha.</p>
<div id="coefficient-upper-and-lower-bounds" class="section level4">
<h4 class="hasAnchor">
<a href="#coefficient-upper-and-lower-bounds" class="anchor"></a>Coefficient upper and lower bounds</h4>
<p>These are recently added features that enhance the scope of the models. Suppose we want to fit our model, but limit the coefficients to be bigger than -0.7 and less than 0.5. This is easily achieved via the <code>upper.limits</code> and <code>lower.limits</code> arguments:</p>
<div class="sourceCode" id="cb47"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb47-1" data-line-number="1">tfit=<span class="kw"><a href="../reference/glmnet.html">glmnet</a></span>(x,y,<span class="dt">lower=</span><span class="op">-</span>.<span class="dv">7</span>,<span class="dt">upper=</span>.<span class="dv">5</span>)</a>
<a class="sourceLine" id="cb47-2" data-line-number="2"><span class="kw"><a href="https://rdrr.io/r/graphics/plot.html">plot</a></span>(tfit)</a></code></pre></div>
<p><img src="glmnet_files/figure-html/unnamed-chunk-29-1.png" width="700"></p>
<p>These are rather arbitrary limits; often we want the coefficients to be positive, so we can set only <code>lower.limit</code> to be 0. (Note, the lower limit must be no bigger than zero, and the upper limit no smaller than zero.) These bounds can be a vector, with different values for each coefficient. If given as a scalar, the same number gets recycled for all.</p>
</div>
<div id="penalty-factors" class="section level4">
<h4 class="hasAnchor">
<a href="#penalty-factors" class="anchor"></a>Penalty factors</h4>
<p>This argument allows users to apply separate penalty factors to each coefficient. Its default is 1 for each parameter, but other values can be specified. In particular, any variable with <code>penalty.factor</code> equal to zero is not penalized at all! Let <span class="math inline">\(v_j\)</span> denote the penalty factor for <span class="math inline">\(j\)</span> th variable. The penalty term becomes <span class="math display">\[
\lambda \sum_{j=1}^p \boldsymbol{v_j} P_\alpha(\beta_j) = \lambda \sum_{j=1}^p \boldsymbol{v_j} \left[ (1-\alpha)\frac{1}{2} \beta_j^2 + \alpha |\beta_j| \right].
\]</span> Note the penalty factors are internally rescaled to sum to nvars.</p>
<p>This is very useful when people have prior knowledge or preference over the variables. In many cases, some variables may be so important that one wants to keep them all the time, which can be achieved by setting corresponding penalty factors to 0:</p>
<div class="sourceCode" id="cb48"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb48-1" data-line-number="1">p.fac =<span class="st"> </span><span class="kw"><a href="https://rdrr.io/r/base/rep.html">rep</a></span>(<span class="dv">1</span>, <span class="dv">20</span>)</a>
<a class="sourceLine" id="cb48-2" data-line-number="2">p.fac[<span class="kw"><a href="https://rdrr.io/r/base/c.html">c</a></span>(<span class="dv">5</span>, <span class="dv">10</span>, <span class="dv">15</span>)] =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb48-3" data-line-number="3">pfit =<span class="st"> </span><span class="kw"><a href="../reference/glmnet.html">glmnet</a></span>(x, y, <span class="dt">penalty.factor =</span> p.fac)</a>
<a class="sourceLine" id="cb48-4" data-line-number="4"><span class="kw"><a href="https://rdrr.io/r/graphics/plot.html">plot</a></span>(pfit, <span class="dt">label =</span> <span class="ot">TRUE</span>)</a></code></pre></div>
<p><img src="glmnet_files/figure-html/unnamed-chunk-30-1.png" width="700"></p>
<p>We see from the labels that the three variables with 0 penalty factors always stay in the model, while the others follow typical regularization paths and shrunken to 0 eventually.</p>
<p>Some other useful arguments. <code>exclude</code> allows one to block certain variables from being the model at all. Of course, one could simply subset these out of <code>x</code>, but sometimes <code>exclude</code> is more useful, since it returns a full vector of coefficients, just with the excluded ones set to zero. There is also an <code>intercept</code> argument which defaults to <code>TRUE</code>; if <code>FALSE</code> the intercept is forced to be zero.</p>
</div>
<div id="customizing-plots" class="section level4">
<h4 class="hasAnchor">
<a href="#customizing-plots" class="anchor"></a>Customizing plots</h4>
<p>Sometimes, especially when the number of variables is small, we want to add variable labels to a plot. Since <code>glmnet</code> is intended primarily for wide data, this is not supprted in <code>plot.glmnet</code>. However, it is easy to do, as the following little toy example shows.</p>
<p>We first generate some data, with 10 variables, and for lack of imagination and ease we give them simple character names. We then fit a glmnet model, and make the standard plot.</p>
<div class="sourceCode" id="cb49"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb49-1" data-line-number="1"><span class="kw"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span>(<span class="dv">101</span>)</a>
<a class="sourceLine" id="cb49-2" data-line-number="2">x=<span class="kw"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span>(<span class="kw"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span>(<span class="dv">1000</span>),<span class="dv">100</span>,<span class="dv">10</span>)</a>
<a class="sourceLine" id="cb49-3" data-line-number="3">y=<span class="kw"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span>(<span class="dv">100</span>)</a>
<a class="sourceLine" id="cb49-4" data-line-number="4">vn=<span class="kw"><a href="https://rdrr.io/r/base/paste.html">paste</a></span>(<span class="st">"var"</span>,<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>)</a>
<a class="sourceLine" id="cb49-5" data-line-number="5">fit=<span class="kw"><a href="../reference/glmnet.html">glmnet</a></span>(x,y)</a>
<a class="sourceLine" id="cb49-6" data-line-number="6"><span class="kw"><a href="https://rdrr.io/r/graphics/plot.html">plot</a></span>(fit)</a></code></pre></div>
<p><img src="glmnet_files/figure-html/unnamed-chunk-31-1.png" width="700"></p>
<p>We wish to label the curves with the variable names. Here s a simple way to do this, using the <code>axis</code> command in R (and a little research into how to customize it). We need to have the positions of the coefficients at the end of the path, and we need to make some space using the <code>par</code> command, so that our labels will fit in. This requires knowing how long your labels are, but here they are all quite short.</p>
<div class="sourceCode" id="cb50"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb50-1" data-line-number="1"><span class="kw"><a href="https://rdrr.io/r/graphics/par.html">par</a></span>(<span class="dt">mar=</span><span class="kw"><a href="https://rdrr.io/r/base/c.html">c</a></span>(<span class="fl">4.5</span>,<span class="fl">4.5</span>,<span class="dv">1</span>,<span class="dv">4</span>))</a>
<a class="sourceLine" id="cb50-2" data-line-number="2"><span class="kw"><a href="https://rdrr.io/r/graphics/plot.html">plot</a></span>(fit)</a>
<a class="sourceLine" id="cb50-3" data-line-number="3">vnat=<span class="kw"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span>(fit)</a>
<a class="sourceLine" id="cb50-4" data-line-number="4">vnat=vnat[<span class="op">-</span><span class="dv">1</span>,<span class="kw"><a href="https://rdrr.io/r/base/nrow.html">ncol</a></span>(vnat)] <span class="co"># remove the intercept, and get the coefficients at the end of the path</span></a>
<a class="sourceLine" id="cb50-5" data-line-number="5"><span class="kw"><a href="https://rdrr.io/r/graphics/axis.html">axis</a></span>(<span class="dv">4</span>, <span class="dt">at=</span>vnat,<span class="dt">line=</span><span class="op">-</span>.<span class="dv">5</span>,<span class="dt">label=</span>vn,<span class="dt">las=</span><span class="dv">1</span>,<span class="dt">tick=</span><span class="ot">FALSE</span>, <span class="dt">cex.axis=</span><span class="fl">0.5</span>)</a></code></pre></div>
<p><img src="glmnet_files/figure-html/unnamed-chunk-32-1.png" width="700"></p>
<p>We have done nothing here to avoid overwriting of labels, in the event that they are close together. This would be a bit more work, but perhaps best left alone, anyway.</p>
</div>
</div>
<div id="multiresponse-gaussian-family" class="section level3">
<h3 class="hasAnchor">
<a href="#multiresponse-gaussian-family" class="anchor"></a>Multiresponse Gaussian Family</h3>
<p>The multiresponse Gaussian family is obtained using <code>family = "mgaussian"</code> option in <code>glmnet</code>. It is very similar to the single-response case above. This is useful when there are a number of (correlated) responses - the so-called “multi-task learning” problem. Here the sharing involves which variables are selected, since when a variable is selected, a coefficient is fit for each response. Most of the options are the same, so we focus here on the differences with the single response model.</p>
<p>Obviously, as the name suggests, <span class="math inline">\(y\)</span> is not a vector, but a matrix of quantitative responses in this section. The coefficients at each value of lambda are also a matrix as a result.</p>
<p>Here we solve the following problem: <span class="math display">\[
\min_{(\beta_0, \beta) \in \mathbb{R}^{(p+1)\times K}}\frac{1}{2N} \sum_{i=1}^N ||y_i -\beta_0-\beta^T x_i||^2_F+\lambda \left[ (1-\alpha)||\beta||_F^2/2 + \alpha\sum_{j=1}^p||\beta_j||_2\right].
\]</span> Here <span class="math inline">\(\beta_j\)</span> is the jth row of the <span class="math inline">\(p\times K\)</span> coefficient matrix <span class="math inline">\(\beta\)</span>, and we replace the absolute penalty on each single coefficient by a group-lasso penalty on each coefficient K-vector <span class="math inline">\(\beta_j\)</span> for a single predictor <span class="math inline">\(x_j\)</span>.</p>
<p>We use a set of data generated beforehand for illustration.</p>
<div class="sourceCode" id="cb51"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb51-1" data-line-number="1"><span class="kw"><a href="https://rdrr.io/r/utils/data.html">data</a></span>(MultiGaussianExample)</a></code></pre></div>
<p>We fit the data, with an object “mfit” returned.</p>
<div class="sourceCode" id="cb52"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb52-1" data-line-number="1">mfit =<span class="st"> </span><span class="kw"><a href="../reference/glmnet.html">glmnet</a></span>(x, y, <span class="dt">family =</span> <span class="st">"mgaussian"</span>)</a></code></pre></div>
<p>For multiresponse Gaussian, the options in <code>glmnet</code> are almost the same as the single-response case, such as <code>alpha</code>, <code>weights</code>, <code>nlambda</code>, <code>standardize</code>. A exception to be noticed is that <code>standardize.response</code> is only for <code>mgaussian</code> family. The default value is <code>FALSE</code>. If <code>standardize.response = TRUE</code>, it standardizes the response variables.</p>
<p>To visualize the coefficients, we use the <code>plot</code> function.</p>
<div class="sourceCode" id="cb53"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb53-1" data-line-number="1"><span class="kw"><a href="https://rdrr.io/r/graphics/plot.html">plot</a></span>(mfit, <span class="dt">xvar =</span> <span class="st">"lambda"</span>, <span class="dt">label =</span> <span class="ot">TRUE</span>, <span class="dt">type.coef =</span> <span class="st">"2norm"</span>)</a></code></pre></div>
<p><img src="glmnet_files/figure-html/unnamed-chunk-35-1.png" width="700"></p>
<p>Note that we set <code>type.coef = "2norm"</code>. Under this setting, a single curve is plotted per variable, with value equal to the <span class="math inline">\(\ell_2\)</span> norm. The default setting is <code>type.coef = "coef"</code>, where a coefficient plot is created for each response (multiple figures).</p>
<p><code>xvar</code> and <code>label</code> are two other options besides ordinary graphical parameters. They are the same as the single-response case.</p>
<p>We can extract the coefficients at requested values of <span class="math inline">\(\lambda\)</span> by using the function <code>coef</code> and make predictions by <code>predict</code>. The usage is similar and we only provide an example of <code>predict</code> here.</p>
<div class="sourceCode" id="cb54"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb54-1" data-line-number="1"><span class="kw"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span>(mfit, <span class="dt">newx =</span> x[<span class="dv">1</span><span class="op">:</span><span class="dv">5</span>,], <span class="dt">s =</span> <span class="kw"><a href="https://rdrr.io/r/base/c.html">c</a></span>(<span class="fl">0.1</span>, <span class="fl">0.01</span>))</a></code></pre></div>
<pre><code>## , , 1
## 
##              y1         y2         y3       y4
## [1,] -4.7106263 -1.1634574  0.6027634 3.740989
## [2,]  4.1301735 -3.0507968 -1.2122630 4.970141
## [3,]  3.1595229 -0.5759621  0.2607981 2.053976
## [4,]  0.6459242  2.1205605 -0.2252050 3.146286
## [5,] -1.1791890  0.1056262 -7.3352965 3.248370
## 
## , , 2
## 
##              y1         y2         y3       y4
## [1,] -4.6415158 -1.2290282  0.6118289 3.779521
## [2,]  4.4712843 -3.2529658 -1.2572583 5.266039
## [3,]  3.4735228 -0.6929231  0.4684037 2.055574
## [4,]  0.7353311  2.2965083 -0.2190297 2.989371
## [5,] -1.2759930  0.2892536 -7.8259206 3.205211</code></pre>
<p>The prediction result is saved in a three-dimensional array with the first two dimensions being the prediction matrix for each response variable and the third indicating the response variables.</p>
<p>We can also do k-fold cross-validation. The options are almost the same as the ordinary Gaussian family and we do not expand here.</p>
<div class="sourceCode" id="cb56"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb56-1" data-line-number="1">cvmfit =<span class="st"> </span><span class="kw"><a href="../reference/cv.glmnet.html">cv.glmnet</a></span>(x, y, <span class="dt">family =</span> <span class="st">"mgaussian"</span>)</a></code></pre></div>
<p>We plot the resulting <code>cv.glmnet</code> object “cvmfit”.</p>
<div class="sourceCode" id="cb57"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb57-1" data-line-number="1"><span class="kw"><a href="https://rdrr.io/r/graphics/plot.html">plot</a></span>(cvmfit)</a></code></pre></div>
<p><img src="glmnet_files/figure-html/unnamed-chunk-38-1.png" width="700"></p>
<p>To show explicitly the selected optimal values of <span class="math inline">\(\lambda\)</span>, type</p>
<div class="sourceCode" id="cb58"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb58-1" data-line-number="1">cvmfit<span class="op">$</span>lambda.min</a></code></pre></div>
<pre><code>## [1] 0.05193158</code></pre>
<div class="sourceCode" id="cb60"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb60-1" data-line-number="1">cvmfit<span class="op">$</span>lambda<span class="fl">.1</span>se</a></code></pre></div>
<pre><code>## [1] 0.174054</code></pre>
<p>As before, the first one is the value at which the minimal mean squared error is achieved and the second is for the most regularized model whose mean squared error is within one standard error of the minimal.</p>
<p>Prediction for <code>cv.glmnet</code> object works almost the same as for <code>glmnet</code> object. We omit the details here.</p>
</div>
</div>
<div id="logistic-regression" class="section level2">
<h2 class="hasAnchor">
<a href="#logistic-regression" class="anchor"></a>Logistic Regression</h2>
<p>Logistic regression is another widely-used model when the response is categorical. If there are two possible outcomes, we use the binomial distribution, else we use the multinomial.</p>
<div id="binomial-models" class="section level3">
<h3 class="hasAnchor">
<a href="#binomial-models" class="anchor"></a>Binomial Models</h3>
<p>For the binomial model, suppose the response variable takes value in <span class="math inline">\(\mathcal{G}=\{1,2\}\)</span>. Denote <span class="math inline">\(y_i = I(g_i=1)\)</span>. We model <span class="math display">\[\mbox{Pr}(G=2|X=x)=\frac{e^{\beta_0+\beta^Tx}}{1+e^{\beta_0+\beta^Tx}},\]</span> which can be written in the following form <span class="math display">\[\log\frac{\mbox{Pr}(G=2|X=x)}{\mbox{Pr}(G=1|X=x)}=\beta_0+\beta^Tx,\]</span> the so-called “logistic” or log-odds transformation.</p>
<p>The objective function for the penalized logistic regression uses the negative binomial log-likelihood, and is <span class="math display">\[
\min_{(\beta_0, \beta) \in \mathbb{R}^{p+1}} -\left[\frac{1}{N} \sum_{i=1}^N y_i \cdot (\beta_0 + x_i^T \beta) - \log (1+e^{(\beta_0+x_i^T \beta)})\right] + \lambda \big[ (1-\alpha)||\beta||_2^2/2 + \alpha||\beta||_1\big].
\]</span> Logistic regression is often plagued with degeneracies when <span class="math inline">\(p &gt; N\)</span> and exhibits wild behavior even when <span class="math inline">\(N\)</span> is close to <span class="math inline">\(p\)</span>; the elastic-net penalty alleviates these issues, and regularizes and selects variables as well.</p>
<p>Our algorithm uses a quadratic approximation to the log-likelihood, and then coordinate descent on the resulting penalized weighted least-squares problem. These constitute an outer and inner loop.</p>
<p>For illustration purpose, we load pre-generated input matrix <code>x</code> and the response vector <code>y</code> from the data file.</p>
<div class="sourceCode" id="cb62"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb62-1" data-line-number="1"><span class="kw"><a href="https://rdrr.io/r/utils/data.html">data</a></span>(BinomialExample)</a></code></pre></div>
<p>The input matrix <span class="math inline">\(x\)</span> is the same as other families. For binomial logistic regression, the response variable <span class="math inline">\(y\)</span> should be either a factor with two levels, or a two-column matrix of counts or proportions.</p>
<p>Other optional arguments of <code>glmnet</code> for binomial regression are almost same as those for Gaussian family. Don’t forget to set <code>family</code> option to “binomial”.</p>
<div class="sourceCode" id="cb63"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb63-1" data-line-number="1">fit =<span class="st"> </span><span class="kw"><a href="../reference/glmnet.html">glmnet</a></span>(x, y, <span class="dt">family =</span> <span class="st">"binomial"</span>)</a></code></pre></div>
<p>Like before, we can print and plot the fitted object, extract the coefficients at specific <span class="math inline">\(\lambda\)</span>’s and also make predictions. For plotting, the optional arguments such as <code>xvar</code> and <code>label</code> are similar to the Gaussian. We plot against the deviance explained and show the labels.</p>
<div class="sourceCode" id="cb64"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb64-1" data-line-number="1"><span class="kw"><a href="https://rdrr.io/r/graphics/plot.html">plot</a></span>(fit, <span class="dt">xvar =</span> <span class="st">"dev"</span>, <span class="dt">label =</span> <span class="ot">TRUE</span>)</a></code></pre></div>
<p><img src="glmnet_files/figure-html/unnamed-chunk-42-1.png" width="700"></p>
<p>Prediction is a little different for logistic from Gaussian, mainly in the option <code>type</code>. “link” and “response” are never equivalent and “class” is only available for logistic regression. In summary, * “link” gives the linear predictors</p>
<ul>
<li><p>“response” gives the fitted probabilities</p></li>
<li><p>“class” produces the class label corresponding to the maximum probability.</p></li>
<li><p>“coefficients” computes the coefficients at values of <code>s</code></p></li>
<li><p>“nonzero” retuns a list of the indices of the nonzero coefficients for each value of <code>s</code>.</p></li>
</ul>
<p>For “binomial” models, results (“link”, “response”, “coefficients”, “nonzero”) are returned only for the class corresponding to the second level of the factor response.</p>
<p>In the following example, we make prediction of the class labels at <span class="math inline">\(\lambda = 0.05, 0.01\)</span>.</p>
<div class="sourceCode" id="cb65"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb65-1" data-line-number="1"><span class="kw"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span>(fit, <span class="dt">newx =</span> x[<span class="dv">1</span><span class="op">:</span><span class="dv">5</span>,], <span class="dt">type =</span> <span class="st">"class"</span>, <span class="dt">s =</span> <span class="kw"><a href="https://rdrr.io/r/base/c.html">c</a></span>(<span class="fl">0.05</span>, <span class="fl">0.01</span>))</a></code></pre></div>
<pre><code>##      1   2  
## [1,] "0" "0"
## [2,] "1" "1"
## [3,] "1" "1"
## [4,] "0" "0"
## [5,] "1" "1"</code></pre>
<p>For logistic regression, <code>cv.glmnet</code> has similar arguments and usage as Gaussian. <code>nfolds</code>, <code>weights</code>, <code>lambda</code>, <code>parallel</code> are all available to users. There are some differences in <code>type.measure</code>: “deviance” and “mse” do not both mean squared loss and “class” is enabled. Hence, * “mse” uses squared loss.</p>
<ul>
<li><p>“deviance” uses actual deviance.</p></li>
<li><p>“mae” uses mean absolute error.</p></li>
<li><p>“class” gives misclassification error.</p></li>
<li><p>“auc” (for two-class logistic regression ONLY) gives area under the ROC curve.</p></li>
</ul>
<p>For example,</p>
<div class="sourceCode" id="cb67"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb67-1" data-line-number="1">cvfit =<span class="st"> </span><span class="kw"><a href="../reference/cv.glmnet.html">cv.glmnet</a></span>(x, y, <span class="dt">family =</span> <span class="st">"binomial"</span>, <span class="dt">type.measure =</span> <span class="st">"class"</span>)</a></code></pre></div>
<p>It uses misclassification error as the criterion for 10-fold cross-validation.</p>
<p>We plot the object and show the optimal values of <span class="math inline">\(\lambda\)</span>.</p>
<div class="sourceCode" id="cb68"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb68-1" data-line-number="1"><span class="kw"><a href="https://rdrr.io/r/graphics/plot.html">plot</a></span>(cvfit)</a></code></pre></div>
<p><img src="glmnet_files/figure-html/unnamed-chunk-45-1.png" width="700"></p>
<div class="sourceCode" id="cb69"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb69-1" data-line-number="1">cvfit<span class="op">$</span>lambda.min</a></code></pre></div>
<pre><code>## [1] 0.01116192</code></pre>
<div class="sourceCode" id="cb71"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb71-1" data-line-number="1">cvfit<span class="op">$</span>lambda<span class="fl">.1</span>se</a></code></pre></div>
<pre><code>## [1] 0.0310587</code></pre>
<p><code>coef</code> and <code>predict</code> are simliar to the Gaussian case and we omit the details. We review by some examples.</p>
<div class="sourceCode" id="cb73"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb73-1" data-line-number="1"><span class="kw"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span>(cvfit, <span class="dt">s =</span> <span class="st">"lambda.min"</span>)</a></code></pre></div>
<pre><code>## 31 x 1 sparse Matrix of class "dgCMatrix"
##                        1
## (Intercept)  0.219571058
## V1           0.127143183
## V2           0.773438290
## V3          -0.622026676
## V4          -1.249153389
## V5          -0.236036348
## V6          -1.086126630
## V7           .          
## V8          -0.662605659
## V9           0.903895120
## V10         -1.662994097
## V11         -0.069429691
## V12         -0.109197704
## V13          .          
## V14          .          
## V15          .          
## V16          0.489302060
## V17          .          
## V18         -0.123121403
## V19         -0.009732698
## V20         -0.063913565
## V21          .          
## V22          0.237278300
## V23          0.413516339
## V24         -0.040687440
## V25          0.746129316
## V26         -0.376173274
## V27         -0.168082915
## V28          0.312045065
## V29         -0.254478998
## V30          0.157077462</code></pre>
<p>As mentioned previously, the results returned here are only for the second level of the factor response.</p>
<div class="sourceCode" id="cb75"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb75-1" data-line-number="1"><span class="kw"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span>(cvfit, <span class="dt">newx =</span> x[<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>,], <span class="dt">s =</span> <span class="st">"lambda.min"</span>, <span class="dt">type =</span> <span class="st">"class"</span>)</a></code></pre></div>
<pre><code>##       1  
##  [1,] "0"
##  [2,] "1"
##  [3,] "1"
##  [4,] "0"
##  [5,] "1"
##  [6,] "0"
##  [7,] "0"
##  [8,] "0"
##  [9,] "1"
## [10,] "1"</code></pre>
<p>Like other GLMs, glmnet allows for an “offset”. This is a fixed vector of N numbers that is added into the linear predictor. For example, you may have fitted some other logistic regression using other variables (and data), and now you want to see if the present variables can add anything. So you use the predicted logit from the other model as an offset in.</p>
</div>
<div id="multinomial-models" class="section level3">
<h3 class="hasAnchor">
<a href="#multinomial-models" class="anchor"></a>Multinomial Models</h3>
<p>For the multinomial model, suppose the response variable has <span class="math inline">\(K\)</span> levels <span class="math inline">\({\cal G}=\{1,2,\ldots,K\}\)</span>. Here we model <span class="math display">\[\mbox{Pr}(G=k|X=x)=\frac{e^{\beta_{0k}+\beta_k^Tx}}{\sum_{\ell=1}^Ke^{\beta_{0\ell}+\beta_\ell^Tx}}.\]</span></p>
<p>Let <span class="math inline">\({Y}\)</span> be the <span class="math inline">\(N \times K\)</span> indicator response matrix, with elements <span class="math inline">\(y_{i\ell} = I(g_i=\ell)\)</span>. Then the elastic-net penalized negative log-likelihood function becomes <span class="math display">\[
\ell(\{\beta_{0k},\beta_{k}\}_1^K) = -\left[\frac{1}{N} \sum_{i=1}^N \Big(\sum_{k=1}^Ky_{il} (\beta_{0k} + x_i^T \beta_k)- \log \big(\sum_{k=1}^K e^{\beta_{0k}+x_i^T \beta_k}\big)\Big)\right] +\lambda \left[ (1-\alpha)||\beta||_F^2/2 + \alpha\sum_{j=1}^p||\beta_j||_q\right].
\]</span> Here we really abuse notation! <span class="math inline">\(\beta\)</span> is a <span class="math inline">\(p\times K\)</span> matrix of coefficients. <span class="math inline">\(\beta_k\)</span> refers to the kth column (for outcome category k), and <span class="math inline">\(\beta_j\)</span> the jth row (vector of K coefficients for variable j). The last penalty term is <span class="math inline">\(||\beta_j||_q\)</span>, we have two options for q: <span class="math inline">\(q\in \{1,2\}\)</span>. When q=1, this is a lasso penalty on each of the parameters. When q=2, this is a grouped-lasso penalty on all the K coefficients for a particular variables, which makes them all be zero or nonzero together.</p>
<p>The standard Newton algorithm can be tedious here. Instead, we use a so-called partial Newton algorithm by making a partial quadratic approximation to the log-likelihood, allowing only <span class="math inline">\((\beta_{0k}, \beta_k)\)</span> to vary for a single class at a time. For each value of <span class="math inline">\(\lambda\)</span>, we first cycle over all classes indexed by <span class="math inline">\(k\)</span>, computing each time a partial quadratic approximation about the parameters of the current class. Then the inner procedure is almost the same as for the binomial case. This is the case for lasso (q=1). When q=2, we use a different approach, which we wont dwell on here.</p>
<p>For the multinomial case, the usage is similar to logistic regression, and we mainly illustrate by examples and address any differences. We load a set of generated data.</p>
<div class="sourceCode" id="cb77"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb77-1" data-line-number="1"><span class="kw"><a href="https://rdrr.io/r/utils/data.html">data</a></span>(MultinomialExample)</a></code></pre></div>
<p>The optional arguments in <code>glmnet</code> for multinomial logistic regression are mostly similar to binomial regression except for a few cases.</p>
<p>The response variable can be a <code>nc &gt;= 2</code> level factor, or a <code>nc</code>-column matrix of counts or proportions. Internally glmnet will make the rows of this matrix sum to 1, and absorb the total mass into the weight for that observation.</p>
<p><code>offset</code> should be a <code>nobs x nc</code> matrix if there is one.</p>
<p>A special option for multinomial regression is <code>type.multinomial</code>, which allows the usage of a grouped lasso penalty if <code>type.multinomial = "grouped"</code>. This will ensure that the multinomial coefficients for a variable are all in or out together, just like for the multi-response Gaussian.</p>
<div class="sourceCode" id="cb78"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb78-1" data-line-number="1">fit =<span class="st"> </span><span class="kw"><a href="../reference/glmnet.html">glmnet</a></span>(x, y, <span class="dt">family =</span> <span class="st">"multinomial"</span>, <span class="dt">type.multinomial =</span> <span class="st">"grouped"</span>)</a></code></pre></div>
<p>We plot the resulting object “fit”.</p>
<div class="sourceCode" id="cb79"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb79-1" data-line-number="1"><span class="kw"><a href="https://rdrr.io/r/graphics/plot.html">plot</a></span>(fit, <span class="dt">xvar =</span> <span class="st">"lambda"</span>, <span class="dt">label =</span> <span class="ot">TRUE</span>, <span class="dt">type.coef =</span> <span class="st">"2norm"</span>)</a></code></pre></div>
<p><img src="glmnet_files/figure-html/unnamed-chunk-51-1.png" width="700"></p>
<p>The options are <code>xvar</code>, <code>label</code> and <code>type.coef</code>, in addition to other ordinary graphical parameters.</p>
<p><code>xvar</code> and <code>label</code> are the same as other families while <code>type.coef</code> is only for multinomial regression and multiresponse Gaussian model. It can produce a figure of coefficients for each response variable if <code>type.coef = "coef"</code> or a figure showing the <span class="math inline">\(\ell_2\)</span>-norm in one figure if <code>type.coef = "2norm"</code></p>
<p>We can also do cross-validation and plot the returned object.</p>
<div class="sourceCode" id="cb80"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb80-1" data-line-number="1">cvfit=<span class="kw"><a href="../reference/cv.glmnet.html">cv.glmnet</a></span>(x, y, <span class="dt">family=</span><span class="st">"multinomial"</span>, <span class="dt">type.multinomial =</span> <span class="st">"grouped"</span>, <span class="dt">parallel =</span> <span class="ot">TRUE</span>)</a></code></pre></div>
<pre><code>## Warning: executing %dopar% sequentially: no parallel backend registered</code></pre>
<div class="sourceCode" id="cb82"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb82-1" data-line-number="1"><span class="kw"><a href="https://rdrr.io/r/graphics/plot.html">plot</a></span>(cvfit)</a></code></pre></div>
<p><img src="glmnet_files/figure-html/unnamed-chunk-52-1.png" width="700"></p>
<p>Note that although <code>type.multinomial</code> is not a typical argument in <code>cv.glmnet</code>, in fact any argument that can be passed to <code>glmnet</code> is valid in the argument list of <code>cv.glmnet</code>. We also use parallel computing to accelerate the calculation.</p>
<p>Users may wish to predict at the optimally selected <span class="math inline">\(\lambda\)</span>:</p>
<div class="sourceCode" id="cb83"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb83-1" data-line-number="1"><span class="kw"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span>(cvfit, <span class="dt">newx =</span> x[<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>,], <span class="dt">s =</span> <span class="st">"lambda.min"</span>, <span class="dt">type =</span> <span class="st">"class"</span>)</a></code></pre></div>
<pre><code>##       1  
##  [1,] "3"
##  [2,] "2"
##  [3,] "2"
##  [4,] "3"
##  [5,] "1"
##  [6,] "3"
##  [7,] "3"
##  [8,] "1"
##  [9,] "1"
## [10,] "2"</code></pre>
</div>
</div>
<div id="poisson-models" class="section level2">
<h2 class="hasAnchor">
<a href="#poisson-models" class="anchor"></a>Poisson Models</h2>
<p>Poisson regression is used to model count data under the assumption of Poisson error, or otherwise non-negative data where the mean and variance are proportional. Like the Gaussian and binomial model, the Poisson is a member of the exponential family of distributions. We usually model its positive mean on the log scale: <span class="math inline">\(\log \mu(x) = \beta_0+\beta' x\)</span>. The log-likelihood for observations <span class="math inline">\(\{x_i,y_i\}_1^N\)</span> is given my <span class="math display">\[
l(\beta|X, Y) = \sum_{i=1}^N \left(y_i (\beta_0+\beta' x_i) - e^{\beta_0+\beta^Tx_i}\right).
\]</span> As before, we optimize the penalized log-lielihood: <span class="math display">\[
\min_{\beta_0,\beta} -\frac1N l(\beta|X, Y)  + \lambda \left((1-\alpha) \sum_{i=1}^N \beta_i^2/2) +\alpha \sum_{i=1}^N |\beta_i|\right).
\]</span></p>
<p>Glmnet uses an outer Newton loop, and an inner weighted least-squares loop (as in logistic regression) to optimize this criterion.</p>
<p>First, we load a pre-generated set of Poisson data.</p>
<div class="sourceCode" id="cb85"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb85-1" data-line-number="1"><span class="kw"><a href="https://rdrr.io/r/utils/data.html">data</a></span>(PoissonExample)</a></code></pre></div>
<p>We apply the function <code>glmnet</code> with the <code>"poisson"</code> option.</p>
<div class="sourceCode" id="cb86"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb86-1" data-line-number="1">fit =<span class="st"> </span><span class="kw"><a href="../reference/glmnet.html">glmnet</a></span>(x, y, <span class="dt">family =</span> <span class="st">"poisson"</span>)</a></code></pre></div>
<p>The optional input arguments of <code>glmnet</code> for <code>"poisson"</code> family are similar to those for others.</p>
<p><code>offset</code> is a useful argument particularly in Poisson models.</p>
<p>When dealing with rate data in Poisson models, the counts collected are often based on different exposures, such as length of time observed, area and years. A poisson rate <span class="math inline">\(\mu(x)\)</span> is relative to a unit exposure time, so if an observation <span class="math inline">\(y_i\)</span> was exposed for <span class="math inline">\(E_i\)</span> units of time, then the expected count would be <span class="math inline">\(E_i\mu(x)\)</span>, and the log mean would be <span class="math inline">\(\log(E_i)+\log(\mu(x))\)</span>. In a case like this, we would supply an <em>offset</em> <span class="math inline">\(\log(E_i)\)</span> for each observation. Hence <code>offset</code> is a vector of length <code>nobs</code> that is included in the linear predictor. Other families can also use options, typically for different reasons.</p>
<p>(Warning: if <code>offset</code> is supplied in <code>glmnet</code>, offsets must also also be supplied to <code>predict</code> to make reasonable predictions.)</p>
<p>Again, we plot the coefficients to have a first sense of the result.</p>
<div class="sourceCode" id="cb87"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb87-1" data-line-number="1"><span class="kw"><a href="https://rdrr.io/r/graphics/plot.html">plot</a></span>(fit)</a></code></pre></div>
<p><img src="glmnet_files/figure-html/unnamed-chunk-56-1.png" width="700"></p>
<p>Like before, we can extract the coefficients and make predictions at certain <span class="math inline">\(\lambda\)</span>’s by using <code>coef</code> and <code>predict</code> respectively. The optional input arguments are similar to those for other families. In function <code>predict</code>, the option <code>type</code>, which is the type of prediction required, has its own specialties for Poisson family. That is, * “link” (default) gives the linear predictors like others * “response” gives the fitted mean * “coefficients” computes the coefficients at the requested values for <code>s</code>, which can also be realized by <code>coef</code> function * “nonzero” returns a a list of the indices of the nonzero coefficients for each value of <code>s</code>.</p>
<p>For example, we can do as follows.</p>
<div class="sourceCode" id="cb88"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb88-1" data-line-number="1"><span class="kw"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span>(fit, <span class="dt">s =</span> <span class="dv">1</span>)</a></code></pre></div>
<pre><code>## 21 x 1 sparse Matrix of class "dgCMatrix"
##                       1
## (Intercept)  0.61123371
## V1           0.45819758
## V2          -0.77060709
## V3           1.34015128
## V4           0.04350500
## V5          -0.20325967
## V6           .         
## V7           .         
## V8           .         
## V9           .         
## V10          .         
## V11          .         
## V12          0.01816309
## V13          .         
## V14          .         
## V15          .         
## V16          .         
## V17          .         
## V18          .         
## V19          .         
## V20          .</code></pre>
<div class="sourceCode" id="cb90"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb90-1" data-line-number="1"><span class="kw"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span>(fit, <span class="dt">newx =</span> x[<span class="dv">1</span><span class="op">:</span><span class="dv">5</span>,], <span class="dt">type =</span> <span class="st">"response"</span>, <span class="dt">s =</span> <span class="kw"><a href="https://rdrr.io/r/base/c.html">c</a></span>(<span class="fl">0.1</span>,<span class="dv">1</span>))</a></code></pre></div>
<pre><code>##               1          2
## [1,]  2.4944232  4.4263365
## [2,] 10.3513120 11.0586174
## [3,]  0.1179704  0.1781626
## [4,]  0.9713412  1.6828778
## [5,]  1.1133472  1.9934537</code></pre>
<p>We may also use cross-validation to find the optimal <span class="math inline">\(\lambda\)</span>’s and thus make inferences.</p>
<div class="sourceCode" id="cb92"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb92-1" data-line-number="1">cvfit =<span class="st"> </span><span class="kw"><a href="../reference/cv.glmnet.html">cv.glmnet</a></span>(x, y, <span class="dt">family =</span> <span class="st">"poisson"</span>)</a></code></pre></div>
<p>Options are almost the same as the Gaussian family except that for <code>type.measure</code>, * “deviance” (default) gives the deviance * “mse” stands for mean squared error * “mae” is for mean absolute error.</p>
<p>We can plot the <code>cv.glmnet</code> object.</p>
<div class="sourceCode" id="cb93"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb93-1" data-line-number="1"><span class="kw"><a href="https://rdrr.io/r/graphics/plot.html">plot</a></span>(cvfit)</a></code></pre></div>
<p><img src="glmnet_files/figure-html/unnamed-chunk-59-1.png" width="700"></p>
<p>We can also show the optimal <span class="math inline">\(\lambda\)</span>’s and the corresponding coefficients.</p>
<div class="sourceCode" id="cb94"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb94-1" data-line-number="1">opt.lam =<span class="st"> </span><span class="kw"><a href="https://rdrr.io/r/base/c.html">c</a></span>(cvfit<span class="op">$</span>lambda.min, cvfit<span class="op">$</span>lambda<span class="fl">.1</span>se)</a>
<a class="sourceLine" id="cb94-2" data-line-number="2"><span class="kw"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span>(cvfit, <span class="dt">s =</span> opt.lam)</a></code></pre></div>
<pre><code>## 21 x 2 sparse Matrix of class "dgCMatrix"
##                        1            2
## (Intercept)  0.070584044  0.200007601
## V1           0.609229006  0.571833738
## V2          -0.972486782 -0.927099773
## V3           1.509004735  1.466409189
## V4           0.225598648  0.192138902
## V5          -0.328715117 -0.301559295
## V6           .            .          
## V7          -0.005088443  .          
## V8           .            .          
## V9           .            .          
## V10          0.006071218  .          
## V11          .            .          
## V12          0.029070537  0.025801372
## V13         -0.014882186  .          
## V14          0.020864442  .          
## V15          .            .          
## V16          0.008606586  .          
## V17          .            .          
## V18          .            .          
## V19         -0.023621751  .          
## V20          0.011789503  0.009436611</code></pre>
<p>The <code>predict</code> method is similar and we do not repeat it here.</p>
</div>
<div id="cox-models" class="section level2">
<h2 class="hasAnchor">
<a href="#cox-models" class="anchor"></a>Cox Models</h2>
<p>The Cox proportional hazards model is commonly used for the study of the relationship beteween predictor variables and survival time. In the usual survival analysis framework, we have data of the form <span class="math inline">\((y_1, x_1, \delta_1), \ldots, (y_n, x_n, \delta_n)\)</span> where <span class="math inline">\(y_i\)</span>, the observed time, is a time of failure if <span class="math inline">\(\delta_i\)</span> is 1 or right-censoring if <span class="math inline">\(\delta_i\)</span> is 0. We also let <span class="math inline">\(t_1 &lt; t_2 &lt; \ldots &lt; t_m\)</span> be the increasing list of unique failure times, and <span class="math inline">\(j(i)\)</span> denote the index of the observation failing at time <span class="math inline">\(t_i\)</span>.</p>
<p>The Cox model assumes a semi-parametric form for the hazard <span class="math display">\[
h_i(t) = h_0(t) e^{x_i^T \beta},
\]</span> where <span class="math inline">\(h_i(t)\)</span> is the hazard for patient <span class="math inline">\(i\)</span> at time <span class="math inline">\(t\)</span>, <span class="math inline">\(h_0(t)\)</span> is a shared baseline hazard, and <span class="math inline">\(\beta\)</span> is a fixed, length <span class="math inline">\(p\)</span> vector. In the classic setting <span class="math inline">\(n \geq p\)</span>, inference is made via the partial likelihood <span class="math display">\[
L(\beta) = \prod_{i=1}^m \frac{e^{x_{j(i)}^T \beta}}{\sum_{j \in R_i} e^{x_j^T \beta}},
\]</span> where <span class="math inline">\(R_i\)</span> is the set of indices <span class="math inline">\(j\)</span> with <span class="math inline">\(y_j \geq t_i\)</span> (those at risk at time <span class="math inline">\(t_i\)</span>).</p>
<p>Note there is no intercept in the Cox mode (its built into the baseline hazard, and like it, would cancel in the partial likelihood.)</p>
<p>We penalize the negative log of the partial likelihood, just like the other models, with an elastic-net penalty.</p>
<p>We use a pre-generated set of sample data and response. Users can load their own data and follow a similar procedure. In this case <span class="math inline">\(x\)</span> must be an <span class="math inline">\(n\times p\)</span> matrix of covariate values - each row corresponds to a patient and each column a covariate. <span class="math inline">\(y\)</span> is an <span class="math inline">\(n \times 2\)</span> matrix, with a column “time” of failure/censoring times, and “status” a 0/1 indicator, with 1 meaning the time is a failure time, and zero a censoring time.</p>
<div class="sourceCode" id="cb96"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb96-1" data-line-number="1"><span class="kw"><a href="https://rdrr.io/r/utils/data.html">data</a></span>(CoxExample)</a>
<a class="sourceLine" id="cb96-2" data-line-number="2">y[<span class="dv">1</span><span class="op">:</span><span class="dv">5</span>,]</a></code></pre></div>
<pre><code>##            time status
## [1,] 1.76877757      1
## [2,] 0.54528404      1
## [3,] 0.04485918      0
## [4,] 0.85032298      0
## [5,] 0.61488426      1</code></pre>
<p>The <code>Surv</code> function in the package <code>survival</code> can create such a matrix. Note, however, that the <code>coxph</code> and related linear models can handle interval and other forms of censoring, while glmnet can only handle right censoring in its present form.</p>
<p>We apply the <code>glmnet</code> function to compute the solution path under default settings.</p>
<div class="sourceCode" id="cb98"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb98-1" data-line-number="1">fit =<span class="st"> </span><span class="kw"><a href="../reference/glmnet.html">glmnet</a></span>(x, y, <span class="dt">family =</span> <span class="st">"cox"</span>)</a></code></pre></div>
<p>All the standard options are available such as <code>alpha</code>, <code>weights</code>, <code>nlambda</code> and <code>standardize</code>. Their usage is similar as in the Gaussian case and we omit the details here. Users can also refer to the help file <code><a href="https://rdrr.io/r/utils/help.html">help(glmnet)</a></code>.</p>
<p>We can plot the coefficients.</p>
<div class="sourceCode" id="cb99"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb99-1" data-line-number="1"><span class="kw"><a href="https://rdrr.io/r/graphics/plot.html">plot</a></span>(fit)</a></code></pre></div>
<p><img src="glmnet_files/figure-html/unnamed-chunk-63-1.png" width="700"></p>
<p>As before, we can extract the coefficients at certain values of <span class="math inline">\(\lambda\)</span>.</p>
<div class="sourceCode" id="cb100"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb100-1" data-line-number="1"><span class="kw"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span>(fit, <span class="dt">s =</span> <span class="fl">0.05</span>)</a></code></pre></div>
<pre><code>## 30 x 1 sparse Matrix of class "dgCMatrix"
##               1
## V1   0.37693638
## V2  -0.09547797
## V3  -0.13595972
## V4   0.09814146
## V5  -0.11437545
## V6  -0.38898545
## V7   0.24291400
## V8   0.03647596
## V9   0.34739813
## V10  0.03865115
## V11  .         
## V12  .         
## V13  .         
## V14  .         
## V15  .         
## V16  .         
## V17  .         
## V18  .         
## V19  .         
## V20  .         
## V21  .         
## V22  .         
## V23  .         
## V24  .         
## V25  .         
## V26  .         
## V27  .         
## V28  .         
## V29  .         
## V30  .</code></pre>
<p>Since the Cox Model is not commonly used for prediction, we do not give an illustrative example on prediction. If needed, users can refer to the help file by typing <code><a href="https://rdrr.io/r/utils/help.html">help(predict.glmnet)</a></code>.</p>
<p>Also, the function <code>cv.glmnet</code> can be used to compute <span class="math inline">\(k\)</span>-fold cross-validation for the Cox model. The usage is similar to that for other families except for two main differences.</p>
<p>One is that <code>type.measure</code> only supports “deviance”(also default), which gives the partial-likelihood.</p>
<p>The other is in the option <code>grouped</code>. <code>grouped = TRUE</code> obtains the CV partial likelihood for the Kth fold by subtraction; by subtracting the log partial likelihood evaluated on the full dataset from that evaluated on the on the (K-1)/K dataset. This makes more efficient use of risk sets. With <code>grouped=FALSE</code> the log partial likelihood is computed only on the Kth fold, which is only reasonable if each fold has a large number of observations.</p>
<div class="sourceCode" id="cb102"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb102-1" data-line-number="1">cvfit =<span class="st"> </span><span class="kw"><a href="../reference/cv.glmnet.html">cv.glmnet</a></span>(x, y, <span class="dt">family =</span> <span class="st">"cox"</span>)</a></code></pre></div>
<p>Once fit, we can view the optimal <span class="math inline">\(\lambda\)</span> value and a cross validated error plot to help evaluate our model.</p>
<div class="sourceCode" id="cb103"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb103-1" data-line-number="1"><span class="kw"><a href="https://rdrr.io/r/graphics/plot.html">plot</a></span>(cvfit)</a></code></pre></div>
<p><img src="glmnet_files/figure-html/unnamed-chunk-66-1.png" width="700"></p>
<p>As previously, the left vertical line in our plot shows us where the CV-error curve hits its minimum. The right vertical line shows us the most regularized model with CV-error within 1 standard deviation of the minimum. We also extract such optimal <span class="math inline">\(\lambda\)</span>’s.</p>
<div class="sourceCode" id="cb104"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb104-1" data-line-number="1">cvfit<span class="op">$</span>lambda.min</a></code></pre></div>
<pre><code>## [1] 0.02107668</code></pre>
<div class="sourceCode" id="cb106"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb106-1" data-line-number="1">cvfit<span class="op">$</span>lambda<span class="fl">.1</span>se</a></code></pre></div>
<pre><code>## [1] 0.05343706</code></pre>
<p>We can check the active covariates in our model and see their coefficients.</p>
<div class="sourceCode" id="cb108"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb108-1" data-line-number="1">coef.min =<span class="st"> </span><span class="kw"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span>(cvfit, <span class="dt">s =</span> <span class="st">"lambda.min"</span>)</a>
<a class="sourceLine" id="cb108-2" data-line-number="2">active.min =<span class="st"> </span><span class="kw"><a href="https://rdrr.io/r/base/which.html">which</a></span>(coef.min <span class="op">!=</span><span class="st"> </span><span class="dv">0</span>)</a>
<a class="sourceLine" id="cb108-3" data-line-number="3">index.min =<span class="st"> </span>coef.min[active.min]</a></code></pre></div>
<div class="sourceCode" id="cb109"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb109-1" data-line-number="1">index.min</a></code></pre></div>
<pre><code>##  [1]  0.47296769 -0.16213158 -0.20518470  0.16374470 -0.17544445 -0.47500198
##  [7]  0.32076305  0.08339592  0.43422644  0.10423130  0.01054257 -0.01125802
## [13] -0.01541834</code></pre>
<div class="sourceCode" id="cb111"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb111-1" data-line-number="1">coef.min</a></code></pre></div>
<pre><code>## 30 x 1 sparse Matrix of class "dgCMatrix"
##               1
## V1   0.47296769
## V2  -0.16213158
## V3  -0.20518470
## V4   0.16374470
## V5  -0.17544445
## V6  -0.47500198
## V7   0.32076305
## V8   0.08339592
## V9   0.43422644
## V10  0.10423130
## V11  .         
## V12  .         
## V13  0.01054257
## V14  .         
## V15  .         
## V16  .         
## V17 -0.01125802
## V18  .         
## V19  .         
## V20  .         
## V21  .         
## V22  .         
## V23  .         
## V24  .         
## V25 -0.01541834
## V26  .         
## V27  .         
## V28  .         
## V29  .         
## V30  .</code></pre>
</div>
<div id="sparse-matrices" class="section level2">
<h2 class="hasAnchor">
<a href="#sparse-matrices" class="anchor"></a>Sparse Matrices</h2>
<p>Our package supports sparse input matrices, which allow efficient storage and operations of large matrices but with only a few nonzero entries. It is available for all families except for the <code>cox</code> family. The usage of sparse matrices (inherit from class <code>"sparseMatrix"</code> as in package <code>Matrix</code>) in <code>glmnet</code> is the same as if a regular matrix is provided.</p>
<p>We load a set of sample data created beforehand.</p>
<div class="sourceCode" id="cb113"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb113-1" data-line-number="1"><span class="kw"><a href="https://rdrr.io/r/utils/data.html">data</a></span>(SparseExample)</a></code></pre></div>
<p>It loads <code>x</code>, a 100*20 sparse input matrix and <code>y</code>, the response vector.</p>
<div class="sourceCode" id="cb114"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb114-1" data-line-number="1"><span class="kw"><a href="https://rdrr.io/r/base/class.html">class</a></span>(x)</a></code></pre></div>
<pre><code>## [1] "dgCMatrix"
## attr(,"package")
## [1] "Matrix"</code></pre>
<p>Users can create a sparse matrix with the function <code>sparseMatrix</code> by providing the locations and values of the nonzero entries. Alternatively, <code>Matrix</code> function can also be used to contruct a sparse matrix by setting <code>sparse = TRUE</code>, but this defeats the purpose somewhat.</p>
<p>We can fit the model the same way as before.</p>
<div class="sourceCode" id="cb116"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb116-1" data-line-number="1">fit =<span class="st"> </span><span class="kw"><a href="../reference/glmnet.html">glmnet</a></span>(x, y)</a></code></pre></div>
<p>We also do the cross-validation and plot the resulting object.</p>
<div class="sourceCode" id="cb117"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb117-1" data-line-number="1">cvfit =<span class="st"> </span><span class="kw"><a href="../reference/cv.glmnet.html">cv.glmnet</a></span>(x, y)</a>
<a class="sourceLine" id="cb117-2" data-line-number="2"><span class="kw"><a href="https://rdrr.io/r/graphics/plot.html">plot</a></span>(cvfit)</a></code></pre></div>
<p><img src="glmnet_files/figure-html/unnamed-chunk-73-1.png" width="700"></p>
<p>The usage of other functions are similar and we do not expand here.</p>
<p>Note that sparse matrices can also be used for <code>newx</code>, the new input matrix in the <code>predict</code> function. For example,</p>
<div class="sourceCode" id="cb118"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb118-1" data-line-number="1">i =<span class="st"> </span><span class="kw"><a href="https://rdrr.io/r/base/sample.html">sample</a></span>(<span class="dv">1</span><span class="op">:</span><span class="dv">5</span>, <span class="dt">size =</span> <span class="dv">25</span>, <span class="dt">replace =</span> <span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb118-2" data-line-number="2">j =<span class="st"> </span><span class="kw"><a href="https://rdrr.io/r/base/sample.html">sample</a></span>(<span class="dv">1</span><span class="op">:</span><span class="dv">20</span>, <span class="dt">size =</span> <span class="dv">25</span>, <span class="dt">replace =</span> <span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb118-3" data-line-number="3">x =<span class="st"> </span><span class="kw"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span>(<span class="dv">25</span>)</a>
<a class="sourceLine" id="cb118-4" data-line-number="4">nx =<span class="st"> </span><span class="kw">sparseMatrix</span>(<span class="dt">i =</span> i, <span class="dt">j =</span> j, <span class="dt">x =</span> x, <span class="dt">dims =</span> <span class="kw"><a href="https://rdrr.io/r/base/c.html">c</a></span>(<span class="dv">5</span>, <span class="dv">20</span>))</a>
<a class="sourceLine" id="cb118-5" data-line-number="5"><span class="kw"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span>(cvfit, <span class="dt">newx =</span> nx, <span class="dt">s =</span> <span class="st">"lambda.min"</span>)</a></code></pre></div>
<pre><code>##               1
## [1,]  0.8286576
## [2,] -0.1938951
## [3,]  0.7690298
## [4,] -0.4358310
## [5,] -0.1450590</code></pre>
</div>
<div id="appendix-0-convergence-criteria" class="section level2">
<h2 class="hasAnchor">
<a href="#appendix-0-convergence-criteria" class="anchor"></a>Appendix 0: Convergence Criteria</h2>
<p>Glmnet uses a convergence criterion that focuses not on coefficient change but rather the impact of the change on the fitted values, and hence the loss part of the objective. The net result is a weighted norm of the coefficient change vector.</p>
<p>For gaussian models it uses the following. Suppose observation <span class="math inline">\(i\)</span> has weight <span class="math inline">\(w_i\)</span>. Let <span class="math inline">\(v_j\)</span> be the (weighted) sum-of-squares for variable <span class="math inline">\(x_j\)</span>: <span class="math display">\[v_j=\sum_{i=1}^Nw_ix_{ij}^2.\]</span> If there is an intercept in the model, these <span class="math inline">\(x_j\)</span> will be centered by the weighted mean, and hence this would be a weighted variance. After <span class="math inline">\(\hat\beta_j^o\)</span> has been updated to <span class="math inline">\(\hat\beta_j^n\)</span>, we compute <span class="math inline">\(\Delta_j=v_j(\hat\beta_j^o-\hat\beta_j^n)^2\)</span>. After a complete cycle of coordinate descent, we look at <span class="math inline">\(\Delta_{max}=\max_j\Delta_j\)</span>. Why this measure? We can write <span class="math display">\[\Delta_j=\frac1N\sum_{i=1}^N w_j(x_{ij}\hat\beta_j^o-x_{ij}\hat\beta_j^n)^2,\]</span> which measures the weighted sum of squares of changes in fitted values for this term. This measures the impact of the change in this coefficient on the fit. If the largest such change is negligible, we stop.</p>
<p>For logistic regression, and other non-Gaussian models it is similar for the inner loop. Only now the weights for each observation are more complex. For example, for logisitic regression the weights are those that arise from the current Newton step, namely <span class="math inline">\(w_i^*=w_i\hat p_i(1-\hat p_i)\)</span>. Here <span class="math inline">\(\hat p_i\)</span> are the fitted probabilities as we entered the current inner loop. The intuition is the same — it measures the impact of the coefficient change on the current weighted least squares loss, or quadratic approximation to the log-likelihood loss.</p>
<p>What about outer-loop convergence? We use the same measure, except now <span class="math inline">\(\hat\beta^o\)</span> is the coefficient vector before we entered this inner loop, and <span class="math inline">\(\hat\beta^n\)</span> the converged solution for this inner loop. Hence if this Newton step had no impact, we declare outer-loop convergence.</p>
</div>
<div id="appendix-1-internal-parameters" class="section level2">
<h2 class="hasAnchor">
<a href="#appendix-1-internal-parameters" class="anchor"></a>Appendix 1: Internal Parameters</h2>
<p>Our package has a set of internal parameters which control some aspects of the computation of the path. The <em>factory default</em> settings are expected to serve in most cases, and users do not need to make changes unless there are special requirements.</p>
<p>There are several parameters that users can change:</p>
<p><code>fdev</code> - minimum fractional change in deviance for stopping path; factory default = 1.0e-5</p>
<p><code>devmax</code> - maximum fraction of explained deviance for stopping path; factory default = 0.999</p>
<ul>
<li><p><code>eps</code> - minimum value of lambda.min.ratio (see glmnet); factory default= 1.0e-6</p></li>
<li><p><code>big</code> - large floating point number; factory default = 9.9e35. Inf in definition of upper.limit is set to big</p></li>
<li><p><code>mnlam</code> - minimum number of path points (lambda values) allowed; factory default = 5</p></li>
<li><p><code>pmin</code> - minimum null probability for any class; factory default = 1.0e-5</p></li>
<li><p><code>exmx</code> - maximum allowed exponent; factory default = 250.0</p></li>
<li><p><code>prec</code> - convergence threshold for multi-response bounds adjustment solution; factory default = 1.0e-10</p></li>
<li><p><code>mxit</code> - maximum iterations for multiresponse bounds adjustment solution; factory default = 100</p></li>
<li><p><code>factory</code> - If <code>TRUE</code>, reset all the parameters to the factory default; default is <code>FALSE</code></p></li>
</ul>
<p>We illustrate the usage by an example. Note that any changes made hold for the duration of the R session, or unless they are changed by the user with a subsequent call to <code>glmnet.control</code>.</p>
<div class="sourceCode" id="cb120"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb120-1" data-line-number="1"><span class="kw"><a href="https://rdrr.io/r/utils/data.html">data</a></span>(QuickStartExample)</a>
<a class="sourceLine" id="cb120-2" data-line-number="2">fit =<span class="st"> </span><span class="kw"><a href="../reference/glmnet.html">glmnet</a></span>(x, y)</a>
<a class="sourceLine" id="cb120-3" data-line-number="3"><span class="kw"><a href="https://rdrr.io/r/base/print.html">print</a></span>(fit)</a></code></pre></div>
<pre><code>## 
## Call:  glmnet(x = x, y = y) 
## 
##    Df    %Dev  Lambda
## 1   0 0.00000 1.63100
## 2   2 0.05528 1.48600
## 3   2 0.14590 1.35400
## 4   2 0.22110 1.23400
## 5   2 0.28360 1.12400
## 6   2 0.33540 1.02400
## 7   4 0.39040 0.93320
## 8   5 0.45600 0.85030
## 9   5 0.51540 0.77470
## 10  6 0.57350 0.70590
## 11  6 0.62550 0.64320
## 12  6 0.66870 0.58610
## 13  6 0.70460 0.53400
## 14  6 0.73440 0.48660
## 15  7 0.76210 0.44330
## 16  7 0.78570 0.40400
## 17  7 0.80530 0.36810
## 18  7 0.82150 0.33540
## 19  7 0.83500 0.30560
## 20  7 0.84620 0.27840
## 21  7 0.85550 0.25370
## 22  7 0.86330 0.23120
## 23  8 0.87060 0.21060
## 24  8 0.87690 0.19190
## 25  8 0.88210 0.17490
## 26  8 0.88650 0.15930
## 27  8 0.89010 0.14520
## 28  8 0.89310 0.13230
## 29  8 0.89560 0.12050
## 30  8 0.89760 0.10980
## 31  9 0.89940 0.10010
## 32  9 0.90100 0.09117
## 33  9 0.90230 0.08307
## 34  9 0.90340 0.07569
## 35 10 0.90430 0.06897
## 36 11 0.90530 0.06284
## 37 11 0.90620 0.05726
## 38 12 0.90700 0.05217
## 39 15 0.90780 0.04754
## 40 16 0.90860 0.04331
## 41 16 0.90930 0.03947
## 42 16 0.90980 0.03596
## 43 17 0.91030 0.03277
## 44 17 0.91070 0.02985
## 45 18 0.91110 0.02720
## 46 18 0.91140 0.02479
## 47 19 0.91170 0.02258
## 48 19 0.91200 0.02058
## 49 19 0.91220 0.01875
## 50 19 0.91240 0.01708
## 51 19 0.91250 0.01557
## 52 19 0.91260 0.01418
## 53 19 0.91270 0.01292
## 54 19 0.91280 0.01178
## 55 19 0.91290 0.01073
## 56 19 0.91290 0.00978
## 57 19 0.91300 0.00891
## 58 19 0.91300 0.00812
## 59 19 0.91310 0.00740
## 60 19 0.91310 0.00674
## 61 19 0.91310 0.00614
## 62 20 0.91310 0.00559
## 63 20 0.91310 0.00510
## 64 20 0.91310 0.00464
## 65 20 0.91320 0.00423
## 66 20 0.91320 0.00386
## 67 20 0.91320 0.00351</code></pre>
<p>We can change the minimum fractional change in deviance for stopping path and compare the results.</p>
<div class="sourceCode" id="cb122"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb122-1" data-line-number="1"><span class="kw"><a href="../reference/glmnet.control.html">glmnet.control</a></span>(<span class="dt">fdev =</span> <span class="dv">0</span>)</a>
<a class="sourceLine" id="cb122-2" data-line-number="2">fit =<span class="st"> </span><span class="kw"><a href="../reference/glmnet.html">glmnet</a></span>(x, y)</a>
<a class="sourceLine" id="cb122-3" data-line-number="3"><span class="kw"><a href="https://rdrr.io/r/base/print.html">print</a></span>(fit)</a></code></pre></div>
<pre><code>## 
## Call:  glmnet(x = x, y = y) 
## 
##     Df    %Dev  Lambda
## 1    0 0.00000 1.63100
## 2    2 0.05528 1.48600
## 3    2 0.14590 1.35400
## 4    2 0.22110 1.23400
## 5    2 0.28360 1.12400
## 6    2 0.33540 1.02400
## 7    4 0.39040 0.93320
## 8    5 0.45600 0.85030
## 9    5 0.51540 0.77470
## 10   6 0.57350 0.70590
## 11   6 0.62550 0.64320
## 12   6 0.66870 0.58610
## 13   6 0.70460 0.53400
## 14   6 0.73440 0.48660
## 15   7 0.76210 0.44330
## 16   7 0.78570 0.40400
## 17   7 0.80530 0.36810
## 18   7 0.82150 0.33540
## 19   7 0.83500 0.30560
## 20   7 0.84620 0.27840
## 21   7 0.85550 0.25370
## 22   7 0.86330 0.23120
## 23   8 0.87060 0.21060
## 24   8 0.87690 0.19190
## 25   8 0.88210 0.17490
## 26   8 0.88650 0.15930
## 27   8 0.89010 0.14520
## 28   8 0.89310 0.13230
## 29   8 0.89560 0.12050
## 30   8 0.89760 0.10980
## 31   9 0.89940 0.10010
## 32   9 0.90100 0.09117
## 33   9 0.90230 0.08307
## 34   9 0.90340 0.07569
## 35  10 0.90430 0.06897
## 36  11 0.90530 0.06284
## 37  11 0.90620 0.05726
## 38  12 0.90700 0.05217
## 39  15 0.90780 0.04754
## 40  16 0.90860 0.04331
## 41  16 0.90930 0.03947
## 42  16 0.90980 0.03596
## 43  17 0.91030 0.03277
## 44  17 0.91070 0.02985
## 45  18 0.91110 0.02720
## 46  18 0.91140 0.02479
## 47  19 0.91170 0.02258
## 48  19 0.91200 0.02058
## 49  19 0.91220 0.01875
## 50  19 0.91240 0.01708
## 51  19 0.91250 0.01557
## 52  19 0.91260 0.01418
## 53  19 0.91270 0.01292
## 54  19 0.91280 0.01178
## 55  19 0.91290 0.01073
## 56  19 0.91290 0.00978
## 57  19 0.91300 0.00891
## 58  19 0.91300 0.00812
## 59  19 0.91310 0.00740
## 60  19 0.91310 0.00674
## 61  19 0.91310 0.00614
## 62  20 0.91310 0.00559
## 63  20 0.91310 0.00510
## 64  20 0.91310 0.00464
## 65  20 0.91320 0.00423
## 66  20 0.91320 0.00386
## 67  20 0.91320 0.00351
## 68  20 0.91320 0.00320
## 69  20 0.91320 0.00292
## 70  20 0.91320 0.00266
## 71  20 0.91320 0.00242
## 72  20 0.91320 0.00221
## 73  20 0.91320 0.00201
## 74  20 0.91320 0.00183
## 75  20 0.91320 0.00167
## 76  20 0.91320 0.00152
## 77  20 0.91320 0.00139
## 78  20 0.91320 0.00126
## 79  20 0.91320 0.00115
## 80  20 0.91320 0.00105
## 81  20 0.91320 0.00096
## 82  20 0.91320 0.00087
## 83  20 0.91320 0.00079
## 84  20 0.91320 0.00072
## 85  20 0.91320 0.00066
## 86  20 0.91320 0.00060
## 87  20 0.91320 0.00055
## 88  20 0.91320 0.00050
## 89  20 0.91320 0.00045
## 90  20 0.91320 0.00041
## 91  20 0.91320 0.00038
## 92  20 0.91320 0.00034
## 93  20 0.91320 0.00031
## 94  20 0.91320 0.00028
## 95  20 0.91320 0.00026
## 96  20 0.91320 0.00024
## 97  20 0.91320 0.00022
## 98  20 0.91320 0.00020
## 99  20 0.91320 0.00018
## 100 20 0.91320 0.00016</code></pre>
<p>We set <code>fdev = 0</code> to continue all along the path, even without much change. The length of the sequence becomes 100, which is the default of <code>nlambda</code>.</p>
<p>Users can also reset to the default settings.</p>
<div class="sourceCode" id="cb124"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb124-1" data-line-number="1"><span class="kw"><a href="../reference/glmnet.control.html">glmnet.control</a></span>(<span class="dt">factory =</span> <span class="ot">TRUE</span>)</a></code></pre></div>
<p>The current settings are obtained as follows.</p>
<div class="sourceCode" id="cb125"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb125-1" data-line-number="1"><span class="kw"><a href="../reference/glmnet.control.html">glmnet.control</a></span>()</a></code></pre></div>
<pre><code>## $fdev
## [1] 1e-05
## 
## $eps
## [1] 1e-06
## 
## $big
## [1] 9.9e+35
## 
## $mnlam
## [1] 5
## 
## $devmax
## [1] 0.999
## 
## $pmin
## [1] 1e-09
## 
## $exmx
## [1] 250
## 
## $itrace
## [1] 0
## 
## $prec
## [1] 1e-10
## 
## $mxit
## [1] 100</code></pre>
</div>
<div id="appendix-2-comparison-with-other-packages" class="section level2">
<h2 class="hasAnchor">
<a href="#appendix-2-comparison-with-other-packages" class="anchor"></a>Appendix 2: Comparison with Other Packages</h2>
<p>Some people may want to use <code>glmnet</code> to solve the Lasso or elastic-net problem at a single <span class="math inline">\(\lambda\)</span>. We compare here the solution by <code>glmnet</code> with other packages (such as CVX), and also as an illustration of parameter settings in this situation.</p>
<p><strong>Warning</strong>: Though such problems can be solved by <code>glmnet</code>, it is <strong>not recommended</strong> and is not the spirit of the package. <code>glmnet</code> fits the <strong>entire</strong> solution path for Lasso or elastic-net problems efficiently with various techniques such as warm start. Those advantages will disappear if the <span class="math inline">\(\lambda\)</span> sequence is forced to be only one value.</p>
<p>Nevertheless, we still illustrate with a typical example in linear model in the following for the purpose of comparison. Given <span class="math inline">\(X, Y\)</span> and <span class="math inline">\(\lambda_0 &gt; 0\)</span>, we want to find <span class="math inline">\(\beta\)</span> such that <span class="math display">\[
\min_{\beta} ||Y - X\beta||_2^2 + \lambda_0 ||\beta||_1,
\]</span> where, say, <span class="math inline">\(\lambda_0 = 8\)</span>.</p>
<p>We first solve using <code>glmnet</code>. Notice that there is no intercept term in the objective function, and the columns of <span class="math inline">\(X\)</span> are not necessarily standardized. Corresponding parameters have to be set to make it work correctly. In addition, there is a <span class="math inline">\(1/(2n)\)</span> factor before the quadratic term by default, we need to adjust <span class="math inline">\(\lambda\)</span> accordingly. For the purpose of comparison, the <code>thresh</code> option is specified to be 1e-20. However, this is not necessary in many practical applications.</p>
<div class="sourceCode" id="cb127"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb127-1" data-line-number="1">fit =<span class="st"> </span><span class="kw"><a href="../reference/glmnet.html">glmnet</a></span>(x, y, <span class="dt">intercept =</span> F, <span class="dt">standardize =</span> F, <span class="dt">lambda =</span> <span class="dv">8</span><span class="op">/</span>(<span class="dv">2</span><span class="op">*</span><span class="kw"><a href="https://rdrr.io/r/base/dim.html">dim</a></span>(x)[<span class="dv">1</span>]), <span class="dt">thresh =</span> <span class="fl">1e-20</span>)</a></code></pre></div>
<p>We then extract the coefficients (with no intercept).</p>
<div class="sourceCode" id="cb128"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb128-1" data-line-number="1">beta_glmnet =<span class="st"> </span><span class="kw"><a href="https://rdrr.io/r/base/matrix.html">as.matrix</a></span>(<span class="kw"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span>(fit, <span class="dt">type =</span> <span class="st">"coefficients"</span>)[<span class="op">-</span><span class="dv">1</span>,])</a></code></pre></div>
<p>In linear model as here this approach worked because we were using squared error loss, but with any nonlinear family, it will probably fail. The reason is we are not using step length optimization, and so rely on very good warm starts to put us in the quadratic region of the loss function.</p>
<p>Alternatively, a more stable and <strong>strongly recommended</strong> way to perform this task is to first fit the entire Lasso or elastic-net path without specifying <code>lambda</code>, but then provide the requested <span class="math inline">\(\lambda_0\)</span> to <code>predict</code> function to extract the corresponding coefficients. In fact, if <span class="math inline">\(\lambda_0\)</span> is not in the <span class="math inline">\(\lambda\)</span> sequence generated by <code>glmnet</code>, the path will be refitted along a new <span class="math inline">\(\lambda\)</span> sequence that includes the requested value <span class="math inline">\(\lambda_0\)</span> and the old sequence, and the coefficients will be returned at <span class="math inline">\(\lambda_0\)</span> based on the new fit. Remember to set <code>exact = TRUE</code> in <code>predict</code> function to get the exact solution. Otherwise, it will be approximated by linear interpolation.</p>
<div class="sourceCode" id="cb129"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb129-1" data-line-number="1">fit =<span class="st"> </span><span class="kw"><a href="../reference/glmnet.html">glmnet</a></span>(x, y, <span class="dt">intercept =</span> F, <span class="dt">standardize =</span> F, <span class="dt">thresh =</span> <span class="fl">1e-20</span>)</a>
<a class="sourceLine" id="cb129-2" data-line-number="2">beta_glmnet =<span class="st"> </span><span class="kw"><a href="https://rdrr.io/r/base/matrix.html">as.matrix</a></span>(<span class="kw"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span>(fit, <span class="dt">s =</span> <span class="dv">8</span><span class="op">/</span>(<span class="dv">2</span><span class="op">*</span><span class="kw"><a href="https://rdrr.io/r/base/dim.html">dim</a></span>(x)[<span class="dv">1</span>]), <span class="dt">type =</span> <span class="st">"coefficients"</span>,</a>
<a class="sourceLine" id="cb129-3" data-line-number="3">                                <span class="dt">exact =</span> <span class="ot">TRUE</span>, <span class="dt">x=</span>x, <span class="dt">y=</span>y)[<span class="op">-</span><span class="dv">1</span>,])</a></code></pre></div>
<p>We also use CVX, a general convex optimization solver, to solve this specific Lasso problem. Users could also call CVX from R using the <code>CVXfromR</code> package and solve the problem as follows.</p>
<div class="sourceCode" id="cb130"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb130-1" data-line-number="1"><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span>(CVXfromR)</a>
<a class="sourceLine" id="cb130-2" data-line-number="2">setup.dir =<span class="st"> "change/this/to/your/cvx/directory"</span></a>
<a class="sourceLine" id="cb130-3" data-line-number="3">n =<span class="st"> </span><span class="kw"><a href="https://rdrr.io/r/base/dim.html">dim</a></span>(x)[<span class="dv">1</span>]; p =<span class="st"> </span><span class="kw"><a href="https://rdrr.io/r/base/dim.html">dim</a></span>(x)[<span class="dv">2</span>]</a>
<a class="sourceLine" id="cb130-4" data-line-number="4">cvxcode =<span class="st"> </span><span class="kw"><a href="https://rdrr.io/r/base/paste.html">paste</a></span>(<span class="st">"variables beta(p)"</span>,</a>
<a class="sourceLine" id="cb130-5" data-line-number="5">                <span class="st">"minimize(square_pos(norm(y - x * beta, 2)) + lambda * norm(beta, 1))"</span>,</a>
<a class="sourceLine" id="cb130-6" data-line-number="6">                <span class="dt">sep =</span> <span class="st">";"</span>)</a>
<a class="sourceLine" id="cb130-7" data-line-number="7">Lasso =<span class="st"> </span><span class="kw">CallCVX</span>(cvxcode, <span class="dt">const.var =</span> <span class="kw"><a href="https://rdrr.io/r/base/list.html">list</a></span>(<span class="dt">p =</span> p, <span class="dt">x =</span> x, <span class="dt">y =</span> y, <span class="dt">lambda =</span> <span class="dv">8</span>), <span class="dt">opt.var.names =</span> <span class="st">"beta"</span>, <span class="dt">setup.dir =</span> setup.dir, <span class="dt">matlab.call =</span> <span class="st">"change/this/to/path/to/matlab"</span>)</a>
<a class="sourceLine" id="cb130-8" data-line-number="8">beta_CVX =<span class="st"> </span>Lasso<span class="op">$</span>beta</a></code></pre></div>
<p>For convenience here, the results were saved in <code>CVXResult.RData</code>, and we simply load in the results.</p>
<div class="sourceCode" id="cb131"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb131-1" data-line-number="1"><span class="kw"><a href="https://rdrr.io/r/utils/data.html">data</a></span>(CVXResults)</a></code></pre></div>
<p>In addition, we use <code>lars</code> to solve the same problem.</p>
<div class="sourceCode" id="cb132"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb132-1" data-line-number="1"><span class="kw"><a href="https://rdrr.io/r/base/library.html">require</a></span>(lars)</a></code></pre></div>
<div class="sourceCode" id="cb133"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb133-1" data-line-number="1">fit_lars =<span class="st"> </span><span class="kw"><a href="https://rdrr.io/pkg/lars/man/lars.html">lars</a></span>(x, y, <span class="dt">type =</span> <span class="st">"lasso"</span>, <span class="dt">intercept =</span> F, <span class="dt">normalize =</span> F)</a>
<a class="sourceLine" id="cb133-2" data-line-number="2">beta_lars =<span class="st"> </span><span class="kw"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span>(fit_lars, <span class="dt">s =</span> <span class="dv">8</span><span class="op">/</span><span class="dv">2</span>, <span class="dt">type =</span> <span class="st">"coefficients"</span>, <span class="dt">mode =</span> <span class="st">"lambda"</span>)<span class="op">$</span>coefficients</a></code></pre></div>
<p>The results are listed below up to 6 decimal digits (due to convergence thresholds).</p>
<div class="sourceCode" id="cb134"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb134-1" data-line-number="1">cmp =<span class="st"> </span><span class="kw"><a href="https://rdrr.io/r/base/Round.html">round</a></span>(<span class="kw"><a href="https://rdrr.io/r/base/cbind.html">cbind</a></span>(beta_glmnet, beta_lars, beta_CVX), <span class="dt">digits =</span> <span class="dv">6</span>)</a>
<a class="sourceLine" id="cb134-2" data-line-number="2"><span class="kw"><a href="https://rdrr.io/r/base/colnames.html">colnames</a></span>(cmp) =<span class="st"> </span><span class="kw"><a href="https://rdrr.io/r/base/c.html">c</a></span>(<span class="st">"beta_glmnet"</span>, <span class="st">"beta_lars"</span>, <span class="st">"beta_CVX"</span>)</a>
<a class="sourceLine" id="cb134-3" data-line-number="3">cmp</a></code></pre></div>
<pre><code>##     beta_glmnet beta_lars  beta_CVX
## V1     1.389118  1.389118  1.389118
## V2     0.007991  0.007991  0.007991
## V3     0.731234  0.731234  0.731234
## V4     0.031119  0.031119  0.031119
## V5    -0.866793 -0.866793 -0.866793
## V6     0.564867  0.564867  0.564867
## V7     0.069678  0.069678  0.069678
## V8     0.358346  0.358346  0.358346
## V9     0.000000  0.000000  0.000000
## V10    0.070565  0.070565  0.070565
## V11    0.173464  0.173464  0.173464
## V12   -0.027472 -0.027472 -0.027472
## V13   -0.017960 -0.017960 -0.017960
## V14   -1.138053 -1.138053 -1.138053
## V15   -0.070990 -0.070990 -0.070990
## V16    0.000000  0.000000  0.000000
## V17    0.000000  0.000000  0.000000
## V18    0.000000  0.000000  0.000000
## V19    0.000000  0.000000  0.000000
## V20   -1.097528 -1.097528 -1.097528</code></pre>
</div>
<div id="references" class="section level2 unnumbered">
<h2 class="hasAnchor">
<a href="#references" class="anchor"></a>References</h2>
<div id="refs" class="references">
<div id="ref-glmnet">
<p>Friedman, Jerome, Trevor Hastie, and Robert Tibshirani. 2010. “Regularization Paths for Generalized Linear Models via Coordinate Descent.” <em>Journal of Statistical Software, Articles</em> 33 (1): 1–22. <a href="https://doi.org/10.18637/jss.v033.i01" class="uri">https://doi.org/10.18637/jss.v033.i01</a>.</p>
</div>
<div id="ref-block">
<p>Simon, Noah, Jerome Friedman, and Trevor Hastie. 2013. “A Blockwise Descent Algorithm for Group-Penalized Multiresponse and Multinomial Regression.”</p>
</div>
<div id="ref-coxnet">
<p>Simon, Noah, Jerome Friedman, Trevor Hastie, and Robert Tibshirani. 2011. “Regularization Paths for Cox’s Proportional Hazards Model via Coordinate Descent.” <em>Journal of Statistical Software, Articles</em> 39 (5): 1–13. <a href="https://doi.org/10.18637/jss.v039.i05" class="uri">https://doi.org/10.18637/jss.v039.i05</a>.</p>
</div>
<div id="ref-strongrules">
<p>Tibshirani, Robert, Jacob Bien, Jerome Friedman, Trevor Hastie, Noah Simon, Jonathan Taylor, and Ryan Tibshirani. 2012. “Strong Rules for Discarding Predictors in Lasso-Type Problems.” <em>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</em> 74 (2): 245–66. <a href="https://doi.org/10.1111/j.1467-9868.2011.01004.x" class="uri">https://doi.org/10.1111/j.1467-9868.2011.01004.x</a>.</p>
</div>
</div>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="sidebar">

        <div id="tocnav">
      <h2 class="hasAnchor">
<a href="#tocnav" class="anchor"></a>Contents</h2>
      <ul class="nav nav-pills nav-stacked">
<li><a href="#introduction">Introduction</a></li>
      <li><a href="#installation">Installation</a></li>
      <li><a href="#quick-start">Quick Start</a></li>
      <li><a href="#linear-regression">Linear Regression</a></li>
      <li><a href="#logistic-regression">Logistic Regression</a></li>
      <li><a href="#poisson-models">Poisson Models</a></li>
      <li><a href="#cox-models">Cox Models</a></li>
      <li><a href="#sparse-matrices">Sparse Matrices</a></li>
      <li><a href="#appendix-0-convergence-criteria">Appendix 0: Convergence Criteria</a></li>
      <li><a href="#appendix-1-internal-parameters">Appendix 1: Internal Parameters</a></li>
      <li><a href="#appendix-2-comparison-with-other-packages">Appendix 2: Comparison with Other Packages</a></li>
      <li><a href="#references">References</a></li>
      </ul>
</div>
      </div>

</div>



      <footer><div class="copyright">
  <p>Developed by Jerome Friedman, Trevor Hastie, Rob Tibshirani, Balasubramanian Narasimhan, Noah Simon.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="https://pkgdown.r-lib.org/">pkgdown</a> 1.4.1.</p>
</div>

      </footer>
</div>

  


  </body>
</html>
