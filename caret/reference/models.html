<!-- Generated by pkgdown: do not edit by hand -->
<!DOCTYPE html>
<html lang="en">
  <head>
  <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<title>A List of Available Models in train — train_model_list • caret</title>


<!-- jquery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js" integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8=" crossorigin="anonymous"></script>
<!-- Bootstrap -->

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha256-916EbMg70RQy9LHiGkXzG8hSg9EdNy97GazNG/aiY1w=" crossorigin="anonymous" />

<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha256-U5ZEeKfGNOja007MMD3YBI0A3OSZOQbeG6z2f2Y0hu8=" crossorigin="anonymous"></script>

<!-- Font Awesome icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.7.1/css/all.min.css" integrity="sha256-nAmazAk6vS34Xqo0BSrTb+abbtFlgsFK7NKSi6o7Y78=" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.7.1/css/v4-shims.min.css" integrity="sha256-6qHlizsOWFskGlwVOKuns+D1nB6ssZrHQrNj1wGplHc=" crossorigin="anonymous" />

<!-- clipboard.js -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.4/clipboard.min.js" integrity="sha256-FiZwavyI2V6+EXO1U+xzLG3IKldpiTFf3153ea9zikQ=" crossorigin="anonymous"></script>

<!-- headroom.js -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.9.4/headroom.min.js" integrity="sha256-DJFC1kqIhelURkuza0AvYal5RxMtpzLjFhsnVIeuk+U=" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.9.4/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script>

<!-- pkgdown -->
<link href="../pkgdown.css" rel="stylesheet">
<script src="../pkgdown.js"></script>




<meta property="og:title" content="A List of Available Models in train — train_model_list" />
<meta property="og:description" content="These models are included in the package via wrappers for train. Custom models can also be created. See the URL below.
AdaBoost Classification Trees (method = 'adaboost')
For classification using package fastAdaboost with tuning parameters:
Number of Trees (nIter, numeric)
Method (method, character)



AdaBoost.M1 (method = 'AdaBoost.M1')
For classification using packages adabag and plyr with tuning parameters:
Number of Trees (mfinal, numeric)
Max Tree Depth (maxdepth, numeric)
Coefficient Type (coeflearn, character)



Adaptive Mixture Discriminant Analysis (method = 'amdai')
For classification using package adaptDA with tuning parameters:
Model Type (model, character)



Adaptive-Network-Based Fuzzy Inference System (method = 'ANFIS')
For regression using package frbs with tuning parameters:
Number of Fuzzy Terms (num.labels, numeric)
Max. Iterations (max.iter, numeric)



Adjacent Categories Probability Model for Ordinal Data (method = 'vglmAdjCat')
For classification using package VGAM with tuning parameters:
Parallel Curves (parallel, logical)
Link Function (link, character)



Bagged AdaBoost (method = 'AdaBag')
For classification using packages adabag and plyr with tuning parameters:
Number of Trees (mfinal, numeric)
Max Tree Depth (maxdepth, numeric)



Bagged CART (method = 'treebag')
For classification and regression using packages ipred, plyr and e1071 with no tuning parameters.

Bagged FDA using gCV Pruning (method = 'bagFDAGCV')
For classification using package earth with tuning parameters:
Product Degree (degree, numeric)


Note: Unlike other packages used by train, the earth package is fully loaded when this model is used.
Bagged Flexible Discriminant Analysis (method = 'bagFDA')
For classification using packages earth and mda with tuning parameters:
Product Degree (degree, numeric)
Number of Terms (nprune, numeric)


Note: Unlike other packages used by train, the earth package is fully loaded when this model is used.
Bagged Logic Regression (method = 'logicBag')
For classification and regression using package logicFS with tuning parameters:
Maximum Number of Leaves (nleaves, numeric)
Number of Trees (ntrees, numeric)


Note: Unlike other packages used by train, the logicFS package is fully loaded when this model is used.
Bagged MARS (method = 'bagEarth')
For classification and regression using package earth with tuning parameters:
Number of Terms (nprune, numeric)
Product Degree (degree, numeric)


Note: Unlike other packages used by train, the earth package is fully loaded when this model is used.
Bagged MARS using gCV Pruning (method = 'bagEarthGCV')
For classification and regression using package earth with tuning parameters:
Product Degree (degree, numeric)


Note: Unlike other packages used by train, the earth package is fully loaded when this model is used.
Bagged Model (method = 'bag')
For classification and regression using package caret with tuning parameters:
Number of Randomly Selected Predictors (vars, numeric)



Bayesian Additive Regression Trees (method = 'bartMachine')
For classification and regression using package bartMachine with tuning parameters:
Number of Trees (num_trees, numeric)
Prior Boundary (k, numeric)
Base Terminal Node Hyperparameter (alpha, numeric)
Power Terminal Node Hyperparameter (beta, numeric)
Degrees of Freedom (nu, numeric)



Bayesian Generalized Linear Model (method = 'bayesglm')
For classification and regression using package arm with no tuning parameters.

Bayesian Regularized Neural Networks (method = 'brnn')
For regression using package brnn with tuning parameters:
Number of  Neurons (neurons, numeric)



Bayesian Ridge Regression (method = 'bridge')
For regression using package monomvn with no tuning parameters.

Bayesian Ridge Regression (Model Averaged) (method = 'blassoAveraged')
For regression using package monomvn with no tuning parameters.
Note: This model makes predictions by averaging the predictions based on the posterior estimates of the regression coefficients. While it is possible that some of these posterior estimates are zero for non-informative predictors, the final predicted value may be a function of many (or even all) predictors.
Binary Discriminant Analysis (method = 'binda')
For classification using package binda with tuning parameters:
Shrinkage Intensity (lambda.freqs, numeric)



Boosted Classification Trees (method = 'ada')
For classification using packages ada and plyr with tuning parameters:
Number of Trees (iter, numeric)
Max Tree Depth (maxdepth, numeric)
Learning Rate (nu, numeric)



Boosted Generalized Additive Model (method = 'gamboost')
For classification and regression using packages mboost, plyr and import with tuning parameters:
Number of  Boosting Iterations (mstop, numeric)
AIC Prune? (prune, character)


Note: The prune option for this model enables the number of iterations to be determined by the optimal AIC value across all iterations. See the examples in ?mboost::mstop. If pruning is not used, the ensemble makes predictions using the exact value of the mstop tuning parameter value.
Boosted Generalized Linear Model (method = 'glmboost')
For classification and regression using packages plyr and mboost with tuning parameters:
Number of  Boosting Iterations (mstop, numeric)
AIC Prune? (prune, character)


Note: The prune option for this model enables the number of iterations to be determined by the optimal AIC value across all iterations. See the examples in ?mboost::mstop. If pruning is not used, the ensemble makes predictions using the exact value of the mstop tuning parameter value.
Boosted Linear Model (method = 'BstLm')
For classification and regression using packages bst and plyr with tuning parameters:
Number of  Boosting Iterations (mstop, numeric)
Shrinkage (nu, numeric)



Boosted Logistic Regression (method = 'LogitBoost')
For classification using package caTools with tuning parameters:
Number of  Boosting Iterations (nIter, numeric)



Boosted Smoothing Spline (method = 'bstSm')
For classification and regression using packages bst and plyr with tuning parameters:
Number of  Boosting Iterations (mstop, numeric)
Shrinkage (nu, numeric)



Boosted Tree (method = 'blackboost')
For classification and regression using packages party, mboost and plyr with tuning parameters:
Number of Trees (mstop, numeric)
Max Tree Depth (maxdepth, numeric)



Boosted Tree (method = 'bstTree')
For classification and regression using packages bst and plyr with tuning parameters:
Number of  Boosting Iterations (mstop, numeric)
Max Tree Depth (maxdepth, numeric)
Shrinkage (nu, numeric)



C4.5-like Trees (method = 'J48')
For classification using package RWeka with tuning parameters:
Confidence Threshold (C, numeric)
Minimum Instances Per Leaf (M, numeric)



C5.0 (method = 'C5.0')
For classification using packages C50 and plyr with tuning parameters:
Number of  Boosting Iterations (trials, numeric)
Model Type (model, character)
Winnow (winnow, logical)



CART (method = 'rpart')
For classification and regression using package rpart with tuning parameters:
Complexity Parameter (cp, numeric)



CART (method = 'rpart1SE')
For classification and regression using package rpart with no tuning parameters.
Note: This CART model replicates the same process used by the rpart function where the model complexity is determined using the one-standard error method. This procedure is replicated inside of the resampling done by train so that an external resampling estimate can be obtained.
CART (method = 'rpart2')
For classification and regression using package rpart with tuning parameters:
Max Tree Depth (maxdepth, numeric)



CART or Ordinal Responses (method = 'rpartScore')
For classification using packages rpartScore and plyr with tuning parameters:
Complexity Parameter (cp, numeric)
Split Function (split, character)
Pruning Measure (prune, character)



CHi-squared Automated Interaction Detection (method = 'chaid')
For classification using package CHAID with tuning parameters:
Merging Threshold (alpha2, numeric)
Splitting former Merged Threshold (alpha3, numeric)
Splitting former Merged Threshold (alpha4, numeric)



Conditional Inference Random Forest (method = 'cforest')
For classification and regression using package party with tuning parameters:
Number of Randomly Selected Predictors (mtry, numeric)



Conditional Inference Tree (method = 'ctree')
For classification and regression using package party with tuning parameters:
1 - P-Value Threshold (mincriterion, numeric)



Conditional Inference Tree (method = 'ctree2')
For classification and regression using package party with tuning parameters:
Max Tree Depth (maxdepth, numeric)
1 - P-Value Threshold (mincriterion, numeric)



Continuation Ratio Model for Ordinal Data (method = 'vglmContRatio')
For classification using package VGAM with tuning parameters:
Parallel Curves (parallel, logical)
Link Function (link, character)



Cost-Sensitive C5.0 (method = 'C5.0Cost')
For classification using packages C50 and plyr with tuning parameters:
Number of  Boosting Iterations (trials, numeric)
Model Type (model, character)
Winnow (winnow, logical)
Cost (cost, numeric)



Cost-Sensitive CART (method = 'rpartCost')
For classification using packages rpart and plyr with tuning parameters:
Complexity Parameter (cp, numeric)
Cost (Cost, numeric)



Cubist (method = 'cubist')
For regression using package Cubist with tuning parameters:
Number of Committees (committees, numeric)
Number of Instances (neighbors, numeric)



Cumulative Probability Model for Ordinal Data (method = 'vglmCumulative')
For classification using package VGAM with tuning parameters:
Parallel Curves (parallel, logical)
Link Function (link, character)



DeepBoost (method = 'deepboost')
For classification using package deepboost with tuning parameters:
Number of  Boosting Iterations (num_iter, numeric)
Tree Depth (tree_depth, numeric)
L1 Regularization (beta, numeric)
Tree Depth Regularization (lambda, numeric)
Loss (loss_type, character)



Diagonal Discriminant Analysis (method = 'dda')
For classification using package sparsediscrim with tuning parameters:
Model (model, character)
Shrinkage Type (shrinkage, character)



Distance Weighted Discrimination with Polynomial Kernel (method = 'dwdPoly')
For classification using package kerndwd with tuning parameters:
Regularization Parameter (lambda, numeric)
q (qval, numeric)
Polynomial Degree (degree, numeric)
Scale (scale, numeric)



Distance Weighted Discrimination with Radial Basis Function Kernel (method = 'dwdRadial')
For classification using packages kernlab and kerndwd with tuning parameters:
Regularization Parameter (lambda, numeric)
q (qval, numeric)
Sigma (sigma, numeric)



Dynamic Evolving Neural-Fuzzy Inference System  (method = 'DENFIS')
For regression using package frbs with tuning parameters:
Threshold (Dthr, numeric)
Max. Iterations (max.iter, numeric)



Elasticnet (method = 'enet')
For regression using package elasticnet with tuning parameters:
Fraction of Full Solution (fraction, numeric)
Weight Decay (lambda, numeric)



Ensembles of Generalized Linear Models (method = 'randomGLM')
For classification and regression using package randomGLM with tuning parameters:
Interaction Order (maxInteractionOrder, numeric)


Note: Unlike other packages used by train, the randomGLM package is fully loaded when this model is used.
eXtreme Gradient Boosting (method = 'xgbDART')
For classification and regression using packages xgboost and plyr with tuning parameters:
Number of  Boosting Iterations (nrounds, numeric)
Max Tree Depth (max_depth, numeric)
Shrinkage (eta, numeric)
Minimum Loss Reduction (gamma, numeric)
Subsample Percentage (subsample, numeric)
Subsample Ratio of Columns (colsample_bytree, numeric)
Fraction of Trees Dropped (rate_drop, numeric)
Prob. of Skipping Drop-out (skip_drop, numeric)
Minimum Sum of Instance Weight (min_child_weight, numeric)



eXtreme Gradient Boosting (method = 'xgbLinear')
For classification and regression using package xgboost with tuning parameters:
Number of  Boosting Iterations (nrounds, numeric)
L2 Regularization (lambda, numeric)
L1 Regularization (alpha, numeric)
Learning Rate (eta, numeric)



eXtreme Gradient Boosting (method = 'xgbTree')
For classification and regression using packages xgboost and plyr with tuning parameters:
Number of  Boosting Iterations (nrounds, numeric)
Max Tree Depth (max_depth, numeric)
Shrinkage (eta, numeric)
Minimum Loss Reduction (gamma, numeric)
Subsample Ratio of Columns (colsample_bytree, numeric)
Minimum Sum of Instance Weight (min_child_weight, numeric)
Subsample Percentage (subsample, numeric)



Extreme Learning Machine (method = 'elm')
For classification and regression using package elmNN with tuning parameters:
Number of Hidden Units (nhid, numeric)
Activation Function (actfun, character)



Factor-Based Linear Discriminant Analysis (method = 'RFlda')
For classification using package HiDimDA with tuning parameters:
Number of  Factors (q, numeric)



Flexible Discriminant Analysis (method = 'fda')
For classification using packages earth and mda with tuning parameters:
Product Degree (degree, numeric)
Number of Terms (nprune, numeric)


Note: Unlike other packages used by train, the earth package is fully loaded when this model is used.
Fuzzy Inference Rules by Descent Method (method = 'FIR.DM')
For regression using package frbs with tuning parameters:
Number of Fuzzy Terms (num.labels, numeric)
Max. Iterations (max.iter, numeric)



Fuzzy Rules Using Chi's Method (method = 'FRBCS.CHI')
For classification using package frbs with tuning parameters:
Number of Fuzzy Terms (num.labels, numeric)
Membership Function (type.mf, character)



Fuzzy Rules Using Genetic Cooperative-Competitive Learning and Pittsburgh (method = 'FH.GBML')
For classification using package frbs with tuning parameters:
Max. Number of Rules (max.num.rule, numeric)
Population Size (popu.size, numeric)
Max. Generations (max.gen, numeric)



Fuzzy Rules Using the Structural Learning Algorithm on Vague Environment (method = 'SLAVE')
For classification using package frbs with tuning parameters:
Number of Fuzzy Terms (num.labels, numeric)
Max. Iterations (max.iter, numeric)
Max. Generations (max.gen, numeric)



Fuzzy Rules via MOGUL (method = 'GFS.FR.MOGUL')
For regression using package frbs with tuning parameters:
Max. Generations (max.gen, numeric)
Max. Iterations (max.iter, numeric)
Max. Tuning Iterations (max.tune, numeric)



Fuzzy Rules via Thrift (method = 'GFS.THRIFT')
For regression using package frbs with tuning parameters:
Population Size (popu.size, numeric)
Number of  Fuzzy Labels (num.labels, numeric)
Max. Generations (max.gen, numeric)



Fuzzy Rules with Weight Factor (method = 'FRBCS.W')
For classification using package frbs with tuning parameters:
Number of Fuzzy Terms (num.labels, numeric)
Membership Function (type.mf, character)



Gaussian Process (method = 'gaussprLinear')
For classification and regression using package kernlab with no tuning parameters.

Gaussian Process with Polynomial Kernel (method = 'gaussprPoly')
For classification and regression using package kernlab with tuning parameters:
Polynomial Degree (degree, numeric)
Scale (scale, numeric)



Gaussian Process with Radial Basis Function Kernel (method = 'gaussprRadial')
For classification and regression using package kernlab with tuning parameters:
Sigma (sigma, numeric)



Generalized Additive Model using LOESS (method = 'gamLoess')
For classification and regression using package gam with tuning parameters:
Span (span, numeric)
Degree (degree, numeric)


Note: Which terms enter the model in a nonlinear manner is determined by the number of unique values for the predictor. For example, if a predictor only has four unique values, most basis expansion method will fail because there are not enough granularity in the data. By default, a predictor must have at least 10 unique values to be used in a nonlinear basis expansion. Unlike other packages used by train, the gam package is fully loaded when this model is used.
Generalized Additive Model using Splines (method = 'bam')
For classification and regression using package mgcv with tuning parameters:
Feature Selection (select, logical)
Method (method, character)


Note: Which terms enter the model in a nonlinear manner is determined by the number of unique values for the predictor. For example, if a predictor only has four unique values, most basis expansion method will fail because there are not enough granularity in the data. By default, a predictor must have at least 10 unique values to be used in a nonlinear basis expansion. Unlike other packages used by train, the mgcv package is fully loaded when this model is used.
Generalized Additive Model using Splines (method = 'gam')
For classification and regression using package mgcv with tuning parameters:
Feature Selection (select, logical)
Method (method, character)


Note: Which terms enter the model in a nonlinear manner is determined by the number of unique values for the predictor. For example, if a predictor only has four unique values, most basis expansion method will fail because there are not enough granularity in the data. By default, a predictor must have at least 10 unique values to be used in a nonlinear basis expansion. Unlike other packages used by train, the mgcv package is fully loaded when this model is used.
Generalized Additive Model using Splines (method = 'gamSpline')
For classification and regression using package gam with tuning parameters:
Degrees of Freedom (df, numeric)


Note: Which terms enter the model in a nonlinear manner is determined by the number of unique values for the predictor. For example, if a predictor only has four unique values, most basis expansion method will fail because there are not enough granularity in the data. By default, a predictor must have at least 10 unique values to be used in a nonlinear basis expansion. Unlike other packages used by train, the gam package is fully loaded when this model is used.
Generalized Linear Model (method = 'glm')
For classification and regression with no tuning parameters.

Generalized Linear Model with Stepwise Feature Selection (method = 'glmStepAIC')
For classification and regression using package MASS with no tuning parameters.

Generalized Partial Least Squares (method = 'gpls')
For classification using package gpls with tuning parameters:
Number of Components (K.prov, numeric)



Genetic Lateral Tuning and Rule Selection of Linguistic Fuzzy Systems (method = 'GFS.LT.RS')
For regression using package frbs with tuning parameters:
Population Size (popu.size, numeric)
Number of  Fuzzy Labels (num.labels, numeric)
Max. Generations (max.gen, numeric)



glmnet (method = 'glmnet_h2o')
For classification and regression using package h2o with tuning parameters:
Mixing Percentage (alpha, numeric)
Regularization Parameter (lambda, numeric)



glmnet (method = 'glmnet')
For classification and regression using packages glmnet and Matrix with tuning parameters:
Mixing Percentage (alpha, numeric)
Regularization Parameter (lambda, numeric)



Gradient Boosting Machines (method = 'gbm_h2o')
For classification and regression using package h2o with tuning parameters:
Number of  Boosting Iterations (ntrees, numeric)
Max Tree Depth (max_depth, numeric)
Min. Terminal Node Size (min_rows, numeric)
Shrinkage (learn_rate, numeric)
Number of Randomly Selected Predictors (col_sample_rate, numeric)



Greedy Prototype Selection (method = 'protoclass')
For classification using packages proxy and protoclass with tuning parameters:
Ball Size (eps, numeric)
Distance Order (Minkowski, numeric)



Heteroscedastic Discriminant Analysis (method = 'hda')
For classification using package hda with tuning parameters:
Gamma (gamma, numeric)
Lambda (lambda, numeric)
Dimension of the Discriminative Subspace (newdim, numeric)



High Dimensional Discriminant Analysis (method = 'hdda')
For classification using package HDclassif with tuning parameters:
Threshold (threshold, character)
Model Type (model, numeric)



High-Dimensional Regularized Discriminant Analysis (method = 'hdrda')
For classification using package sparsediscrim with tuning parameters:
Gamma (gamma, numeric)
Lambda (lambda, numeric)
Shrinkage Type (shrinkage_type, character)



Hybrid Neural Fuzzy Inference System (method = 'HYFIS')
For regression using package frbs with tuning parameters:
Number of Fuzzy Terms (num.labels, numeric)
Max. Iterations (max.iter, numeric)



Independent Component Regression (method = 'icr')
For regression using package fastICA with tuning parameters:
Number of Components (n.comp, numeric)



k-Nearest Neighbors (method = 'kknn')
For classification and regression using package kknn with tuning parameters:
Max. Number of Neighbors (kmax, numeric)
Distance (distance, numeric)
Kernel (kernel, character)



k-Nearest Neighbors (method = 'knn')
For classification and regression with tuning parameters:
Number of Neighbors (k, numeric)



L2 Regularized Linear Support Vector Machines with Class Weights (method = 'svmLinearWeights2')
For classification using package LiblineaR with tuning parameters:
Cost (cost, numeric)
Loss Function (Loss, character)
Class Weight (weight, numeric)



L2 Regularized Support Vector Machine (dual) with Linear Kernel (method = 'svmLinear3')
For classification and regression using package LiblineaR with tuning parameters:
Cost (cost, numeric)
Loss Function (Loss, character)



Learning Vector Quantization (method = 'lvq')
For classification using package class with tuning parameters:
Codebook Size (size, numeric)
Number of Prototypes (k, numeric)



Least Angle Regression (method = 'lars')
For regression using package lars with tuning parameters:
Fraction (fraction, numeric)



Least Angle Regression (method = 'lars2')
For regression using package lars with tuning parameters:
Number of Steps (step, numeric)



Least Squares Support Vector Machine (method = 'lssvmLinear')
For classification using package kernlab with tuning parameters:
Regularization Parameter (tau, numeric)



Least Squares Support Vector Machine with Polynomial Kernel (method = 'lssvmPoly')
For classification using package kernlab with tuning parameters:
Polynomial Degree (degree, numeric)
Scale (scale, numeric)
Regularization Parameter (tau, numeric)



Least Squares Support Vector Machine with Radial Basis Function Kernel (method = 'lssvmRadial')
For classification using package kernlab with tuning parameters:
Sigma (sigma, numeric)
Regularization Parameter (tau, numeric)



Linear Discriminant Analysis (method = 'lda')
For classification using package MASS with no tuning parameters.

Linear Discriminant Analysis (method = 'lda2')
For classification using package MASS with tuning parameters:
Number of Discriminant Functions (dimen, numeric)



Linear Discriminant Analysis with Stepwise Feature Selection (method = 'stepLDA')
For classification using packages klaR and MASS with tuning parameters:
Maximum Number of Variables (maxvar, numeric)
Search Direction (direction, character)



Linear Distance Weighted Discrimination (method = 'dwdLinear')
For classification using package kerndwd with tuning parameters:
Regularization Parameter (lambda, numeric)
q (qval, numeric)



Linear Regression (method = 'lm')
For regression with tuning parameters:
intercept (intercept, logical)



Linear Regression with Backwards Selection (method = 'leapBackward')
For regression using package leaps with tuning parameters:
Maximum Number of Predictors (nvmax, numeric)



Linear Regression with Forward Selection (method = 'leapForward')
For regression using package leaps with tuning parameters:
Maximum Number of Predictors (nvmax, numeric)



Linear Regression with Stepwise Selection (method = 'leapSeq')
For regression using package leaps with tuning parameters:
Maximum Number of Predictors (nvmax, numeric)



Linear Regression with Stepwise Selection (method = 'lmStepAIC')
For regression using package MASS with no tuning parameters.

Linear Support Vector Machines with Class Weights (method = 'svmLinearWeights')
For classification using package e1071 with tuning parameters:
Cost (cost, numeric)
Class Weight (weight, numeric)



Localized Linear Discriminant Analysis (method = 'loclda')
For classification using package klaR with tuning parameters:
Number of Nearest Neighbors (k, numeric)



Logic Regression (method = 'logreg')
For classification and regression using package LogicReg with tuning parameters:
Maximum Number of Leaves (treesize, numeric)
Number of Trees (ntrees, numeric)



Logistic Model Trees (method = 'LMT')
For classification using package RWeka with tuning parameters:
Number of  Iteratons (iter, numeric)



Maximum Uncertainty Linear Discriminant Analysis (method = 'Mlda')
For classification using package HiDimDA with no tuning parameters.

Mixture Discriminant Analysis (method = 'mda')
For classification using package mda with tuning parameters:
Number of Subclasses Per Class (subclasses, numeric)



Model Averaged Naive Bayes Classifier (method = 'manb')
For classification using package bnclassify with tuning parameters:
Smoothing Parameter (smooth, numeric)
Prior Probability (prior, numeric)



Model Averaged Neural Network (method = 'avNNet')
For classification and regression using package nnet with tuning parameters:
Number of Hidden Units (size, numeric)
Weight Decay (decay, numeric)
Bagging (bag, logical)



Model Rules (method = 'M5Rules')
For regression using package RWeka with tuning parameters:
Pruned (pruned, character)
Smoothed (smoothed, character)



Model Tree (method = 'M5')
For regression using package RWeka with tuning parameters:
Pruned (pruned, character)
Smoothed (smoothed, character)
Rules (rules, character)



Monotone Multi-Layer Perceptron Neural Network (method = 'monmlp')
For classification and regression using package monmlp with tuning parameters:
Number of Hidden Units (hidden1, numeric)
Number of Models (n.ensemble, numeric)



Multi-Layer Perceptron (method = 'mlp')
For classification and regression using package RSNNS with tuning parameters:
Number of Hidden Units (size, numeric)



Multi-Layer Perceptron (method = 'mlpWeightDecay')
For classification and regression using package RSNNS with tuning parameters:
Number of Hidden Units (size, numeric)
Weight Decay (decay, numeric)



Multi-Layer Perceptron, multiple layers (method = 'mlpWeightDecayML')
For classification and regression using package RSNNS with tuning parameters:
Number of Hidden Units layer1 (layer1, numeric)
Number of Hidden Units layer2 (layer2, numeric)
Number of Hidden Units layer3 (layer3, numeric)
Weight Decay (decay, numeric)



Multi-Layer Perceptron, with multiple layers (method = 'mlpML')
For classification and regression using package RSNNS with tuning parameters:
Number of Hidden Units layer1 (layer1, numeric)
Number of Hidden Units layer2 (layer2, numeric)
Number of Hidden Units layer3 (layer3, numeric)



Multi-Step Adaptive MCP-Net (method = 'msaenet')
For classification and regression using package msaenet with tuning parameters:
Alpha (alphas, numeric)
Number of Adaptive Estimation Steps (nsteps, numeric)
Adaptive Weight Scaling Factor (scale, numeric)



Multilayer Perceptron Network by Stochastic Gradient Descent (method = 'mlpSGD')
For classification and regression using packages FCNN4R and plyr with tuning parameters:
Number of Hidden Units (size, numeric)
L2 Regularization (l2reg, numeric)
RMSE Gradient Scaling (lambda, numeric)
Learning Rate (learn_rate, numeric)
Momentum (momentum, numeric)
Learning Rate Decay (gamma, numeric)
Batch Size (minibatchsz, numeric)
Number of Models (repeats, numeric)



Multilayer Perceptron Network with Dropout (method = 'mlpKerasDropout')
For classification and regression using package keras with tuning parameters:
Number of Hidden Units (size, numeric)
Dropout Rate (dropout, numeric)
Batch Size (batch_size, numeric)
Learning Rate (lr, numeric)
Rho (rho, numeric)
Learning Rate Decay (decay, numeric)
Activation Function (activation, character)


Note: After train completes, the keras model object is serialized so that it can be used between R session. When predicting, the code will temporarily unsearalize the object. To make the predictions more efficient, the user might want to use keras::unsearlize_model(object$finalModel$object) in the current R session so that that operation is only done once. Also, this model cannot be run in parallel due to the nature of how tensorflow does the computations. Unlike other packages used by train, the dplyr package is fully loaded when this model is used.
Multilayer Perceptron Network with Dropout (method = 'mlpKerasDropoutCost')
For classification using package keras with tuning parameters:
Number of Hidden Units (size, numeric)
Dropout Rate (dropout, numeric)
Batch Size (batch_size, numeric)
Learning Rate (lr, numeric)
Rho (rho, numeric)
Learning Rate Decay (decay, numeric)
Cost (cost, numeric)
Activation Function (activation, character)


Note: After train completes, the keras model object is serialized so that it can be used between R session. When predicting, the code will temporarily unsearalize the object. To make the predictions more efficient, the user might want to use keras::unsearlize_model(object$finalModel$object) in the current R session so that that operation is only done once. Also, this model cannot be run in parallel due to the nature of how tensorflow does the computations. Finally, the cost parameter weights the first class in the outcome vector. Unlike other packages used by train, the dplyr package is fully loaded when this model is used.
Multilayer Perceptron Network with Weight Decay (method = 'mlpKerasDecay')
For classification and regression using package keras with tuning parameters:
Number of Hidden Units (size, numeric)
L2 Regularization (lambda, numeric)
Batch Size (batch_size, numeric)
Learning Rate (lr, numeric)
Rho (rho, numeric)
Learning Rate Decay (decay, numeric)
Activation Function (activation, character)


Note: After train completes, the keras model object is serialized so that it can be used between R session. When predicting, the code will temporarily unsearalize the object. To make the predictions more efficient, the user might want to use keras::unsearlize_model(object$finalModel$object) in the current R session so that that operation is only done once. Also, this model cannot be run in parallel due to the nature of how tensorflow does the computations. Unlike other packages used by train, the dplyr package is fully loaded when this model is used.
Multilayer Perceptron Network with Weight Decay (method = 'mlpKerasDecayCost')
For classification using package keras with tuning parameters:
Number of Hidden Units (size, numeric)
L2 Regularization (lambda, numeric)
Batch Size (batch_size, numeric)
Learning Rate (lr, numeric)
Rho (rho, numeric)
Learning Rate Decay (decay, numeric)
Cost (cost, numeric)
Activation Function (activation, character)


Note: After train completes, the keras model object is serialized so that it can be used between R session. When predicting, the code will temporarily unsearalize the object. To make the predictions more efficient, the user might want to use keras::unsearlize_model(object$finalModel$object) in the current R session so that that operation is only done once. Also, this model cannot be run in parallel due to the nature of how tensorflow does the computations. Finally, the cost parameter weights the first class in the outcome vector. Unlike other packages used by train, the dplyr package is fully loaded when this model is used.
Multivariate Adaptive Regression Spline (method = 'earth')
For classification and regression using package earth with tuning parameters:
Number of Terms (nprune, numeric)
Product Degree (degree, numeric)


Note: Unlike other packages used by train, the earth package is fully loaded when this model is used.
Multivariate Adaptive Regression Splines (method = 'gcvEarth')
For classification and regression using package earth with tuning parameters:
Product Degree (degree, numeric)


Note: Unlike other packages used by train, the earth package is fully loaded when this model is used.
Naive Bayes (method = 'naive_bayes')
For classification using package naivebayes with tuning parameters:
Laplace Correction (laplace, numeric)
Distribution Type (usekernel, logical)
Bandwidth Adjustment (adjust, numeric)



Naive Bayes (method = 'nb')
For classification using package klaR with tuning parameters:
Laplace Correction (fL, numeric)
Distribution Type (usekernel, logical)
Bandwidth Adjustment (adjust, numeric)



Naive Bayes Classifier (method = 'nbDiscrete')
For classification using package bnclassify with tuning parameters:
Smoothing Parameter (smooth, numeric)



Naive Bayes Classifier with Attribute Weighting (method = 'awnb')
For classification using package bnclassify with tuning parameters:
Smoothing Parameter (smooth, numeric)



Nearest Shrunken Centroids (method = 'pam')
For classification using package pamr with tuning parameters:
Shrinkage Threshold (threshold, numeric)



Negative Binomial Generalized Linear Model (method = 'glm.nb')
For regression using package MASS with tuning parameters:
Link Function (link, character)



Neural Network (method = 'mxnet')
For classification and regression using package mxnet with tuning parameters:
Number of Hidden Units in Layer 1 (layer1, numeric)
Number of Hidden Units in Layer 2 (layer2, numeric)
Number of Hidden Units in Layer 3 (layer3, numeric)
Learning Rate (learning.rate, numeric)
Momentum (momentum, numeric)
Dropout Rate (dropout, numeric)
Activation Function (activation, character)


Note: The mxnet package is not yet on CRAN. See http://mxnet.io for installation instructions.
Neural Network (method = 'mxnetAdam')
For classification and regression using package mxnet with tuning parameters:
Number of Hidden Units in Layer 1 (layer1, numeric)
Number of Hidden Units in Layer 2 (layer2, numeric)
Number of Hidden Units in Layer 3 (layer3, numeric)
Dropout Rate (dropout, numeric)
beta1 (beta1, numeric)
beta2 (beta2, numeric)
Learning Rate (learningrate, numeric)
Activation Function (activation, character)


Note: The mxnet package is not yet on CRAN. See http://mxnet.io for installation instructions. Users are strongly advised to define num.round themselves.
Neural Network (method = 'neuralnet')
For regression using package neuralnet with tuning parameters:
Number of Hidden Units in Layer 1 (layer1, numeric)
Number of Hidden Units in Layer 2 (layer2, numeric)
Number of Hidden Units in Layer 3 (layer3, numeric)



Neural Network (method = 'nnet')
For classification and regression using package nnet with tuning parameters:
Number of Hidden Units (size, numeric)
Weight Decay (decay, numeric)



Neural Networks with Feature Extraction (method = 'pcaNNet')
For classification and regression using package nnet with tuning parameters:
Number of Hidden Units (size, numeric)
Weight Decay (decay, numeric)



Non-Convex Penalized Quantile Regression (method = 'rqnc')
For regression using package rqPen with tuning parameters:
L1 Penalty (lambda, numeric)
Penalty Type (penalty, character)



Non-Informative Model (method = 'null')
For classification and regression with no tuning parameters.
Note: Since this model always predicts the same value, R-squared values will always be estimated to be NA.
Non-Negative Least Squares (method = 'nnls')
For regression using package nnls with no tuning parameters.

Oblique Random Forest (method = 'ORFlog')
For classification using package obliqueRF with tuning parameters:
Number of Randomly Selected Predictors (mtry, numeric)


Note: Unlike other packages used by train, the obliqueRF package is fully loaded when this model is used.
Oblique Random Forest (method = 'ORFpls')
For classification using package obliqueRF with tuning parameters:
Number of Randomly Selected Predictors (mtry, numeric)


Note: Unlike other packages used by train, the obliqueRF package is fully loaded when this model is used.
Oblique Random Forest (method = 'ORFridge')
For classification using package obliqueRF with tuning parameters:
Number of Randomly Selected Predictors (mtry, numeric)


Note: Unlike other packages used by train, the obliqueRF package is fully loaded when this model is used.
Oblique Random Forest (method = 'ORFsvm')
For classification using package obliqueRF with tuning parameters:
Number of Randomly Selected Predictors (mtry, numeric)


Note: Unlike other packages used by train, the obliqueRF package is fully loaded when this model is used.
Optimal Weighted Nearest Neighbor Classifier (method = 'ownn')
For classification using package snn with tuning parameters:
Number of Neighbors (K, numeric)



Ordered Logistic or Probit Regression (method = 'polr')
For classification using package MASS with tuning parameters:
parameter (method, character)



Parallel Random Forest (method = 'parRF')
For classification and regression using packages e1071, randomForest, foreach and import with tuning parameters:
Number of Randomly Selected Predictors (mtry, numeric)



partDSA (method = 'partDSA')
For classification and regression using package partDSA with tuning parameters:
Number of Terminal Partitions (cut.off.growth, numeric)
Minimum Percent Difference (MPD, numeric)



Partial Least Squares (method = 'kernelpls')
For classification and regression using package pls with tuning parameters:
Number of Components (ncomp, numeric)



Partial Least Squares (method = 'pls')
For classification and regression using package pls with tuning parameters:
Number of Components (ncomp, numeric)



Partial Least Squares (method = 'simpls')
For classification and regression using package pls with tuning parameters:
Number of Components (ncomp, numeric)



Partial Least Squares (method = 'widekernelpls')
For classification and regression using package pls with tuning parameters:
Number of Components (ncomp, numeric)



Partial Least Squares Generalized Linear Models  (method = 'plsRglm')
For classification and regression using package plsRglm with tuning parameters:
Number of PLS Components (nt, numeric)
p-Value threshold (alpha.pvals.expli, numeric)


Note: Unlike other packages used by train, the plsRglm package is fully loaded when this model is used.
Patient Rule Induction Method (method = 'PRIM')
For classification using package supervisedPRIM with tuning parameters:
peeling quantile (peel.alpha, numeric)
pasting quantile (paste.alpha, numeric)
minimum mass (mass.min, numeric)



Penalized Discriminant Analysis (method = 'pda')
For classification using package mda with tuning parameters:
Shrinkage Penalty Coefficient (lambda, numeric)



Penalized Discriminant Analysis (method = 'pda2')
For classification using package mda with tuning parameters:
Degrees of Freedom (df, numeric)



Penalized Linear Discriminant Analysis (method = 'PenalizedLDA')
For classification using packages penalizedLDA and plyr with tuning parameters:
L1 Penalty (lambda, numeric)
Number of Discriminant Functions (K, numeric)



Penalized Linear Regression (method = 'penalized')
For regression using package penalized with tuning parameters:
L1 Penalty (lambda1, numeric)
L2 Penalty (lambda2, numeric)



Penalized Logistic Regression (method = 'plr')
For classification using package stepPlr with tuning parameters:
L2 Penalty (lambda, numeric)
Complexity Parameter (cp, character)



Penalized Multinomial Regression (method = 'multinom')
For classification using package nnet with tuning parameters:
Weight Decay (decay, numeric)



Penalized Ordinal Regression (method = 'ordinalNet')
For classification using packages ordinalNet and plyr with tuning parameters:
Mixing Percentage (alpha, numeric)
Selection Criterion (criteria, character)
Link Function (link, character)


Note: Requires ordinalNet package version &amp;gt;= 2.0
Polynomial Kernel Regularized Least Squares (method = 'krlsPoly')
For regression using package KRLS with tuning parameters:
Regularization Parameter (lambda, numeric)
Polynomial Degree (degree, numeric)



Principal Component Analysis (method = 'pcr')
For regression using package pls with tuning parameters:
Number of Components (ncomp, numeric)



Projection Pursuit Regression (method = 'ppr')
For regression with tuning parameters:
Number of  Terms (nterms, numeric)



Quadratic Discriminant Analysis (method = 'qda')
For classification using package MASS with no tuning parameters.

Quadratic Discriminant Analysis with Stepwise Feature Selection (method = 'stepQDA')
For classification using packages klaR and MASS with tuning parameters:
Maximum Number of Variables (maxvar, numeric)
Search Direction (direction, character)



Quantile Random Forest (method = 'qrf')
For regression using package quantregForest with tuning parameters:
Number of Randomly Selected Predictors (mtry, numeric)



Quantile Regression Neural Network (method = 'qrnn')
For regression using package qrnn with tuning parameters:
Number of Hidden Units (n.hidden, numeric)
Weight Decay (penalty, numeric)
Bagged Models? (bag, logical)



Quantile Regression with LASSO penalty (method = 'rqlasso')
For regression using package rqPen with tuning parameters:
L1 Penalty (lambda, numeric)



Radial Basis Function Kernel Regularized Least Squares (method = 'krlsRadial')
For regression using packages KRLS and kernlab with tuning parameters:
Regularization Parameter (lambda, numeric)
Sigma (sigma, numeric)



Radial Basis Function Network (method = 'rbf')
For classification and regression using package RSNNS with tuning parameters:
Number of Hidden Units (size, numeric)



Radial Basis Function Network (method = 'rbfDDA')
For classification and regression using package RSNNS with tuning parameters:
Activation Limit for Conflicting Classes (negativeThreshold, numeric)



Random Ferns (method = 'rFerns')
For classification using package rFerns with tuning parameters:
Fern Depth (depth, numeric)



Random Forest (method = 'ranger')
For classification and regression using packages e1071, ranger and dplyr with tuning parameters:
Number of Randomly Selected Predictors (mtry, numeric)
Splitting Rule (splitrule, character)
Minimal Node Size (min.node.size, numeric)



Random Forest (method = 'Rborist')
For classification and regression using package Rborist with tuning parameters:
Number of Randomly Selected Predictors (predFixed, numeric)
Minimal Node Size (minNode, numeric)



Random Forest (method = 'rf')
For classification and regression using package randomForest with tuning parameters:
Number of Randomly Selected Predictors (mtry, numeric)



Random Forest by Randomization (method = 'extraTrees')
For classification and regression using package extraTrees with tuning parameters:
Number of  Randomly Selected Predictors (mtry, numeric)
Number of  Random Cuts (numRandomCuts, numeric)



Random Forest Rule-Based Model (method = 'rfRules')
For classification and regression using packages randomForest, inTrees and plyr with tuning parameters:
Number of Randomly Selected Predictors (mtry, numeric)
Maximum Rule Depth (maxdepth, numeric)



Regularized Discriminant Analysis (method = 'rda')
For classification using package klaR with tuning parameters:
Gamma (gamma, numeric)
Lambda (lambda, numeric)



Regularized Linear Discriminant Analysis (method = 'rlda')
For classification using package sparsediscrim with tuning parameters:
Regularization Method (estimator, character)



Regularized Logistic Regression (method = 'regLogistic')
For classification using package LiblineaR with tuning parameters:
Cost (cost, numeric)
Loss Function (loss, character)
Tolerance (epsilon, numeric)



Regularized Random Forest (method = 'RRF')
For classification and regression using packages randomForest and RRF with tuning parameters:
Number of Randomly Selected Predictors (mtry, numeric)
Regularization Value (coefReg, numeric)
Importance Coefficient (coefImp, numeric)



Regularized Random Forest (method = 'RRFglobal')
For classification and regression using package RRF with tuning parameters:
Number of Randomly Selected Predictors (mtry, numeric)
Regularization Value (coefReg, numeric)



Relaxed Lasso (method = 'relaxo')
For regression using packages relaxo and plyr with tuning parameters:
Penalty Parameter (lambda, numeric)
Relaxation Parameter (phi, numeric)



Relevance Vector Machines with Linear Kernel (method = 'rvmLinear')
For regression using package kernlab with no tuning parameters.

Relevance Vector Machines with Polynomial Kernel (method = 'rvmPoly')
For regression using package kernlab with tuning parameters:
Scale (scale, numeric)
Polynomial Degree (degree, numeric)



Relevance Vector Machines with Radial Basis Function Kernel (method = 'rvmRadial')
For regression using package kernlab with tuning parameters:
Sigma (sigma, numeric)



Ridge Regression (method = 'ridge')
For regression using package elasticnet with tuning parameters:
Weight Decay (lambda, numeric)



Ridge Regression with Variable Selection (method = 'foba')
For regression using package foba with tuning parameters:
Number of Variables Retained (k, numeric)
L2 Penalty (lambda, numeric)



Robust Linear Discriminant Analysis (method = 'Linda')
For classification using package rrcov with no tuning parameters.

Robust Linear Model (method = 'rlm')
For regression using package MASS with tuning parameters:
intercept (intercept, logical)
psi (psi, character)



Robust Mixture Discriminant Analysis (method = 'rmda')
For classification using package robustDA with tuning parameters:
Number of Subclasses Per Class (K, numeric)
Model (model, character)



Robust Quadratic Discriminant Analysis (method = 'QdaCov')
For classification using package rrcov with no tuning parameters.

Robust Regularized Linear Discriminant Analysis (method = 'rrlda')
For classification using package rrlda with tuning parameters:
Penalty Parameter (lambda, numeric)
Robustness Parameter (hp, numeric)
Penalty Type (penalty, character)


Note: Unlike other packages used by train, the rrlda package is fully loaded when this model is used.
Robust SIMCA (method = 'RSimca')
For classification using package rrcovHD with no tuning parameters.
Note: Unlike other packages used by train, the rrcovHD package is fully loaded when this model is used.
ROC-Based Classifier (method = 'rocc')
For classification using package rocc with tuning parameters:
Number of Variables Retained (xgenes, numeric)



Rotation Forest (method = 'rotationForest')
For classification using package rotationForest with tuning parameters:
Number of Variable Subsets (K, numeric)
Ensemble Size (L, numeric)



Rotation Forest (method = 'rotationForestCp')
For classification using packages rpart, plyr and rotationForest with tuning parameters:
Number of Variable Subsets (K, numeric)
Ensemble Size (L, numeric)
Complexity Parameter (cp, numeric)



Rule-Based Classifier (method = 'JRip')
For classification using package RWeka with tuning parameters:
Number of  Optimizations (NumOpt, numeric)
Number of  Folds (NumFolds, numeric)
Min Weights (MinWeights, numeric)



Rule-Based Classifier (method = 'PART')
For classification using package RWeka with tuning parameters:
Confidence Threshold (threshold, numeric)
Pruning (pruned, character)



Self-Organizing Maps (method = 'xyf')
For classification and regression using package kohonen with tuning parameters:
Rows (xdim, numeric)
Columns (ydim, numeric)
Layer Weight (user.weights, numeric)
Topology (topo, character)


Note: As of version 3.0.0 of the kohonen package, the argument user.weights replaces the old alpha parameter. user.weights is usually a vector of relative weights such as c(1, 3) but is parameterized here as a proportion such as c(1-.75, .75) where the .75 is the value of the tuning parameter passed to train and indicates that the outcome layer has 3 times the weight as the predictor layer.
Semi-Naive Structure Learner Wrapper (method = 'nbSearch')
For classification using package bnclassify with tuning parameters:
Number of Folds (k, numeric)
Minimum Absolute Improvement (epsilon, numeric)
Smoothing Parameter (smooth, numeric)
Final Smoothing Parameter (final_smooth, numeric)
Search Direction (direction, character)



Shrinkage Discriminant Analysis (method = 'sda')
For classification using package sda with tuning parameters:
Diagonalize (diagonal, logical)
shrinkage (lambda, numeric)



SIMCA (method = 'CSimca')
For classification using packages rrcov and rrcovHD with no tuning parameters.

Simplified TSK Fuzzy Rules (method = 'FS.HGD')
For regression using package frbs with tuning parameters:
Number of Fuzzy Terms (num.labels, numeric)
Max. Iterations (max.iter, numeric)



Single C5.0 Ruleset (method = 'C5.0Rules')
For classification using package C50 with no tuning parameters.

Single C5.0 Tree (method = 'C5.0Tree')
For classification using package C50 with no tuning parameters.

Single Rule Classification (method = 'OneR')
For classification using package RWeka with no tuning parameters.

Sparse Distance Weighted Discrimination (method = 'sdwd')
For classification using package sdwd with tuning parameters:
L1 Penalty (lambda, numeric)
L2 Penalty (lambda2, numeric)



Sparse Linear Discriminant Analysis (method = 'sparseLDA')
For classification using package sparseLDA with tuning parameters:
Number of  Predictors (NumVars, numeric)
Lambda (lambda, numeric)



Sparse Mixture Discriminant Analysis (method = 'smda')
For classification using package sparseLDA with tuning parameters:
Number of  Predictors (NumVars, numeric)
Lambda (lambda, numeric)
Number of  Subclasses (R, numeric)



Sparse Partial Least Squares (method = 'spls')
For classification and regression using package spls with tuning parameters:
Number of Components (K, numeric)
Threshold (eta, numeric)
Kappa (kappa, numeric)



Spike and Slab Regression (method = 'spikeslab')
For regression using packages spikeslab and plyr with tuning parameters:
Variables Retained (vars, numeric)


Note: Unlike other packages used by train, the spikeslab package is fully loaded when this model is used.
Stabilized Linear Discriminant Analysis (method = 'slda')
For classification using package ipred with no tuning parameters.

Stabilized Nearest Neighbor Classifier (method = 'snn')
For classification using package snn with tuning parameters:
Stabilization Parameter (lambda, numeric)



Stacked AutoEncoder Deep Neural Network (method = 'dnn')
For classification and regression using package deepnet with tuning parameters:
Hidden Layer 1 (layer1, numeric)
Hidden Layer 2 (layer2, numeric)
Hidden Layer 3 (layer3, numeric)
Hidden Dropouts (hidden_dropout, numeric)
Visible Dropout (visible_dropout, numeric)



Stochastic Gradient Boosting (method = 'gbm')
For classification and regression using packages gbm and plyr with tuning parameters:
Number of  Boosting Iterations (n.trees, numeric)
Max Tree Depth (interaction.depth, numeric)
Shrinkage (shrinkage, numeric)
Min. Terminal Node Size (n.minobsinnode, numeric)



Subtractive Clustering and Fuzzy c-Means Rules (method = 'SBC')
For regression using package frbs with tuning parameters:
Radius (r.a, numeric)
Upper Threshold (eps.high, numeric)
Lower Threshold (eps.low, numeric)



Supervised Principal Component Analysis (method = 'superpc')
For regression using package superpc with tuning parameters:
Threshold (threshold, numeric)
Number of Components (n.components, numeric)



Support Vector Machines with Boundrange String Kernel (method = 'svmBoundrangeString')
For classification and regression using package kernlab with tuning parameters:
length (length, numeric)
Cost (C, numeric)



Support Vector Machines with Class Weights (method = 'svmRadialWeights')
For classification using package kernlab with tuning parameters:
Sigma (sigma, numeric)
Cost (C, numeric)
Weight (Weight, numeric)



Support Vector Machines with Exponential String Kernel (method = 'svmExpoString')
For classification and regression using package kernlab with tuning parameters:
lambda (lambda, numeric)
Cost (C, numeric)



Support Vector Machines with Linear Kernel (method = 'svmLinear')
For classification and regression using package kernlab with tuning parameters:
Cost (C, numeric)



Support Vector Machines with Linear Kernel (method = 'svmLinear2')
For classification and regression using package e1071 with tuning parameters:
Cost (cost, numeric)



Support Vector Machines with Polynomial Kernel (method = 'svmPoly')
For classification and regression using package kernlab with tuning parameters:
Polynomial Degree (degree, numeric)
Scale (scale, numeric)
Cost (C, numeric)



Support Vector Machines with Radial Basis Function Kernel (method = 'svmRadial')
For classification and regression using package kernlab with tuning parameters:
Sigma (sigma, numeric)
Cost (C, numeric)



Support Vector Machines with Radial Basis Function Kernel (method = 'svmRadialCost')
For classification and regression using package kernlab with tuning parameters:
Cost (C, numeric)



Support Vector Machines with Radial Basis Function Kernel (method = 'svmRadialSigma')
For classification and regression using package kernlab with tuning parameters:
Sigma (sigma, numeric)
Cost (C, numeric)


Note: This SVM model tunes over the cost parameter and the RBF kernel parameter sigma. In the latter case, using tuneLength will, at most, evaluate six values of the kernel parameter. This enables a broad search over the cost parameter and a relatively narrow search over sigma
Support Vector Machines with Spectrum String Kernel (method = 'svmSpectrumString')
For classification and regression using package kernlab with tuning parameters:
length (length, numeric)
Cost (C, numeric)



The Bayesian lasso (method = 'blasso')
For regression using package monomvn with tuning parameters:
Sparsity Threshold (sparsity, numeric)


Note: This model creates predictions using the mean of the posterior distributions but sets some parameters specifically to zero based on the tuning parameter sparsity. For example, when sparsity = .5, only coefficients where at least half the posterior estimates are nonzero are used.
The lasso (method = 'lasso')
For regression using package elasticnet with tuning parameters:
Fraction of Full Solution (fraction, numeric)



Tree Augmented Naive Bayes Classifier (method = 'tan')
For classification using package bnclassify with tuning parameters:
Score Function (score, character)
Smoothing Parameter (smooth, numeric)



Tree Augmented Naive Bayes Classifier Structure Learner Wrapper (method = 'tanSearch')
For classification using package bnclassify with tuning parameters:
Number of Folds (k, numeric)
Minimum Absolute Improvement (epsilon, numeric)
Smoothing Parameter (smooth, numeric)
Final Smoothing Parameter (final_smooth, numeric)
Super-Parent (sp, logical)



Tree Augmented Naive Bayes Classifier with Attribute Weighting (method = 'awtan')
For classification using package bnclassify with tuning parameters:
Score Function (score, character)
Smoothing Parameter (smooth, numeric)



Tree Models from Genetic Algorithms (method = 'evtree')
For classification and regression using package evtree with tuning parameters:
Complexity Parameter (alpha, numeric)



Tree-Based Ensembles (method = 'nodeHarvest')
For classification and regression using package nodeHarvest with tuning parameters:
Maximum Interaction Depth (maxinter, numeric)
Prediction Mode (mode, character)



Variational Bayesian Multinomial Probit Regression (method = 'vbmpRadial')
For classification using package vbmp with tuning parameters:
Theta Estimated (estimateTheta, character)



Wang and Mendel Fuzzy Rules (method = 'WM')
For regression using package frbs with tuning parameters:
Number of Fuzzy Terms (num.labels, numeric)
Membership Function (type.mf, character)



Weighted Subspace Random Forest (method = 'wsrf')
For classification using package wsrf with tuning parameters:
Number of Randomly Selected Predictors (mtry, numeric)



" />
<meta name="twitter:card" content="summary" />




<!-- mathjax -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script>

<!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->



  </head>

  <body>
    <div class="container template-reference-topic">
      <header>
      <div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">caret</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="Released version">6.0.84</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="../index.html">
    <span class="fas fa fas fa-home fa-lg"></span>
     
  </a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    <span class="fas fa fas fa-archive fa-lg"></span>
     
    R Packages
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="../../boot/index.html">boot</a>
    </li>
    <li>
      <a href="../../broom/index.html">broom</a>
    </li>
    <li>
      <a href="../../caret/index.html">caret</a>
    </li>
    <li>
      <a href="../../cluster/index.html">cluster</a>
    </li>
    <li>
      <a href="../../coefplot/index.html">coefplot</a>
    </li>
    <li>
      <a href="../../data.table/index.html">data.table</a>
    </li>
    <li>
      <a href="../../devtools/index.html">devtools</a>
    </li>
    <li>
      <a href="../../dplyr/index.html">dplyr</a>
    </li>
    <li>
      <a href="../../e1071/index.html">e1071</a>
    </li>
    <li>
      <a href="../../forcats/index.html">forcats</a>
    </li>
    <li>
      <a href="../../gbm/index.html">gbm</a>
    </li>
    <li>
      <a href="../../ggplot2/index.html">ggplot2</a>
    </li>
    <li>
      <a href="../../glmnet/index.html">glmnet</a>
    </li>
    <li>
      <a href="../../gridExtra/index.html">gridExtra</a>
    </li>
    <li>
      <a href="../../ISLR/index.html">ISLR</a>
    </li>
    <li>
      <a href="../../MASS/index.html">MASS</a>
    </li>
    <li>
      <a href="../../pdp/index.html">pdp</a>
    </li>
    <li>
      <a href="../../pls/index.html">pls</a>
    </li>
    <li>
      <a href="../../plyr/index.html">plyr</a>
    </li>
    <li>
      <a href="../../pROC/index.html">pROC</a>
    </li>
    <li>
      <a href="../../purrr/index.html">purrr</a>
    </li>
    <li>
      <a href="../../randomForest/index.html">randomForest</a>
    </li>
    <li>
      <a href="../../readr/index.html">readr</a>
    </li>
    <li>
      <a href="../../rpart/index.html">rpart</a>
    </li>
    <li>
      <a href="../../rpart.plot/index.html">rpart.plot</a>
    </li>
    <li>
      <a href="../../stringr/index.html">stringr</a>
    </li>
    <li>
      <a href="../../tibble/index.html">tibble</a>
    </li>
    <li>
      <a href="../../tidyr/index.html">tidyr</a>
    </li>
    <li>
      <a href="../../xgboost/index.html">xgboost</a>
    </li>
  </ul>
</li>
<li>
  <a href="../articles/caret.html">Get started</a>
</li>
<li>
  <a href="../reference/index.html">Reference</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="https://github.com/topepo/caret">
    <span class="fab fa fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
      
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

      

      </header>

<div class="row">
  <div class="col-md-9 contents">
    <div class="page-header">
    <h1>A List of Available Models in train</h1>
    
    <div class="hidden name"><code>models.Rd</code></div>
    </div>

    <div class="ref-description">
    <p>These models are included in the package via wrappers for <code><a href='train.html'>train</a></code>. Custom models can also be created. See the URL below.</p>
<p><strong>AdaBoost Classification Trees</strong> (<code>method = 'adaboost'</code>)</p>
<p>For classification using package <span class="pkg">fastAdaboost</span> with tuning parameters:</p><ul>
<li><p>Number of Trees (<code>nIter</code>, numeric)</p></li>
<li><p>Method (<code>method</code>, character)</p></li>
</ul>


<p><strong>AdaBoost.M1</strong> (<code>method = 'AdaBoost.M1'</code>)</p>
<p>For classification using packages <span class="pkg">adabag</span> and <span class="pkg">plyr</span> with tuning parameters:</p><ul>
<li><p>Number of Trees (<code>mfinal</code>, numeric)</p></li>
<li><p>Max Tree Depth (<code>maxdepth</code>, numeric)</p></li>
<li><p>Coefficient Type (<code>coeflearn</code>, character)</p></li>
</ul>


<p><strong>Adaptive Mixture Discriminant Analysis</strong> (<code>method = 'amdai'</code>)</p>
<p>For classification using package <span class="pkg">adaptDA</span> with tuning parameters:</p><ul>
<li><p>Model Type (<code>model</code>, character)</p></li>
</ul>


<p><strong>Adaptive-Network-Based Fuzzy Inference System</strong> (<code>method = 'ANFIS'</code>)</p>
<p>For regression using package <span class="pkg">frbs</span> with tuning parameters:</p><ul>
<li><p>Number of Fuzzy Terms (<code>num.labels</code>, numeric)</p></li>
<li><p>Max. Iterations (<code>max.iter</code>, numeric)</p></li>
</ul>


<p><strong>Adjacent Categories Probability Model for Ordinal Data</strong> (<code>method = 'vglmAdjCat'</code>)</p>
<p>For classification using package <span class="pkg">VGAM</span> with tuning parameters:</p><ul>
<li><p>Parallel Curves (<code>parallel</code>, logical)</p></li>
<li><p>Link Function (<code>link</code>, character)</p></li>
</ul>


<p><strong>Bagged AdaBoost</strong> (<code>method = 'AdaBag'</code>)</p>
<p>For classification using packages <span class="pkg">adabag</span> and <span class="pkg">plyr</span> with tuning parameters:</p><ul>
<li><p>Number of Trees (<code>mfinal</code>, numeric)</p></li>
<li><p>Max Tree Depth (<code>maxdepth</code>, numeric)</p></li>
</ul>


<p><strong>Bagged CART</strong> (<code>method = 'treebag'</code>)</p>
<p>For classification and regression using packages <span class="pkg">ipred</span>, <span class="pkg">plyr</span> and <span class="pkg">e1071</span> with no tuning parameters.</p>

<p><strong>Bagged FDA using gCV Pruning</strong> (<code>method = 'bagFDAGCV'</code>)</p>
<p>For classification using package <span class="pkg">earth</span> with tuning parameters:</p><ul>
<li><p>Product Degree (<code>degree</code>, numeric)</p></li>
</ul>

<p>Note: Unlike other packages used by <code>train</code>, the <code>earth</code> package is fully loaded when this model is used.</p>
<p><strong>Bagged Flexible Discriminant Analysis</strong> (<code>method = 'bagFDA'</code>)</p>
<p>For classification using packages <span class="pkg">earth</span> and <span class="pkg">mda</span> with tuning parameters:</p><ul>
<li><p>Product Degree (<code>degree</code>, numeric)</p></li>
<li><p>Number of Terms (<code>nprune</code>, numeric)</p></li>
</ul>

<p>Note: Unlike other packages used by <code>train</code>, the <code>earth</code> package is fully loaded when this model is used.</p>
<p><strong>Bagged Logic Regression</strong> (<code>method = 'logicBag'</code>)</p>
<p>For classification and regression using package <span class="pkg">logicFS</span> with tuning parameters:</p><ul>
<li><p>Maximum Number of Leaves (<code>nleaves</code>, numeric)</p></li>
<li><p>Number of Trees (<code>ntrees</code>, numeric)</p></li>
</ul>

<p>Note: Unlike other packages used by <code>train</code>, the <code>logicFS</code> package is fully loaded when this model is used.</p>
<p><strong>Bagged MARS</strong> (<code>method = 'bagEarth'</code>)</p>
<p>For classification and regression using package <span class="pkg">earth</span> with tuning parameters:</p><ul>
<li><p>Number of Terms (<code>nprune</code>, numeric)</p></li>
<li><p>Product Degree (<code>degree</code>, numeric)</p></li>
</ul>

<p>Note: Unlike other packages used by <code>train</code>, the <code>earth</code> package is fully loaded when this model is used.</p>
<p><strong>Bagged MARS using gCV Pruning</strong> (<code>method = 'bagEarthGCV'</code>)</p>
<p>For classification and regression using package <span class="pkg">earth</span> with tuning parameters:</p><ul>
<li><p>Product Degree (<code>degree</code>, numeric)</p></li>
</ul>

<p>Note: Unlike other packages used by <code>train</code>, the <code>earth</code> package is fully loaded when this model is used.</p>
<p><strong>Bagged Model</strong> (<code>method = 'bag'</code>)</p>
<p>For classification and regression using package <span class="pkg">caret</span> with tuning parameters:</p><ul>
<li><p>Number of Randomly Selected Predictors (<code>vars</code>, numeric)</p></li>
</ul>


<p><strong>Bayesian Additive Regression Trees</strong> (<code>method = 'bartMachine'</code>)</p>
<p>For classification and regression using package <span class="pkg">bartMachine</span> with tuning parameters:</p><ul>
<li><p>Number of Trees (<code>num_trees</code>, numeric)</p></li>
<li><p>Prior Boundary (<code>k</code>, numeric)</p></li>
<li><p>Base Terminal Node Hyperparameter (<code>alpha</code>, numeric)</p></li>
<li><p>Power Terminal Node Hyperparameter (<code>beta</code>, numeric)</p></li>
<li><p>Degrees of Freedom (<code>nu</code>, numeric)</p></li>
</ul>


<p><strong>Bayesian Generalized Linear Model</strong> (<code>method = 'bayesglm'</code>)</p>
<p>For classification and regression using package <span class="pkg">arm</span> with no tuning parameters.</p>

<p><strong>Bayesian Regularized Neural Networks</strong> (<code>method = 'brnn'</code>)</p>
<p>For regression using package <span class="pkg">brnn</span> with tuning parameters:</p><ul>
<li><p>Number of  Neurons (<code>neurons</code>, numeric)</p></li>
</ul>


<p><strong>Bayesian Ridge Regression</strong> (<code>method = 'bridge'</code>)</p>
<p>For regression using package <span class="pkg">monomvn</span> with no tuning parameters.</p>

<p><strong>Bayesian Ridge Regression (Model Averaged)</strong> (<code>method = 'blassoAveraged'</code>)</p>
<p>For regression using package <span class="pkg">monomvn</span> with no tuning parameters.</p>
<p>Note: This model makes predictions by averaging the predictions based on the posterior estimates of the regression coefficients. While it is possible that some of these posterior estimates are zero for non-informative predictors, the final predicted value may be a function of many (or even all) predictors.</p>
<p><strong>Binary Discriminant Analysis</strong> (<code>method = 'binda'</code>)</p>
<p>For classification using package <span class="pkg">binda</span> with tuning parameters:</p><ul>
<li><p>Shrinkage Intensity (<code>lambda.freqs</code>, numeric)</p></li>
</ul>


<p><strong>Boosted Classification Trees</strong> (<code>method = 'ada'</code>)</p>
<p>For classification using packages <span class="pkg">ada</span> and <span class="pkg">plyr</span> with tuning parameters:</p><ul>
<li><p>Number of Trees (<code>iter</code>, numeric)</p></li>
<li><p>Max Tree Depth (<code>maxdepth</code>, numeric)</p></li>
<li><p>Learning Rate (<code>nu</code>, numeric)</p></li>
</ul>


<p><strong>Boosted Generalized Additive Model</strong> (<code>method = 'gamboost'</code>)</p>
<p>For classification and regression using packages <span class="pkg">mboost</span>, <span class="pkg">plyr</span> and <span class="pkg">import</span> with tuning parameters:</p><ul>
<li><p>Number of  Boosting Iterations (<code>mstop</code>, numeric)</p></li>
<li><p>AIC Prune? (<code>prune</code>, character)</p></li>
</ul>

<p>Note: The <code>prune</code> option for this model enables the number of iterations to be determined by the optimal AIC value across all iterations. See the examples in <code>?mboost::mstop</code>. If pruning is not used, the ensemble makes predictions using the exact value of the <code>mstop</code> tuning parameter value.</p>
<p><strong>Boosted Generalized Linear Model</strong> (<code>method = 'glmboost'</code>)</p>
<p>For classification and regression using packages <span class="pkg">plyr</span> and <span class="pkg">mboost</span> with tuning parameters:</p><ul>
<li><p>Number of  Boosting Iterations (<code>mstop</code>, numeric)</p></li>
<li><p>AIC Prune? (<code>prune</code>, character)</p></li>
</ul>

<p>Note: The <code>prune</code> option for this model enables the number of iterations to be determined by the optimal AIC value across all iterations. See the examples in <code>?mboost::mstop</code>. If pruning is not used, the ensemble makes predictions using the exact value of the <code>mstop</code> tuning parameter value.</p>
<p><strong>Boosted Linear Model</strong> (<code>method = 'BstLm'</code>)</p>
<p>For classification and regression using packages <span class="pkg">bst</span> and <span class="pkg">plyr</span> with tuning parameters:</p><ul>
<li><p>Number of  Boosting Iterations (<code>mstop</code>, numeric)</p></li>
<li><p>Shrinkage (<code>nu</code>, numeric)</p></li>
</ul>


<p><strong>Boosted Logistic Regression</strong> (<code>method = 'LogitBoost'</code>)</p>
<p>For classification using package <span class="pkg">caTools</span> with tuning parameters:</p><ul>
<li><p>Number of  Boosting Iterations (<code>nIter</code>, numeric)</p></li>
</ul>


<p><strong>Boosted Smoothing Spline</strong> (<code>method = 'bstSm'</code>)</p>
<p>For classification and regression using packages <span class="pkg">bst</span> and <span class="pkg">plyr</span> with tuning parameters:</p><ul>
<li><p>Number of  Boosting Iterations (<code>mstop</code>, numeric)</p></li>
<li><p>Shrinkage (<code>nu</code>, numeric)</p></li>
</ul>


<p><strong>Boosted Tree</strong> (<code>method = 'blackboost'</code>)</p>
<p>For classification and regression using packages <span class="pkg">party</span>, <span class="pkg">mboost</span> and <span class="pkg">plyr</span> with tuning parameters:</p><ul>
<li><p>Number of Trees (<code>mstop</code>, numeric)</p></li>
<li><p>Max Tree Depth (<code>maxdepth</code>, numeric)</p></li>
</ul>


<p><strong>Boosted Tree</strong> (<code>method = 'bstTree'</code>)</p>
<p>For classification and regression using packages <span class="pkg">bst</span> and <span class="pkg">plyr</span> with tuning parameters:</p><ul>
<li><p>Number of  Boosting Iterations (<code>mstop</code>, numeric)</p></li>
<li><p>Max Tree Depth (<code>maxdepth</code>, numeric)</p></li>
<li><p>Shrinkage (<code>nu</code>, numeric)</p></li>
</ul>


<p><strong>C4.5-like Trees</strong> (<code>method = 'J48'</code>)</p>
<p>For classification using package <span class="pkg">RWeka</span> with tuning parameters:</p><ul>
<li><p>Confidence Threshold (<code>C</code>, numeric)</p></li>
<li><p>Minimum Instances Per Leaf (<code>M</code>, numeric)</p></li>
</ul>


<p><strong>C5.0</strong> (<code>method = 'C5.0'</code>)</p>
<p>For classification using packages <span class="pkg">C50</span> and <span class="pkg">plyr</span> with tuning parameters:</p><ul>
<li><p>Number of  Boosting Iterations (<code>trials</code>, numeric)</p></li>
<li><p>Model Type (<code>model</code>, character)</p></li>
<li><p>Winnow (<code>winnow</code>, logical)</p></li>
</ul>


<p><strong>CART</strong> (<code>method = 'rpart'</code>)</p>
<p>For classification and regression using package <span class="pkg">rpart</span> with tuning parameters:</p><ul>
<li><p>Complexity Parameter (<code>cp</code>, numeric)</p></li>
</ul>


<p><strong>CART</strong> (<code>method = 'rpart1SE'</code>)</p>
<p>For classification and regression using package <span class="pkg">rpart</span> with no tuning parameters.</p>
<p>Note: This CART model replicates the same process used by the <code>rpart</code> function where the model complexity is determined using the one-standard error method. This procedure is replicated inside of the resampling done by <code>train</code> so that an external resampling estimate can be obtained.</p>
<p><strong>CART</strong> (<code>method = 'rpart2'</code>)</p>
<p>For classification and regression using package <span class="pkg">rpart</span> with tuning parameters:</p><ul>
<li><p>Max Tree Depth (<code>maxdepth</code>, numeric)</p></li>
</ul>


<p><strong>CART or Ordinal Responses</strong> (<code>method = 'rpartScore'</code>)</p>
<p>For classification using packages <span class="pkg">rpartScore</span> and <span class="pkg">plyr</span> with tuning parameters:</p><ul>
<li><p>Complexity Parameter (<code>cp</code>, numeric)</p></li>
<li><p>Split Function (<code>split</code>, character)</p></li>
<li><p>Pruning Measure (<code>prune</code>, character)</p></li>
</ul>


<p><strong>CHi-squared Automated Interaction Detection</strong> (<code>method = 'chaid'</code>)</p>
<p>For classification using package <span class="pkg">CHAID</span> with tuning parameters:</p><ul>
<li><p>Merging Threshold (<code>alpha2</code>, numeric)</p></li>
<li><p>Splitting former Merged Threshold (<code>alpha3</code>, numeric)</p></li>
<li><p>Splitting former Merged Threshold (<code>alpha4</code>, numeric)</p></li>
</ul>


<p><strong>Conditional Inference Random Forest</strong> (<code>method = 'cforest'</code>)</p>
<p>For classification and regression using package <span class="pkg">party</span> with tuning parameters:</p><ul>
<li><p>Number of Randomly Selected Predictors (<code>mtry</code>, numeric)</p></li>
</ul>


<p><strong>Conditional Inference Tree</strong> (<code>method = 'ctree'</code>)</p>
<p>For classification and regression using package <span class="pkg">party</span> with tuning parameters:</p><ul>
<li><p>1 - P-Value Threshold (<code>mincriterion</code>, numeric)</p></li>
</ul>


<p><strong>Conditional Inference Tree</strong> (<code>method = 'ctree2'</code>)</p>
<p>For classification and regression using package <span class="pkg">party</span> with tuning parameters:</p><ul>
<li><p>Max Tree Depth (<code>maxdepth</code>, numeric)</p></li>
<li><p>1 - P-Value Threshold (<code>mincriterion</code>, numeric)</p></li>
</ul>


<p><strong>Continuation Ratio Model for Ordinal Data</strong> (<code>method = 'vglmContRatio'</code>)</p>
<p>For classification using package <span class="pkg">VGAM</span> with tuning parameters:</p><ul>
<li><p>Parallel Curves (<code>parallel</code>, logical)</p></li>
<li><p>Link Function (<code>link</code>, character)</p></li>
</ul>


<p><strong>Cost-Sensitive C5.0</strong> (<code>method = 'C5.0Cost'</code>)</p>
<p>For classification using packages <span class="pkg">C50</span> and <span class="pkg">plyr</span> with tuning parameters:</p><ul>
<li><p>Number of  Boosting Iterations (<code>trials</code>, numeric)</p></li>
<li><p>Model Type (<code>model</code>, character)</p></li>
<li><p>Winnow (<code>winnow</code>, logical)</p></li>
<li><p>Cost (<code>cost</code>, numeric)</p></li>
</ul>


<p><strong>Cost-Sensitive CART</strong> (<code>method = 'rpartCost'</code>)</p>
<p>For classification using packages <span class="pkg">rpart</span> and <span class="pkg">plyr</span> with tuning parameters:</p><ul>
<li><p>Complexity Parameter (<code>cp</code>, numeric)</p></li>
<li><p>Cost (<code>Cost</code>, numeric)</p></li>
</ul>


<p><strong>Cubist</strong> (<code>method = 'cubist'</code>)</p>
<p>For regression using package <span class="pkg">Cubist</span> with tuning parameters:</p><ul>
<li><p>Number of Committees (<code>committees</code>, numeric)</p></li>
<li><p>Number of Instances (<code>neighbors</code>, numeric)</p></li>
</ul>


<p><strong>Cumulative Probability Model for Ordinal Data</strong> (<code>method = 'vglmCumulative'</code>)</p>
<p>For classification using package <span class="pkg">VGAM</span> with tuning parameters:</p><ul>
<li><p>Parallel Curves (<code>parallel</code>, logical)</p></li>
<li><p>Link Function (<code>link</code>, character)</p></li>
</ul>


<p><strong>DeepBoost</strong> (<code>method = 'deepboost'</code>)</p>
<p>For classification using package <span class="pkg">deepboost</span> with tuning parameters:</p><ul>
<li><p>Number of  Boosting Iterations (<code>num_iter</code>, numeric)</p></li>
<li><p>Tree Depth (<code>tree_depth</code>, numeric)</p></li>
<li><p>L1 Regularization (<code>beta</code>, numeric)</p></li>
<li><p>Tree Depth Regularization (<code>lambda</code>, numeric)</p></li>
<li><p>Loss (<code>loss_type</code>, character)</p></li>
</ul>


<p><strong>Diagonal Discriminant Analysis</strong> (<code>method = 'dda'</code>)</p>
<p>For classification using package <span class="pkg">sparsediscrim</span> with tuning parameters:</p><ul>
<li><p>Model (<code>model</code>, character)</p></li>
<li><p>Shrinkage Type (<code>shrinkage</code>, character)</p></li>
</ul>


<p><strong>Distance Weighted Discrimination with Polynomial Kernel</strong> (<code>method = 'dwdPoly'</code>)</p>
<p>For classification using package <span class="pkg">kerndwd</span> with tuning parameters:</p><ul>
<li><p>Regularization Parameter (<code>lambda</code>, numeric)</p></li>
<li><p>q (<code>qval</code>, numeric)</p></li>
<li><p>Polynomial Degree (<code>degree</code>, numeric)</p></li>
<li><p>Scale (<code>scale</code>, numeric)</p></li>
</ul>


<p><strong>Distance Weighted Discrimination with Radial Basis Function Kernel</strong> (<code>method = 'dwdRadial'</code>)</p>
<p>For classification using packages <span class="pkg">kernlab</span> and <span class="pkg">kerndwd</span> with tuning parameters:</p><ul>
<li><p>Regularization Parameter (<code>lambda</code>, numeric)</p></li>
<li><p>q (<code>qval</code>, numeric)</p></li>
<li><p>Sigma (<code>sigma</code>, numeric)</p></li>
</ul>


<p><strong>Dynamic Evolving Neural-Fuzzy Inference System </strong> (<code>method = 'DENFIS'</code>)</p>
<p>For regression using package <span class="pkg">frbs</span> with tuning parameters:</p><ul>
<li><p>Threshold (<code>Dthr</code>, numeric)</p></li>
<li><p>Max. Iterations (<code>max.iter</code>, numeric)</p></li>
</ul>


<p><strong>Elasticnet</strong> (<code>method = 'enet'</code>)</p>
<p>For regression using package <span class="pkg">elasticnet</span> with tuning parameters:</p><ul>
<li><p>Fraction of Full Solution (<code>fraction</code>, numeric)</p></li>
<li><p>Weight Decay (<code>lambda</code>, numeric)</p></li>
</ul>


<p><strong>Ensembles of Generalized Linear Models</strong> (<code>method = 'randomGLM'</code>)</p>
<p>For classification and regression using package <span class="pkg">randomGLM</span> with tuning parameters:</p><ul>
<li><p>Interaction Order (<code>maxInteractionOrder</code>, numeric)</p></li>
</ul>

<p>Note: Unlike other packages used by <code>train</code>, the <code>randomGLM</code> package is fully loaded when this model is used.</p>
<p><strong>eXtreme Gradient Boosting</strong> (<code>method = 'xgbDART'</code>)</p>
<p>For classification and regression using packages <span class="pkg">xgboost</span> and <span class="pkg">plyr</span> with tuning parameters:</p><ul>
<li><p>Number of  Boosting Iterations (<code>nrounds</code>, numeric)</p></li>
<li><p>Max Tree Depth (<code>max_depth</code>, numeric)</p></li>
<li><p>Shrinkage (<code>eta</code>, numeric)</p></li>
<li><p>Minimum Loss Reduction (<code>gamma</code>, numeric)</p></li>
<li><p>Subsample Percentage (<code>subsample</code>, numeric)</p></li>
<li><p>Subsample Ratio of Columns (<code>colsample_bytree</code>, numeric)</p></li>
<li><p>Fraction of Trees Dropped (<code>rate_drop</code>, numeric)</p></li>
<li><p>Prob. of Skipping Drop-out (<code>skip_drop</code>, numeric)</p></li>
<li><p>Minimum Sum of Instance Weight (<code>min_child_weight</code>, numeric)</p></li>
</ul>


<p><strong>eXtreme Gradient Boosting</strong> (<code>method = 'xgbLinear'</code>)</p>
<p>For classification and regression using package <span class="pkg">xgboost</span> with tuning parameters:</p><ul>
<li><p>Number of  Boosting Iterations (<code>nrounds</code>, numeric)</p></li>
<li><p>L2 Regularization (<code>lambda</code>, numeric)</p></li>
<li><p>L1 Regularization (<code>alpha</code>, numeric)</p></li>
<li><p>Learning Rate (<code>eta</code>, numeric)</p></li>
</ul>


<p><strong>eXtreme Gradient Boosting</strong> (<code>method = 'xgbTree'</code>)</p>
<p>For classification and regression using packages <span class="pkg">xgboost</span> and <span class="pkg">plyr</span> with tuning parameters:</p><ul>
<li><p>Number of  Boosting Iterations (<code>nrounds</code>, numeric)</p></li>
<li><p>Max Tree Depth (<code>max_depth</code>, numeric)</p></li>
<li><p>Shrinkage (<code>eta</code>, numeric)</p></li>
<li><p>Minimum Loss Reduction (<code>gamma</code>, numeric)</p></li>
<li><p>Subsample Ratio of Columns (<code>colsample_bytree</code>, numeric)</p></li>
<li><p>Minimum Sum of Instance Weight (<code>min_child_weight</code>, numeric)</p></li>
<li><p>Subsample Percentage (<code>subsample</code>, numeric)</p></li>
</ul>


<p><strong>Extreme Learning Machine</strong> (<code>method = 'elm'</code>)</p>
<p>For classification and regression using package <span class="pkg">elmNN</span> with tuning parameters:</p><ul>
<li><p>Number of Hidden Units (<code>nhid</code>, numeric)</p></li>
<li><p>Activation Function (<code>actfun</code>, character)</p></li>
</ul>


<p><strong>Factor-Based Linear Discriminant Analysis</strong> (<code>method = 'RFlda'</code>)</p>
<p>For classification using package <span class="pkg">HiDimDA</span> with tuning parameters:</p><ul>
<li><p>Number of  Factors (<code>q</code>, numeric)</p></li>
</ul>


<p><strong>Flexible Discriminant Analysis</strong> (<code>method = 'fda'</code>)</p>
<p>For classification using packages <span class="pkg">earth</span> and <span class="pkg">mda</span> with tuning parameters:</p><ul>
<li><p>Product Degree (<code>degree</code>, numeric)</p></li>
<li><p>Number of Terms (<code>nprune</code>, numeric)</p></li>
</ul>

<p>Note: Unlike other packages used by <code>train</code>, the <code>earth</code> package is fully loaded when this model is used.</p>
<p><strong>Fuzzy Inference Rules by Descent Method</strong> (<code>method = 'FIR.DM'</code>)</p>
<p>For regression using package <span class="pkg">frbs</span> with tuning parameters:</p><ul>
<li><p>Number of Fuzzy Terms (<code>num.labels</code>, numeric)</p></li>
<li><p>Max. Iterations (<code>max.iter</code>, numeric)</p></li>
</ul>


<p><strong>Fuzzy Rules Using Chi's Method</strong> (<code>method = 'FRBCS.CHI'</code>)</p>
<p>For classification using package <span class="pkg">frbs</span> with tuning parameters:</p><ul>
<li><p>Number of Fuzzy Terms (<code>num.labels</code>, numeric)</p></li>
<li><p>Membership Function (<code>type.mf</code>, character)</p></li>
</ul>


<p><strong>Fuzzy Rules Using Genetic Cooperative-Competitive Learning and Pittsburgh</strong> (<code>method = 'FH.GBML'</code>)</p>
<p>For classification using package <span class="pkg">frbs</span> with tuning parameters:</p><ul>
<li><p>Max. Number of Rules (<code>max.num.rule</code>, numeric)</p></li>
<li><p>Population Size (<code>popu.size</code>, numeric)</p></li>
<li><p>Max. Generations (<code>max.gen</code>, numeric)</p></li>
</ul>


<p><strong>Fuzzy Rules Using the Structural Learning Algorithm on Vague Environment</strong> (<code>method = 'SLAVE'</code>)</p>
<p>For classification using package <span class="pkg">frbs</span> with tuning parameters:</p><ul>
<li><p>Number of Fuzzy Terms (<code>num.labels</code>, numeric)</p></li>
<li><p>Max. Iterations (<code>max.iter</code>, numeric)</p></li>
<li><p>Max. Generations (<code>max.gen</code>, numeric)</p></li>
</ul>


<p><strong>Fuzzy Rules via MOGUL</strong> (<code>method = 'GFS.FR.MOGUL'</code>)</p>
<p>For regression using package <span class="pkg">frbs</span> with tuning parameters:</p><ul>
<li><p>Max. Generations (<code>max.gen</code>, numeric)</p></li>
<li><p>Max. Iterations (<code>max.iter</code>, numeric)</p></li>
<li><p>Max. Tuning Iterations (<code>max.tune</code>, numeric)</p></li>
</ul>


<p><strong>Fuzzy Rules via Thrift</strong> (<code>method = 'GFS.THRIFT'</code>)</p>
<p>For regression using package <span class="pkg">frbs</span> with tuning parameters:</p><ul>
<li><p>Population Size (<code>popu.size</code>, numeric)</p></li>
<li><p>Number of  Fuzzy Labels (<code>num.labels</code>, numeric)</p></li>
<li><p>Max. Generations (<code>max.gen</code>, numeric)</p></li>
</ul>


<p><strong>Fuzzy Rules with Weight Factor</strong> (<code>method = 'FRBCS.W'</code>)</p>
<p>For classification using package <span class="pkg">frbs</span> with tuning parameters:</p><ul>
<li><p>Number of Fuzzy Terms (<code>num.labels</code>, numeric)</p></li>
<li><p>Membership Function (<code>type.mf</code>, character)</p></li>
</ul>


<p><strong>Gaussian Process</strong> (<code>method = 'gaussprLinear'</code>)</p>
<p>For classification and regression using package <span class="pkg">kernlab</span> with no tuning parameters.</p>

<p><strong>Gaussian Process with Polynomial Kernel</strong> (<code>method = 'gaussprPoly'</code>)</p>
<p>For classification and regression using package <span class="pkg">kernlab</span> with tuning parameters:</p><ul>
<li><p>Polynomial Degree (<code>degree</code>, numeric)</p></li>
<li><p>Scale (<code>scale</code>, numeric)</p></li>
</ul>


<p><strong>Gaussian Process with Radial Basis Function Kernel</strong> (<code>method = 'gaussprRadial'</code>)</p>
<p>For classification and regression using package <span class="pkg">kernlab</span> with tuning parameters:</p><ul>
<li><p>Sigma (<code>sigma</code>, numeric)</p></li>
</ul>


<p><strong>Generalized Additive Model using LOESS</strong> (<code>method = 'gamLoess'</code>)</p>
<p>For classification and regression using package <span class="pkg">gam</span> with tuning parameters:</p><ul>
<li><p>Span (<code>span</code>, numeric)</p></li>
<li><p>Degree (<code>degree</code>, numeric)</p></li>
</ul>

<p>Note: Which terms enter the model in a nonlinear manner is determined by the number of unique values for the predictor. For example, if a predictor only has four unique values, most basis expansion method will fail because there are not enough granularity in the data. By default, a predictor must have at least 10 unique values to be used in a nonlinear basis expansion. Unlike other packages used by <code>train</code>, the <code>gam</code> package is fully loaded when this model is used.</p>
<p><strong>Generalized Additive Model using Splines</strong> (<code>method = 'bam'</code>)</p>
<p>For classification and regression using package <span class="pkg">mgcv</span> with tuning parameters:</p><ul>
<li><p>Feature Selection (<code>select</code>, logical)</p></li>
<li><p>Method (<code>method</code>, character)</p></li>
</ul>

<p>Note: Which terms enter the model in a nonlinear manner is determined by the number of unique values for the predictor. For example, if a predictor only has four unique values, most basis expansion method will fail because there are not enough granularity in the data. By default, a predictor must have at least 10 unique values to be used in a nonlinear basis expansion. Unlike other packages used by <code>train</code>, the <code>mgcv</code> package is fully loaded when this model is used.</p>
<p><strong>Generalized Additive Model using Splines</strong> (<code>method = 'gam'</code>)</p>
<p>For classification and regression using package <span class="pkg">mgcv</span> with tuning parameters:</p><ul>
<li><p>Feature Selection (<code>select</code>, logical)</p></li>
<li><p>Method (<code>method</code>, character)</p></li>
</ul>

<p>Note: Which terms enter the model in a nonlinear manner is determined by the number of unique values for the predictor. For example, if a predictor only has four unique values, most basis expansion method will fail because there are not enough granularity in the data. By default, a predictor must have at least 10 unique values to be used in a nonlinear basis expansion. Unlike other packages used by <code>train</code>, the <code>mgcv</code> package is fully loaded when this model is used.</p>
<p><strong>Generalized Additive Model using Splines</strong> (<code>method = 'gamSpline'</code>)</p>
<p>For classification and regression using package <span class="pkg">gam</span> with tuning parameters:</p><ul>
<li><p>Degrees of Freedom (<code>df</code>, numeric)</p></li>
</ul>

<p>Note: Which terms enter the model in a nonlinear manner is determined by the number of unique values for the predictor. For example, if a predictor only has four unique values, most basis expansion method will fail because there are not enough granularity in the data. By default, a predictor must have at least 10 unique values to be used in a nonlinear basis expansion. Unlike other packages used by <code>train</code>, the <code>gam</code> package is fully loaded when this model is used.</p>
<p><strong>Generalized Linear Model</strong> (<code>method = 'glm'</code>)</p>
<p>For classification and regression with no tuning parameters.</p>

<p><strong>Generalized Linear Model with Stepwise Feature Selection</strong> (<code>method = 'glmStepAIC'</code>)</p>
<p>For classification and regression using package <span class="pkg">MASS</span> with no tuning parameters.</p>

<p><strong>Generalized Partial Least Squares</strong> (<code>method = 'gpls'</code>)</p>
<p>For classification using package <span class="pkg">gpls</span> with tuning parameters:</p><ul>
<li><p>Number of Components (<code>K.prov</code>, numeric)</p></li>
</ul>


<p><strong>Genetic Lateral Tuning and Rule Selection of Linguistic Fuzzy Systems</strong> (<code>method = 'GFS.LT.RS'</code>)</p>
<p>For regression using package <span class="pkg">frbs</span> with tuning parameters:</p><ul>
<li><p>Population Size (<code>popu.size</code>, numeric)</p></li>
<li><p>Number of  Fuzzy Labels (<code>num.labels</code>, numeric)</p></li>
<li><p>Max. Generations (<code>max.gen</code>, numeric)</p></li>
</ul>


<p><strong>glmnet</strong> (<code>method = 'glmnet_h2o'</code>)</p>
<p>For classification and regression using package <span class="pkg">h2o</span> with tuning parameters:</p><ul>
<li><p>Mixing Percentage (<code>alpha</code>, numeric)</p></li>
<li><p>Regularization Parameter (<code>lambda</code>, numeric)</p></li>
</ul>


<p><strong>glmnet</strong> (<code>method = 'glmnet'</code>)</p>
<p>For classification and regression using packages <span class="pkg">glmnet</span> and <span class="pkg">Matrix</span> with tuning parameters:</p><ul>
<li><p>Mixing Percentage (<code>alpha</code>, numeric)</p></li>
<li><p>Regularization Parameter (<code>lambda</code>, numeric)</p></li>
</ul>


<p><strong>Gradient Boosting Machines</strong> (<code>method = 'gbm_h2o'</code>)</p>
<p>For classification and regression using package <span class="pkg">h2o</span> with tuning parameters:</p><ul>
<li><p>Number of  Boosting Iterations (<code>ntrees</code>, numeric)</p></li>
<li><p>Max Tree Depth (<code>max_depth</code>, numeric)</p></li>
<li><p>Min. Terminal Node Size (<code>min_rows</code>, numeric)</p></li>
<li><p>Shrinkage (<code>learn_rate</code>, numeric)</p></li>
<li><p>Number of Randomly Selected Predictors (<code>col_sample_rate</code>, numeric)</p></li>
</ul>


<p><strong>Greedy Prototype Selection</strong> (<code>method = 'protoclass'</code>)</p>
<p>For classification using packages <span class="pkg">proxy</span> and <span class="pkg">protoclass</span> with tuning parameters:</p><ul>
<li><p>Ball Size (<code>eps</code>, numeric)</p></li>
<li><p>Distance Order (<code>Minkowski</code>, numeric)</p></li>
</ul>


<p><strong>Heteroscedastic Discriminant Analysis</strong> (<code>method = 'hda'</code>)</p>
<p>For classification using package <span class="pkg">hda</span> with tuning parameters:</p><ul>
<li><p>Gamma (<code>gamma</code>, numeric)</p></li>
<li><p>Lambda (<code>lambda</code>, numeric)</p></li>
<li><p>Dimension of the Discriminative Subspace (<code>newdim</code>, numeric)</p></li>
</ul>


<p><strong>High Dimensional Discriminant Analysis</strong> (<code>method = 'hdda'</code>)</p>
<p>For classification using package <span class="pkg">HDclassif</span> with tuning parameters:</p><ul>
<li><p>Threshold (<code>threshold</code>, character)</p></li>
<li><p>Model Type (<code>model</code>, numeric)</p></li>
</ul>


<p><strong>High-Dimensional Regularized Discriminant Analysis</strong> (<code>method = 'hdrda'</code>)</p>
<p>For classification using package <span class="pkg">sparsediscrim</span> with tuning parameters:</p><ul>
<li><p>Gamma (<code>gamma</code>, numeric)</p></li>
<li><p>Lambda (<code>lambda</code>, numeric)</p></li>
<li><p>Shrinkage Type (<code>shrinkage_type</code>, character)</p></li>
</ul>


<p><strong>Hybrid Neural Fuzzy Inference System</strong> (<code>method = 'HYFIS'</code>)</p>
<p>For regression using package <span class="pkg">frbs</span> with tuning parameters:</p><ul>
<li><p>Number of Fuzzy Terms (<code>num.labels</code>, numeric)</p></li>
<li><p>Max. Iterations (<code>max.iter</code>, numeric)</p></li>
</ul>


<p><strong>Independent Component Regression</strong> (<code>method = 'icr'</code>)</p>
<p>For regression using package <span class="pkg">fastICA</span> with tuning parameters:</p><ul>
<li><p>Number of Components (<code>n.comp</code>, numeric)</p></li>
</ul>


<p><strong>k-Nearest Neighbors</strong> (<code>method = 'kknn'</code>)</p>
<p>For classification and regression using package <span class="pkg">kknn</span> with tuning parameters:</p><ul>
<li><p>Max. Number of Neighbors (<code>kmax</code>, numeric)</p></li>
<li><p>Distance (<code>distance</code>, numeric)</p></li>
<li><p>Kernel (<code>kernel</code>, character)</p></li>
</ul>


<p><strong>k-Nearest Neighbors</strong> (<code>method = 'knn'</code>)</p>
<p>For classification and regression with tuning parameters:</p><ul>
<li><p>Number of Neighbors (<code>k</code>, numeric)</p></li>
</ul>


<p><strong>L2 Regularized Linear Support Vector Machines with Class Weights</strong> (<code>method = 'svmLinearWeights2'</code>)</p>
<p>For classification using package <span class="pkg">LiblineaR</span> with tuning parameters:</p><ul>
<li><p>Cost (<code>cost</code>, numeric)</p></li>
<li><p>Loss Function (<code>Loss</code>, character)</p></li>
<li><p>Class Weight (<code>weight</code>, numeric)</p></li>
</ul>


<p><strong>L2 Regularized Support Vector Machine (dual) with Linear Kernel</strong> (<code>method = 'svmLinear3'</code>)</p>
<p>For classification and regression using package <span class="pkg">LiblineaR</span> with tuning parameters:</p><ul>
<li><p>Cost (<code>cost</code>, numeric)</p></li>
<li><p>Loss Function (<code>Loss</code>, character)</p></li>
</ul>


<p><strong>Learning Vector Quantization</strong> (<code>method = 'lvq'</code>)</p>
<p>For classification using package <span class="pkg">class</span> with tuning parameters:</p><ul>
<li><p>Codebook Size (<code>size</code>, numeric)</p></li>
<li><p>Number of Prototypes (<code>k</code>, numeric)</p></li>
</ul>


<p><strong>Least Angle Regression</strong> (<code>method = 'lars'</code>)</p>
<p>For regression using package <span class="pkg">lars</span> with tuning parameters:</p><ul>
<li><p>Fraction (<code>fraction</code>, numeric)</p></li>
</ul>


<p><strong>Least Angle Regression</strong> (<code>method = 'lars2'</code>)</p>
<p>For regression using package <span class="pkg">lars</span> with tuning parameters:</p><ul>
<li><p>Number of Steps (<code>step</code>, numeric)</p></li>
</ul>


<p><strong>Least Squares Support Vector Machine</strong> (<code>method = 'lssvmLinear'</code>)</p>
<p>For classification using package <span class="pkg">kernlab</span> with tuning parameters:</p><ul>
<li><p>Regularization Parameter (<code>tau</code>, numeric)</p></li>
</ul>


<p><strong>Least Squares Support Vector Machine with Polynomial Kernel</strong> (<code>method = 'lssvmPoly'</code>)</p>
<p>For classification using package <span class="pkg">kernlab</span> with tuning parameters:</p><ul>
<li><p>Polynomial Degree (<code>degree</code>, numeric)</p></li>
<li><p>Scale (<code>scale</code>, numeric)</p></li>
<li><p>Regularization Parameter (<code>tau</code>, numeric)</p></li>
</ul>


<p><strong>Least Squares Support Vector Machine with Radial Basis Function Kernel</strong> (<code>method = 'lssvmRadial'</code>)</p>
<p>For classification using package <span class="pkg">kernlab</span> with tuning parameters:</p><ul>
<li><p>Sigma (<code>sigma</code>, numeric)</p></li>
<li><p>Regularization Parameter (<code>tau</code>, numeric)</p></li>
</ul>


<p><strong>Linear Discriminant Analysis</strong> (<code>method = 'lda'</code>)</p>
<p>For classification using package <span class="pkg">MASS</span> with no tuning parameters.</p>

<p><strong>Linear Discriminant Analysis</strong> (<code>method = 'lda2'</code>)</p>
<p>For classification using package <span class="pkg">MASS</span> with tuning parameters:</p><ul>
<li><p>Number of Discriminant Functions (<code>dimen</code>, numeric)</p></li>
</ul>


<p><strong>Linear Discriminant Analysis with Stepwise Feature Selection</strong> (<code>method = 'stepLDA'</code>)</p>
<p>For classification using packages <span class="pkg">klaR</span> and <span class="pkg">MASS</span> with tuning parameters:</p><ul>
<li><p>Maximum Number of Variables (<code>maxvar</code>, numeric)</p></li>
<li><p>Search Direction (<code>direction</code>, character)</p></li>
</ul>


<p><strong>Linear Distance Weighted Discrimination</strong> (<code>method = 'dwdLinear'</code>)</p>
<p>For classification using package <span class="pkg">kerndwd</span> with tuning parameters:</p><ul>
<li><p>Regularization Parameter (<code>lambda</code>, numeric)</p></li>
<li><p>q (<code>qval</code>, numeric)</p></li>
</ul>


<p><strong>Linear Regression</strong> (<code>method = 'lm'</code>)</p>
<p>For regression with tuning parameters:</p><ul>
<li><p>intercept (<code>intercept</code>, logical)</p></li>
</ul>


<p><strong>Linear Regression with Backwards Selection</strong> (<code>method = 'leapBackward'</code>)</p>
<p>For regression using package <span class="pkg">leaps</span> with tuning parameters:</p><ul>
<li><p>Maximum Number of Predictors (<code>nvmax</code>, numeric)</p></li>
</ul>


<p><strong>Linear Regression with Forward Selection</strong> (<code>method = 'leapForward'</code>)</p>
<p>For regression using package <span class="pkg">leaps</span> with tuning parameters:</p><ul>
<li><p>Maximum Number of Predictors (<code>nvmax</code>, numeric)</p></li>
</ul>


<p><strong>Linear Regression with Stepwise Selection</strong> (<code>method = 'leapSeq'</code>)</p>
<p>For regression using package <span class="pkg">leaps</span> with tuning parameters:</p><ul>
<li><p>Maximum Number of Predictors (<code>nvmax</code>, numeric)</p></li>
</ul>


<p><strong>Linear Regression with Stepwise Selection</strong> (<code>method = 'lmStepAIC'</code>)</p>
<p>For regression using package <span class="pkg">MASS</span> with no tuning parameters.</p>

<p><strong>Linear Support Vector Machines with Class Weights</strong> (<code>method = 'svmLinearWeights'</code>)</p>
<p>For classification using package <span class="pkg">e1071</span> with tuning parameters:</p><ul>
<li><p>Cost (<code>cost</code>, numeric)</p></li>
<li><p>Class Weight (<code>weight</code>, numeric)</p></li>
</ul>


<p><strong>Localized Linear Discriminant Analysis</strong> (<code>method = 'loclda'</code>)</p>
<p>For classification using package <span class="pkg">klaR</span> with tuning parameters:</p><ul>
<li><p>Number of Nearest Neighbors (<code>k</code>, numeric)</p></li>
</ul>


<p><strong>Logic Regression</strong> (<code>method = 'logreg'</code>)</p>
<p>For classification and regression using package <span class="pkg">LogicReg</span> with tuning parameters:</p><ul>
<li><p>Maximum Number of Leaves (<code>treesize</code>, numeric)</p></li>
<li><p>Number of Trees (<code>ntrees</code>, numeric)</p></li>
</ul>


<p><strong>Logistic Model Trees</strong> (<code>method = 'LMT'</code>)</p>
<p>For classification using package <span class="pkg">RWeka</span> with tuning parameters:</p><ul>
<li><p>Number of  Iteratons (<code>iter</code>, numeric)</p></li>
</ul>


<p><strong>Maximum Uncertainty Linear Discriminant Analysis</strong> (<code>method = 'Mlda'</code>)</p>
<p>For classification using package <span class="pkg">HiDimDA</span> with no tuning parameters.</p>

<p><strong>Mixture Discriminant Analysis</strong> (<code>method = 'mda'</code>)</p>
<p>For classification using package <span class="pkg">mda</span> with tuning parameters:</p><ul>
<li><p>Number of Subclasses Per Class (<code>subclasses</code>, numeric)</p></li>
</ul>


<p><strong>Model Averaged Naive Bayes Classifier</strong> (<code>method = 'manb'</code>)</p>
<p>For classification using package <span class="pkg">bnclassify</span> with tuning parameters:</p><ul>
<li><p>Smoothing Parameter (<code>smooth</code>, numeric)</p></li>
<li><p>Prior Probability (<code>prior</code>, numeric)</p></li>
</ul>


<p><strong>Model Averaged Neural Network</strong> (<code>method = 'avNNet'</code>)</p>
<p>For classification and regression using package <span class="pkg">nnet</span> with tuning parameters:</p><ul>
<li><p>Number of Hidden Units (<code>size</code>, numeric)</p></li>
<li><p>Weight Decay (<code>decay</code>, numeric)</p></li>
<li><p>Bagging (<code>bag</code>, logical)</p></li>
</ul>


<p><strong>Model Rules</strong> (<code>method = 'M5Rules'</code>)</p>
<p>For regression using package <span class="pkg">RWeka</span> with tuning parameters:</p><ul>
<li><p>Pruned (<code>pruned</code>, character)</p></li>
<li><p>Smoothed (<code>smoothed</code>, character)</p></li>
</ul>


<p><strong>Model Tree</strong> (<code>method = 'M5'</code>)</p>
<p>For regression using package <span class="pkg">RWeka</span> with tuning parameters:</p><ul>
<li><p>Pruned (<code>pruned</code>, character)</p></li>
<li><p>Smoothed (<code>smoothed</code>, character)</p></li>
<li><p>Rules (<code>rules</code>, character)</p></li>
</ul>


<p><strong>Monotone Multi-Layer Perceptron Neural Network</strong> (<code>method = 'monmlp'</code>)</p>
<p>For classification and regression using package <span class="pkg">monmlp</span> with tuning parameters:</p><ul>
<li><p>Number of Hidden Units (<code>hidden1</code>, numeric)</p></li>
<li><p>Number of Models (<code>n.ensemble</code>, numeric)</p></li>
</ul>


<p><strong>Multi-Layer Perceptron</strong> (<code>method = 'mlp'</code>)</p>
<p>For classification and regression using package <span class="pkg">RSNNS</span> with tuning parameters:</p><ul>
<li><p>Number of Hidden Units (<code>size</code>, numeric)</p></li>
</ul>


<p><strong>Multi-Layer Perceptron</strong> (<code>method = 'mlpWeightDecay'</code>)</p>
<p>For classification and regression using package <span class="pkg">RSNNS</span> with tuning parameters:</p><ul>
<li><p>Number of Hidden Units (<code>size</code>, numeric)</p></li>
<li><p>Weight Decay (<code>decay</code>, numeric)</p></li>
</ul>


<p><strong>Multi-Layer Perceptron, multiple layers</strong> (<code>method = 'mlpWeightDecayML'</code>)</p>
<p>For classification and regression using package <span class="pkg">RSNNS</span> with tuning parameters:</p><ul>
<li><p>Number of Hidden Units layer1 (<code>layer1</code>, numeric)</p></li>
<li><p>Number of Hidden Units layer2 (<code>layer2</code>, numeric)</p></li>
<li><p>Number of Hidden Units layer3 (<code>layer3</code>, numeric)</p></li>
<li><p>Weight Decay (<code>decay</code>, numeric)</p></li>
</ul>


<p><strong>Multi-Layer Perceptron, with multiple layers</strong> (<code>method = 'mlpML'</code>)</p>
<p>For classification and regression using package <span class="pkg">RSNNS</span> with tuning parameters:</p><ul>
<li><p>Number of Hidden Units layer1 (<code>layer1</code>, numeric)</p></li>
<li><p>Number of Hidden Units layer2 (<code>layer2</code>, numeric)</p></li>
<li><p>Number of Hidden Units layer3 (<code>layer3</code>, numeric)</p></li>
</ul>


<p><strong>Multi-Step Adaptive MCP-Net</strong> (<code>method = 'msaenet'</code>)</p>
<p>For classification and regression using package <span class="pkg">msaenet</span> with tuning parameters:</p><ul>
<li><p>Alpha (<code>alphas</code>, numeric)</p></li>
<li><p>Number of Adaptive Estimation Steps (<code>nsteps</code>, numeric)</p></li>
<li><p>Adaptive Weight Scaling Factor (<code>scale</code>, numeric)</p></li>
</ul>


<p><strong>Multilayer Perceptron Network by Stochastic Gradient Descent</strong> (<code>method = 'mlpSGD'</code>)</p>
<p>For classification and regression using packages <span class="pkg">FCNN4R</span> and <span class="pkg">plyr</span> with tuning parameters:</p><ul>
<li><p>Number of Hidden Units (<code>size</code>, numeric)</p></li>
<li><p>L2 Regularization (<code>l2reg</code>, numeric)</p></li>
<li><p>RMSE Gradient Scaling (<code>lambda</code>, numeric)</p></li>
<li><p>Learning Rate (<code>learn_rate</code>, numeric)</p></li>
<li><p>Momentum (<code>momentum</code>, numeric)</p></li>
<li><p>Learning Rate Decay (<code>gamma</code>, numeric)</p></li>
<li><p>Batch Size (<code>minibatchsz</code>, numeric)</p></li>
<li><p>Number of Models (<code>repeats</code>, numeric)</p></li>
</ul>


<p><strong>Multilayer Perceptron Network with Dropout</strong> (<code>method = 'mlpKerasDropout'</code>)</p>
<p>For classification and regression using package <span class="pkg">keras</span> with tuning parameters:</p><ul>
<li><p>Number of Hidden Units (<code>size</code>, numeric)</p></li>
<li><p>Dropout Rate (<code>dropout</code>, numeric)</p></li>
<li><p>Batch Size (<code>batch_size</code>, numeric)</p></li>
<li><p>Learning Rate (<code>lr</code>, numeric)</p></li>
<li><p>Rho (<code>rho</code>, numeric)</p></li>
<li><p>Learning Rate Decay (<code>decay</code>, numeric)</p></li>
<li><p>Activation Function (<code>activation</code>, character)</p></li>
</ul>

<p>Note: After <code>train</code> completes, the keras model object is serialized so that it can be used between R session. When predicting, the code will temporarily unsearalize the object. To make the predictions more efficient, the user might want to use <code>keras::unsearlize_model(object$finalModel$object)</code> in the current R session so that that operation is only done once. Also, this model cannot be run in parallel due to the nature of how tensorflow does the computations. Unlike other packages used by <code>train</code>, the <code>dplyr</code> package is fully loaded when this model is used.</p>
<p><strong>Multilayer Perceptron Network with Dropout</strong> (<code>method = 'mlpKerasDropoutCost'</code>)</p>
<p>For classification using package <span class="pkg">keras</span> with tuning parameters:</p><ul>
<li><p>Number of Hidden Units (<code>size</code>, numeric)</p></li>
<li><p>Dropout Rate (<code>dropout</code>, numeric)</p></li>
<li><p>Batch Size (<code>batch_size</code>, numeric)</p></li>
<li><p>Learning Rate (<code>lr</code>, numeric)</p></li>
<li><p>Rho (<code>rho</code>, numeric)</p></li>
<li><p>Learning Rate Decay (<code>decay</code>, numeric)</p></li>
<li><p>Cost (<code>cost</code>, numeric)</p></li>
<li><p>Activation Function (<code>activation</code>, character)</p></li>
</ul>

<p>Note: After <code>train</code> completes, the keras model object is serialized so that it can be used between R session. When predicting, the code will temporarily unsearalize the object. To make the predictions more efficient, the user might want to use <code>keras::unsearlize_model(object$finalModel$object)</code> in the current R session so that that operation is only done once. Also, this model cannot be run in parallel due to the nature of how tensorflow does the computations. Finally, the cost parameter weights the first class in the outcome vector. Unlike other packages used by <code>train</code>, the <code>dplyr</code> package is fully loaded when this model is used.</p>
<p><strong>Multilayer Perceptron Network with Weight Decay</strong> (<code>method = 'mlpKerasDecay'</code>)</p>
<p>For classification and regression using package <span class="pkg">keras</span> with tuning parameters:</p><ul>
<li><p>Number of Hidden Units (<code>size</code>, numeric)</p></li>
<li><p>L2 Regularization (<code>lambda</code>, numeric)</p></li>
<li><p>Batch Size (<code>batch_size</code>, numeric)</p></li>
<li><p>Learning Rate (<code>lr</code>, numeric)</p></li>
<li><p>Rho (<code>rho</code>, numeric)</p></li>
<li><p>Learning Rate Decay (<code>decay</code>, numeric)</p></li>
<li><p>Activation Function (<code>activation</code>, character)</p></li>
</ul>

<p>Note: After <code>train</code> completes, the keras model object is serialized so that it can be used between R session. When predicting, the code will temporarily unsearalize the object. To make the predictions more efficient, the user might want to use <code>keras::unsearlize_model(object$finalModel$object)</code> in the current R session so that that operation is only done once. Also, this model cannot be run in parallel due to the nature of how tensorflow does the computations. Unlike other packages used by <code>train</code>, the <code>dplyr</code> package is fully loaded when this model is used.</p>
<p><strong>Multilayer Perceptron Network with Weight Decay</strong> (<code>method = 'mlpKerasDecayCost'</code>)</p>
<p>For classification using package <span class="pkg">keras</span> with tuning parameters:</p><ul>
<li><p>Number of Hidden Units (<code>size</code>, numeric)</p></li>
<li><p>L2 Regularization (<code>lambda</code>, numeric)</p></li>
<li><p>Batch Size (<code>batch_size</code>, numeric)</p></li>
<li><p>Learning Rate (<code>lr</code>, numeric)</p></li>
<li><p>Rho (<code>rho</code>, numeric)</p></li>
<li><p>Learning Rate Decay (<code>decay</code>, numeric)</p></li>
<li><p>Cost (<code>cost</code>, numeric)</p></li>
<li><p>Activation Function (<code>activation</code>, character)</p></li>
</ul>

<p>Note: After <code>train</code> completes, the keras model object is serialized so that it can be used between R session. When predicting, the code will temporarily unsearalize the object. To make the predictions more efficient, the user might want to use <code>keras::unsearlize_model(object$finalModel$object)</code> in the current R session so that that operation is only done once. Also, this model cannot be run in parallel due to the nature of how tensorflow does the computations. Finally, the cost parameter weights the first class in the outcome vector. Unlike other packages used by <code>train</code>, the <code>dplyr</code> package is fully loaded when this model is used.</p>
<p><strong>Multivariate Adaptive Regression Spline</strong> (<code>method = 'earth'</code>)</p>
<p>For classification and regression using package <span class="pkg">earth</span> with tuning parameters:</p><ul>
<li><p>Number of Terms (<code>nprune</code>, numeric)</p></li>
<li><p>Product Degree (<code>degree</code>, numeric)</p></li>
</ul>

<p>Note: Unlike other packages used by <code>train</code>, the <code>earth</code> package is fully loaded when this model is used.</p>
<p><strong>Multivariate Adaptive Regression Splines</strong> (<code>method = 'gcvEarth'</code>)</p>
<p>For classification and regression using package <span class="pkg">earth</span> with tuning parameters:</p><ul>
<li><p>Product Degree (<code>degree</code>, numeric)</p></li>
</ul>

<p>Note: Unlike other packages used by <code>train</code>, the <code>earth</code> package is fully loaded when this model is used.</p>
<p><strong>Naive Bayes</strong> (<code>method = 'naive_bayes'</code>)</p>
<p>For classification using package <span class="pkg">naivebayes</span> with tuning parameters:</p><ul>
<li><p>Laplace Correction (<code>laplace</code>, numeric)</p></li>
<li><p>Distribution Type (<code>usekernel</code>, logical)</p></li>
<li><p>Bandwidth Adjustment (<code>adjust</code>, numeric)</p></li>
</ul>


<p><strong>Naive Bayes</strong> (<code>method = 'nb'</code>)</p>
<p>For classification using package <span class="pkg">klaR</span> with tuning parameters:</p><ul>
<li><p>Laplace Correction (<code>fL</code>, numeric)</p></li>
<li><p>Distribution Type (<code>usekernel</code>, logical)</p></li>
<li><p>Bandwidth Adjustment (<code>adjust</code>, numeric)</p></li>
</ul>


<p><strong>Naive Bayes Classifier</strong> (<code>method = 'nbDiscrete'</code>)</p>
<p>For classification using package <span class="pkg">bnclassify</span> with tuning parameters:</p><ul>
<li><p>Smoothing Parameter (<code>smooth</code>, numeric)</p></li>
</ul>


<p><strong>Naive Bayes Classifier with Attribute Weighting</strong> (<code>method = 'awnb'</code>)</p>
<p>For classification using package <span class="pkg">bnclassify</span> with tuning parameters:</p><ul>
<li><p>Smoothing Parameter (<code>smooth</code>, numeric)</p></li>
</ul>


<p><strong>Nearest Shrunken Centroids</strong> (<code>method = 'pam'</code>)</p>
<p>For classification using package <span class="pkg">pamr</span> with tuning parameters:</p><ul>
<li><p>Shrinkage Threshold (<code>threshold</code>, numeric)</p></li>
</ul>


<p><strong>Negative Binomial Generalized Linear Model</strong> (<code>method = 'glm.nb'</code>)</p>
<p>For regression using package <span class="pkg">MASS</span> with tuning parameters:</p><ul>
<li><p>Link Function (<code>link</code>, character)</p></li>
</ul>


<p><strong>Neural Network</strong> (<code>method = 'mxnet'</code>)</p>
<p>For classification and regression using package <span class="pkg">mxnet</span> with tuning parameters:</p><ul>
<li><p>Number of Hidden Units in Layer 1 (<code>layer1</code>, numeric)</p></li>
<li><p>Number of Hidden Units in Layer 2 (<code>layer2</code>, numeric)</p></li>
<li><p>Number of Hidden Units in Layer 3 (<code>layer3</code>, numeric)</p></li>
<li><p>Learning Rate (<code>learning.rate</code>, numeric)</p></li>
<li><p>Momentum (<code>momentum</code>, numeric)</p></li>
<li><p>Dropout Rate (<code>dropout</code>, numeric)</p></li>
<li><p>Activation Function (<code>activation</code>, character)</p></li>
</ul>

<p>Note: The <code>mxnet</code> package is not yet on CRAN. See <a href='http://mxnet.io'>http://mxnet.io</a> for installation instructions.</p>
<p><strong>Neural Network</strong> (<code>method = 'mxnetAdam'</code>)</p>
<p>For classification and regression using package <span class="pkg">mxnet</span> with tuning parameters:</p><ul>
<li><p>Number of Hidden Units in Layer 1 (<code>layer1</code>, numeric)</p></li>
<li><p>Number of Hidden Units in Layer 2 (<code>layer2</code>, numeric)</p></li>
<li><p>Number of Hidden Units in Layer 3 (<code>layer3</code>, numeric)</p></li>
<li><p>Dropout Rate (<code>dropout</code>, numeric)</p></li>
<li><p>beta1 (<code>beta1</code>, numeric)</p></li>
<li><p>beta2 (<code>beta2</code>, numeric)</p></li>
<li><p>Learning Rate (<code>learningrate</code>, numeric)</p></li>
<li><p>Activation Function (<code>activation</code>, character)</p></li>
</ul>

<p>Note: The <code>mxnet</code> package is not yet on CRAN. See <a href='http://mxnet.io'>http://mxnet.io</a> for installation instructions. Users are strongly advised to define <code>num.round</code> themselves.</p>
<p><strong>Neural Network</strong> (<code>method = 'neuralnet'</code>)</p>
<p>For regression using package <span class="pkg">neuralnet</span> with tuning parameters:</p><ul>
<li><p>Number of Hidden Units in Layer 1 (<code>layer1</code>, numeric)</p></li>
<li><p>Number of Hidden Units in Layer 2 (<code>layer2</code>, numeric)</p></li>
<li><p>Number of Hidden Units in Layer 3 (<code>layer3</code>, numeric)</p></li>
</ul>


<p><strong>Neural Network</strong> (<code>method = 'nnet'</code>)</p>
<p>For classification and regression using package <span class="pkg">nnet</span> with tuning parameters:</p><ul>
<li><p>Number of Hidden Units (<code>size</code>, numeric)</p></li>
<li><p>Weight Decay (<code>decay</code>, numeric)</p></li>
</ul>


<p><strong>Neural Networks with Feature Extraction</strong> (<code>method = 'pcaNNet'</code>)</p>
<p>For classification and regression using package <span class="pkg">nnet</span> with tuning parameters:</p><ul>
<li><p>Number of Hidden Units (<code>size</code>, numeric)</p></li>
<li><p>Weight Decay (<code>decay</code>, numeric)</p></li>
</ul>


<p><strong>Non-Convex Penalized Quantile Regression</strong> (<code>method = 'rqnc'</code>)</p>
<p>For regression using package <span class="pkg">rqPen</span> with tuning parameters:</p><ul>
<li><p>L1 Penalty (<code>lambda</code>, numeric)</p></li>
<li><p>Penalty Type (<code>penalty</code>, character)</p></li>
</ul>


<p><strong>Non-Informative Model</strong> (<code>method = 'null'</code>)</p>
<p>For classification and regression with no tuning parameters.</p>
<p>Note: Since this model always predicts the same value, R-squared values will always be estimated to be NA.</p>
<p><strong>Non-Negative Least Squares</strong> (<code>method = 'nnls'</code>)</p>
<p>For regression using package <span class="pkg">nnls</span> with no tuning parameters.</p>

<p><strong>Oblique Random Forest</strong> (<code>method = 'ORFlog'</code>)</p>
<p>For classification using package <span class="pkg">obliqueRF</span> with tuning parameters:</p><ul>
<li><p>Number of Randomly Selected Predictors (<code>mtry</code>, numeric)</p></li>
</ul>

<p>Note: Unlike other packages used by <code>train</code>, the <code>obliqueRF</code> package is fully loaded when this model is used.</p>
<p><strong>Oblique Random Forest</strong> (<code>method = 'ORFpls'</code>)</p>
<p>For classification using package <span class="pkg">obliqueRF</span> with tuning parameters:</p><ul>
<li><p>Number of Randomly Selected Predictors (<code>mtry</code>, numeric)</p></li>
</ul>

<p>Note: Unlike other packages used by <code>train</code>, the <code>obliqueRF</code> package is fully loaded when this model is used.</p>
<p><strong>Oblique Random Forest</strong> (<code>method = 'ORFridge'</code>)</p>
<p>For classification using package <span class="pkg">obliqueRF</span> with tuning parameters:</p><ul>
<li><p>Number of Randomly Selected Predictors (<code>mtry</code>, numeric)</p></li>
</ul>

<p>Note: Unlike other packages used by <code>train</code>, the <code>obliqueRF</code> package is fully loaded when this model is used.</p>
<p><strong>Oblique Random Forest</strong> (<code>method = 'ORFsvm'</code>)</p>
<p>For classification using package <span class="pkg">obliqueRF</span> with tuning parameters:</p><ul>
<li><p>Number of Randomly Selected Predictors (<code>mtry</code>, numeric)</p></li>
</ul>

<p>Note: Unlike other packages used by <code>train</code>, the <code>obliqueRF</code> package is fully loaded when this model is used.</p>
<p><strong>Optimal Weighted Nearest Neighbor Classifier</strong> (<code>method = 'ownn'</code>)</p>
<p>For classification using package <span class="pkg">snn</span> with tuning parameters:</p><ul>
<li><p>Number of Neighbors (<code>K</code>, numeric)</p></li>
</ul>


<p><strong>Ordered Logistic or Probit Regression</strong> (<code>method = 'polr'</code>)</p>
<p>For classification using package <span class="pkg">MASS</span> with tuning parameters:</p><ul>
<li><p>parameter (<code>method</code>, character)</p></li>
</ul>


<p><strong>Parallel Random Forest</strong> (<code>method = 'parRF'</code>)</p>
<p>For classification and regression using packages <span class="pkg">e1071</span>, <span class="pkg">randomForest</span>, <span class="pkg">foreach</span> and <span class="pkg">import</span> with tuning parameters:</p><ul>
<li><p>Number of Randomly Selected Predictors (<code>mtry</code>, numeric)</p></li>
</ul>


<p><strong>partDSA</strong> (<code>method = 'partDSA'</code>)</p>
<p>For classification and regression using package <span class="pkg">partDSA</span> with tuning parameters:</p><ul>
<li><p>Number of Terminal Partitions (<code>cut.off.growth</code>, numeric)</p></li>
<li><p>Minimum Percent Difference (<code>MPD</code>, numeric)</p></li>
</ul>


<p><strong>Partial Least Squares</strong> (<code>method = 'kernelpls'</code>)</p>
<p>For classification and regression using package <span class="pkg">pls</span> with tuning parameters:</p><ul>
<li><p>Number of Components (<code>ncomp</code>, numeric)</p></li>
</ul>


<p><strong>Partial Least Squares</strong> (<code>method = 'pls'</code>)</p>
<p>For classification and regression using package <span class="pkg">pls</span> with tuning parameters:</p><ul>
<li><p>Number of Components (<code>ncomp</code>, numeric)</p></li>
</ul>


<p><strong>Partial Least Squares</strong> (<code>method = 'simpls'</code>)</p>
<p>For classification and regression using package <span class="pkg">pls</span> with tuning parameters:</p><ul>
<li><p>Number of Components (<code>ncomp</code>, numeric)</p></li>
</ul>


<p><strong>Partial Least Squares</strong> (<code>method = 'widekernelpls'</code>)</p>
<p>For classification and regression using package <span class="pkg">pls</span> with tuning parameters:</p><ul>
<li><p>Number of Components (<code>ncomp</code>, numeric)</p></li>
</ul>


<p><strong>Partial Least Squares Generalized Linear Models </strong> (<code>method = 'plsRglm'</code>)</p>
<p>For classification and regression using package <span class="pkg">plsRglm</span> with tuning parameters:</p><ul>
<li><p>Number of PLS Components (<code>nt</code>, numeric)</p></li>
<li><p>p-Value threshold (<code>alpha.pvals.expli</code>, numeric)</p></li>
</ul>

<p>Note: Unlike other packages used by <code>train</code>, the <code>plsRglm</code> package is fully loaded when this model is used.</p>
<p><strong>Patient Rule Induction Method</strong> (<code>method = 'PRIM'</code>)</p>
<p>For classification using package <span class="pkg">supervisedPRIM</span> with tuning parameters:</p><ul>
<li><p>peeling quantile (<code>peel.alpha</code>, numeric)</p></li>
<li><p>pasting quantile (<code>paste.alpha</code>, numeric)</p></li>
<li><p>minimum mass (<code>mass.min</code>, numeric)</p></li>
</ul>


<p><strong>Penalized Discriminant Analysis</strong> (<code>method = 'pda'</code>)</p>
<p>For classification using package <span class="pkg">mda</span> with tuning parameters:</p><ul>
<li><p>Shrinkage Penalty Coefficient (<code>lambda</code>, numeric)</p></li>
</ul>


<p><strong>Penalized Discriminant Analysis</strong> (<code>method = 'pda2'</code>)</p>
<p>For classification using package <span class="pkg">mda</span> with tuning parameters:</p><ul>
<li><p>Degrees of Freedom (<code>df</code>, numeric)</p></li>
</ul>


<p><strong>Penalized Linear Discriminant Analysis</strong> (<code>method = 'PenalizedLDA'</code>)</p>
<p>For classification using packages <span class="pkg">penalizedLDA</span> and <span class="pkg">plyr</span> with tuning parameters:</p><ul>
<li><p>L1 Penalty (<code>lambda</code>, numeric)</p></li>
<li><p>Number of Discriminant Functions (<code>K</code>, numeric)</p></li>
</ul>


<p><strong>Penalized Linear Regression</strong> (<code>method = 'penalized'</code>)</p>
<p>For regression using package <span class="pkg">penalized</span> with tuning parameters:</p><ul>
<li><p>L1 Penalty (<code>lambda1</code>, numeric)</p></li>
<li><p>L2 Penalty (<code>lambda2</code>, numeric)</p></li>
</ul>


<p><strong>Penalized Logistic Regression</strong> (<code>method = 'plr'</code>)</p>
<p>For classification using package <span class="pkg">stepPlr</span> with tuning parameters:</p><ul>
<li><p>L2 Penalty (<code>lambda</code>, numeric)</p></li>
<li><p>Complexity Parameter (<code>cp</code>, character)</p></li>
</ul>


<p><strong>Penalized Multinomial Regression</strong> (<code>method = 'multinom'</code>)</p>
<p>For classification using package <span class="pkg">nnet</span> with tuning parameters:</p><ul>
<li><p>Weight Decay (<code>decay</code>, numeric)</p></li>
</ul>


<p><strong>Penalized Ordinal Regression</strong> (<code>method = 'ordinalNet'</code>)</p>
<p>For classification using packages <span class="pkg">ordinalNet</span> and <span class="pkg">plyr</span> with tuning parameters:</p><ul>
<li><p>Mixing Percentage (<code>alpha</code>, numeric)</p></li>
<li><p>Selection Criterion (<code>criteria</code>, character)</p></li>
<li><p>Link Function (<code>link</code>, character)</p></li>
</ul>

<p>Note: Requires ordinalNet package version &gt;= 2.0</p>
<p><strong>Polynomial Kernel Regularized Least Squares</strong> (<code>method = 'krlsPoly'</code>)</p>
<p>For regression using package <span class="pkg">KRLS</span> with tuning parameters:</p><ul>
<li><p>Regularization Parameter (<code>lambda</code>, numeric)</p></li>
<li><p>Polynomial Degree (<code>degree</code>, numeric)</p></li>
</ul>


<p><strong>Principal Component Analysis</strong> (<code>method = 'pcr'</code>)</p>
<p>For regression using package <span class="pkg">pls</span> with tuning parameters:</p><ul>
<li><p>Number of Components (<code>ncomp</code>, numeric)</p></li>
</ul>


<p><strong>Projection Pursuit Regression</strong> (<code>method = 'ppr'</code>)</p>
<p>For regression with tuning parameters:</p><ul>
<li><p>Number of  Terms (<code>nterms</code>, numeric)</p></li>
</ul>


<p><strong>Quadratic Discriminant Analysis</strong> (<code>method = 'qda'</code>)</p>
<p>For classification using package <span class="pkg">MASS</span> with no tuning parameters.</p>

<p><strong>Quadratic Discriminant Analysis with Stepwise Feature Selection</strong> (<code>method = 'stepQDA'</code>)</p>
<p>For classification using packages <span class="pkg">klaR</span> and <span class="pkg">MASS</span> with tuning parameters:</p><ul>
<li><p>Maximum Number of Variables (<code>maxvar</code>, numeric)</p></li>
<li><p>Search Direction (<code>direction</code>, character)</p></li>
</ul>


<p><strong>Quantile Random Forest</strong> (<code>method = 'qrf'</code>)</p>
<p>For regression using package <span class="pkg">quantregForest</span> with tuning parameters:</p><ul>
<li><p>Number of Randomly Selected Predictors (<code>mtry</code>, numeric)</p></li>
</ul>


<p><strong>Quantile Regression Neural Network</strong> (<code>method = 'qrnn'</code>)</p>
<p>For regression using package <span class="pkg">qrnn</span> with tuning parameters:</p><ul>
<li><p>Number of Hidden Units (<code>n.hidden</code>, numeric)</p></li>
<li><p>Weight Decay (<code>penalty</code>, numeric)</p></li>
<li><p>Bagged Models? (<code>bag</code>, logical)</p></li>
</ul>


<p><strong>Quantile Regression with LASSO penalty</strong> (<code>method = 'rqlasso'</code>)</p>
<p>For regression using package <span class="pkg">rqPen</span> with tuning parameters:</p><ul>
<li><p>L1 Penalty (<code>lambda</code>, numeric)</p></li>
</ul>


<p><strong>Radial Basis Function Kernel Regularized Least Squares</strong> (<code>method = 'krlsRadial'</code>)</p>
<p>For regression using packages <span class="pkg">KRLS</span> and <span class="pkg">kernlab</span> with tuning parameters:</p><ul>
<li><p>Regularization Parameter (<code>lambda</code>, numeric)</p></li>
<li><p>Sigma (<code>sigma</code>, numeric)</p></li>
</ul>


<p><strong>Radial Basis Function Network</strong> (<code>method = 'rbf'</code>)</p>
<p>For classification and regression using package <span class="pkg">RSNNS</span> with tuning parameters:</p><ul>
<li><p>Number of Hidden Units (<code>size</code>, numeric)</p></li>
</ul>


<p><strong>Radial Basis Function Network</strong> (<code>method = 'rbfDDA'</code>)</p>
<p>For classification and regression using package <span class="pkg">RSNNS</span> with tuning parameters:</p><ul>
<li><p>Activation Limit for Conflicting Classes (<code>negativeThreshold</code>, numeric)</p></li>
</ul>


<p><strong>Random Ferns</strong> (<code>method = 'rFerns'</code>)</p>
<p>For classification using package <span class="pkg">rFerns</span> with tuning parameters:</p><ul>
<li><p>Fern Depth (<code>depth</code>, numeric)</p></li>
</ul>


<p><strong>Random Forest</strong> (<code>method = 'ranger'</code>)</p>
<p>For classification and regression using packages <span class="pkg">e1071</span>, <span class="pkg">ranger</span> and <span class="pkg">dplyr</span> with tuning parameters:</p><ul>
<li><p>Number of Randomly Selected Predictors (<code>mtry</code>, numeric)</p></li>
<li><p>Splitting Rule (<code>splitrule</code>, character)</p></li>
<li><p>Minimal Node Size (<code>min.node.size</code>, numeric)</p></li>
</ul>


<p><strong>Random Forest</strong> (<code>method = 'Rborist'</code>)</p>
<p>For classification and regression using package <span class="pkg">Rborist</span> with tuning parameters:</p><ul>
<li><p>Number of Randomly Selected Predictors (<code>predFixed</code>, numeric)</p></li>
<li><p>Minimal Node Size (<code>minNode</code>, numeric)</p></li>
</ul>


<p><strong>Random Forest</strong> (<code>method = 'rf'</code>)</p>
<p>For classification and regression using package <span class="pkg">randomForest</span> with tuning parameters:</p><ul>
<li><p>Number of Randomly Selected Predictors (<code>mtry</code>, numeric)</p></li>
</ul>


<p><strong>Random Forest by Randomization</strong> (<code>method = 'extraTrees'</code>)</p>
<p>For classification and regression using package <span class="pkg">extraTrees</span> with tuning parameters:</p><ul>
<li><p>Number of  Randomly Selected Predictors (<code>mtry</code>, numeric)</p></li>
<li><p>Number of  Random Cuts (<code>numRandomCuts</code>, numeric)</p></li>
</ul>


<p><strong>Random Forest Rule-Based Model</strong> (<code>method = 'rfRules'</code>)</p>
<p>For classification and regression using packages <span class="pkg">randomForest</span>, <span class="pkg">inTrees</span> and <span class="pkg">plyr</span> with tuning parameters:</p><ul>
<li><p>Number of Randomly Selected Predictors (<code>mtry</code>, numeric)</p></li>
<li><p>Maximum Rule Depth (<code>maxdepth</code>, numeric)</p></li>
</ul>


<p><strong>Regularized Discriminant Analysis</strong> (<code>method = 'rda'</code>)</p>
<p>For classification using package <span class="pkg">klaR</span> with tuning parameters:</p><ul>
<li><p>Gamma (<code>gamma</code>, numeric)</p></li>
<li><p>Lambda (<code>lambda</code>, numeric)</p></li>
</ul>


<p><strong>Regularized Linear Discriminant Analysis</strong> (<code>method = 'rlda'</code>)</p>
<p>For classification using package <span class="pkg">sparsediscrim</span> with tuning parameters:</p><ul>
<li><p>Regularization Method (<code>estimator</code>, character)</p></li>
</ul>


<p><strong>Regularized Logistic Regression</strong> (<code>method = 'regLogistic'</code>)</p>
<p>For classification using package <span class="pkg">LiblineaR</span> with tuning parameters:</p><ul>
<li><p>Cost (<code>cost</code>, numeric)</p></li>
<li><p>Loss Function (<code>loss</code>, character)</p></li>
<li><p>Tolerance (<code>epsilon</code>, numeric)</p></li>
</ul>


<p><strong>Regularized Random Forest</strong> (<code>method = 'RRF'</code>)</p>
<p>For classification and regression using packages <span class="pkg">randomForest</span> and <span class="pkg">RRF</span> with tuning parameters:</p><ul>
<li><p>Number of Randomly Selected Predictors (<code>mtry</code>, numeric)</p></li>
<li><p>Regularization Value (<code>coefReg</code>, numeric)</p></li>
<li><p>Importance Coefficient (<code>coefImp</code>, numeric)</p></li>
</ul>


<p><strong>Regularized Random Forest</strong> (<code>method = 'RRFglobal'</code>)</p>
<p>For classification and regression using package <span class="pkg">RRF</span> with tuning parameters:</p><ul>
<li><p>Number of Randomly Selected Predictors (<code>mtry</code>, numeric)</p></li>
<li><p>Regularization Value (<code>coefReg</code>, numeric)</p></li>
</ul>


<p><strong>Relaxed Lasso</strong> (<code>method = 'relaxo'</code>)</p>
<p>For regression using packages <span class="pkg">relaxo</span> and <span class="pkg">plyr</span> with tuning parameters:</p><ul>
<li><p>Penalty Parameter (<code>lambda</code>, numeric)</p></li>
<li><p>Relaxation Parameter (<code>phi</code>, numeric)</p></li>
</ul>


<p><strong>Relevance Vector Machines with Linear Kernel</strong> (<code>method = 'rvmLinear'</code>)</p>
<p>For regression using package <span class="pkg">kernlab</span> with no tuning parameters.</p>

<p><strong>Relevance Vector Machines with Polynomial Kernel</strong> (<code>method = 'rvmPoly'</code>)</p>
<p>For regression using package <span class="pkg">kernlab</span> with tuning parameters:</p><ul>
<li><p>Scale (<code>scale</code>, numeric)</p></li>
<li><p>Polynomial Degree (<code>degree</code>, numeric)</p></li>
</ul>


<p><strong>Relevance Vector Machines with Radial Basis Function Kernel</strong> (<code>method = 'rvmRadial'</code>)</p>
<p>For regression using package <span class="pkg">kernlab</span> with tuning parameters:</p><ul>
<li><p>Sigma (<code>sigma</code>, numeric)</p></li>
</ul>


<p><strong>Ridge Regression</strong> (<code>method = 'ridge'</code>)</p>
<p>For regression using package <span class="pkg">elasticnet</span> with tuning parameters:</p><ul>
<li><p>Weight Decay (<code>lambda</code>, numeric)</p></li>
</ul>


<p><strong>Ridge Regression with Variable Selection</strong> (<code>method = 'foba'</code>)</p>
<p>For regression using package <span class="pkg">foba</span> with tuning parameters:</p><ul>
<li><p>Number of Variables Retained (<code>k</code>, numeric)</p></li>
<li><p>L2 Penalty (<code>lambda</code>, numeric)</p></li>
</ul>


<p><strong>Robust Linear Discriminant Analysis</strong> (<code>method = 'Linda'</code>)</p>
<p>For classification using package <span class="pkg">rrcov</span> with no tuning parameters.</p>

<p><strong>Robust Linear Model</strong> (<code>method = 'rlm'</code>)</p>
<p>For regression using package <span class="pkg">MASS</span> with tuning parameters:</p><ul>
<li><p>intercept (<code>intercept</code>, logical)</p></li>
<li><p>psi (<code>psi</code>, character)</p></li>
</ul>


<p><strong>Robust Mixture Discriminant Analysis</strong> (<code>method = 'rmda'</code>)</p>
<p>For classification using package <span class="pkg">robustDA</span> with tuning parameters:</p><ul>
<li><p>Number of Subclasses Per Class (<code>K</code>, numeric)</p></li>
<li><p>Model (<code>model</code>, character)</p></li>
</ul>


<p><strong>Robust Quadratic Discriminant Analysis</strong> (<code>method = 'QdaCov'</code>)</p>
<p>For classification using package <span class="pkg">rrcov</span> with no tuning parameters.</p>

<p><strong>Robust Regularized Linear Discriminant Analysis</strong> (<code>method = 'rrlda'</code>)</p>
<p>For classification using package <span class="pkg">rrlda</span> with tuning parameters:</p><ul>
<li><p>Penalty Parameter (<code>lambda</code>, numeric)</p></li>
<li><p>Robustness Parameter (<code>hp</code>, numeric)</p></li>
<li><p>Penalty Type (<code>penalty</code>, character)</p></li>
</ul>

<p>Note: Unlike other packages used by <code>train</code>, the <code>rrlda</code> package is fully loaded when this model is used.</p>
<p><strong>Robust SIMCA</strong> (<code>method = 'RSimca'</code>)</p>
<p>For classification using package <span class="pkg">rrcovHD</span> with no tuning parameters.</p>
<p>Note: Unlike other packages used by <code>train</code>, the <code>rrcovHD</code> package is fully loaded when this model is used.</p>
<p><strong>ROC-Based Classifier</strong> (<code>method = 'rocc'</code>)</p>
<p>For classification using package <span class="pkg">rocc</span> with tuning parameters:</p><ul>
<li><p>Number of Variables Retained (<code>xgenes</code>, numeric)</p></li>
</ul>


<p><strong>Rotation Forest</strong> (<code>method = 'rotationForest'</code>)</p>
<p>For classification using package <span class="pkg">rotationForest</span> with tuning parameters:</p><ul>
<li><p>Number of Variable Subsets (<code>K</code>, numeric)</p></li>
<li><p>Ensemble Size (<code>L</code>, numeric)</p></li>
</ul>


<p><strong>Rotation Forest</strong> (<code>method = 'rotationForestCp'</code>)</p>
<p>For classification using packages <span class="pkg">rpart</span>, <span class="pkg">plyr</span> and <span class="pkg">rotationForest</span> with tuning parameters:</p><ul>
<li><p>Number of Variable Subsets (<code>K</code>, numeric)</p></li>
<li><p>Ensemble Size (<code>L</code>, numeric)</p></li>
<li><p>Complexity Parameter (<code>cp</code>, numeric)</p></li>
</ul>


<p><strong>Rule-Based Classifier</strong> (<code>method = 'JRip'</code>)</p>
<p>For classification using package <span class="pkg">RWeka</span> with tuning parameters:</p><ul>
<li><p>Number of  Optimizations (<code>NumOpt</code>, numeric)</p></li>
<li><p>Number of  Folds (<code>NumFolds</code>, numeric)</p></li>
<li><p>Min Weights (<code>MinWeights</code>, numeric)</p></li>
</ul>


<p><strong>Rule-Based Classifier</strong> (<code>method = 'PART'</code>)</p>
<p>For classification using package <span class="pkg">RWeka</span> with tuning parameters:</p><ul>
<li><p>Confidence Threshold (<code>threshold</code>, numeric)</p></li>
<li><p>Pruning (<code>pruned</code>, character)</p></li>
</ul>


<p><strong>Self-Organizing Maps</strong> (<code>method = 'xyf'</code>)</p>
<p>For classification and regression using package <span class="pkg">kohonen</span> with tuning parameters:</p><ul>
<li><p>Rows (<code>xdim</code>, numeric)</p></li>
<li><p>Columns (<code>ydim</code>, numeric)</p></li>
<li><p>Layer Weight (<code>user.weights</code>, numeric)</p></li>
<li><p>Topology (<code>topo</code>, character)</p></li>
</ul>

<p>Note: As of version 3.0.0 of the kohonen package, the argument <code>user.weights</code> replaces the old <code>alpha</code> parameter. <code>user.weights</code> is usually a vector of relative weights such as <code><a href='https://rdrr.io/r/base/c.html'>c(1, 3)</a></code> but is parameterized here as a proportion such as <code><a href='https://rdrr.io/r/base/c.html'>c(1-.75, .75)</a></code> where the .75 is the value of the tuning parameter passed to <code>train</code> and indicates that the outcome layer has 3 times the weight as the predictor layer.</p>
<p><strong>Semi-Naive Structure Learner Wrapper</strong> (<code>method = 'nbSearch'</code>)</p>
<p>For classification using package <span class="pkg">bnclassify</span> with tuning parameters:</p><ul>
<li><p>Number of Folds (<code>k</code>, numeric)</p></li>
<li><p>Minimum Absolute Improvement (<code>epsilon</code>, numeric)</p></li>
<li><p>Smoothing Parameter (<code>smooth</code>, numeric)</p></li>
<li><p>Final Smoothing Parameter (<code>final_smooth</code>, numeric)</p></li>
<li><p>Search Direction (<code>direction</code>, character)</p></li>
</ul>


<p><strong>Shrinkage Discriminant Analysis</strong> (<code>method = 'sda'</code>)</p>
<p>For classification using package <span class="pkg">sda</span> with tuning parameters:</p><ul>
<li><p>Diagonalize (<code>diagonal</code>, logical)</p></li>
<li><p>shrinkage (<code>lambda</code>, numeric)</p></li>
</ul>


<p><strong>SIMCA</strong> (<code>method = 'CSimca'</code>)</p>
<p>For classification using packages <span class="pkg">rrcov</span> and <span class="pkg">rrcovHD</span> with no tuning parameters.</p>

<p><strong>Simplified TSK Fuzzy Rules</strong> (<code>method = 'FS.HGD'</code>)</p>
<p>For regression using package <span class="pkg">frbs</span> with tuning parameters:</p><ul>
<li><p>Number of Fuzzy Terms (<code>num.labels</code>, numeric)</p></li>
<li><p>Max. Iterations (<code>max.iter</code>, numeric)</p></li>
</ul>


<p><strong>Single C5.0 Ruleset</strong> (<code>method = 'C5.0Rules'</code>)</p>
<p>For classification using package <span class="pkg">C50</span> with no tuning parameters.</p>

<p><strong>Single C5.0 Tree</strong> (<code>method = 'C5.0Tree'</code>)</p>
<p>For classification using package <span class="pkg">C50</span> with no tuning parameters.</p>

<p><strong>Single Rule Classification</strong> (<code>method = 'OneR'</code>)</p>
<p>For classification using package <span class="pkg">RWeka</span> with no tuning parameters.</p>

<p><strong>Sparse Distance Weighted Discrimination</strong> (<code>method = 'sdwd'</code>)</p>
<p>For classification using package <span class="pkg">sdwd</span> with tuning parameters:</p><ul>
<li><p>L1 Penalty (<code>lambda</code>, numeric)</p></li>
<li><p>L2 Penalty (<code>lambda2</code>, numeric)</p></li>
</ul>


<p><strong>Sparse Linear Discriminant Analysis</strong> (<code>method = 'sparseLDA'</code>)</p>
<p>For classification using package <span class="pkg">sparseLDA</span> with tuning parameters:</p><ul>
<li><p>Number of  Predictors (<code>NumVars</code>, numeric)</p></li>
<li><p>Lambda (<code>lambda</code>, numeric)</p></li>
</ul>


<p><strong>Sparse Mixture Discriminant Analysis</strong> (<code>method = 'smda'</code>)</p>
<p>For classification using package <span class="pkg">sparseLDA</span> with tuning parameters:</p><ul>
<li><p>Number of  Predictors (<code>NumVars</code>, numeric)</p></li>
<li><p>Lambda (<code>lambda</code>, numeric)</p></li>
<li><p>Number of  Subclasses (<code>R</code>, numeric)</p></li>
</ul>


<p><strong>Sparse Partial Least Squares</strong> (<code>method = 'spls'</code>)</p>
<p>For classification and regression using package <span class="pkg">spls</span> with tuning parameters:</p><ul>
<li><p>Number of Components (<code>K</code>, numeric)</p></li>
<li><p>Threshold (<code>eta</code>, numeric)</p></li>
<li><p>Kappa (<code>kappa</code>, numeric)</p></li>
</ul>


<p><strong>Spike and Slab Regression</strong> (<code>method = 'spikeslab'</code>)</p>
<p>For regression using packages <span class="pkg">spikeslab</span> and <span class="pkg">plyr</span> with tuning parameters:</p><ul>
<li><p>Variables Retained (<code>vars</code>, numeric)</p></li>
</ul>

<p>Note: Unlike other packages used by <code>train</code>, the <code>spikeslab</code> package is fully loaded when this model is used.</p>
<p><strong>Stabilized Linear Discriminant Analysis</strong> (<code>method = 'slda'</code>)</p>
<p>For classification using package <span class="pkg">ipred</span> with no tuning parameters.</p>

<p><strong>Stabilized Nearest Neighbor Classifier</strong> (<code>method = 'snn'</code>)</p>
<p>For classification using package <span class="pkg">snn</span> with tuning parameters:</p><ul>
<li><p>Stabilization Parameter (<code>lambda</code>, numeric)</p></li>
</ul>


<p><strong>Stacked AutoEncoder Deep Neural Network</strong> (<code>method = 'dnn'</code>)</p>
<p>For classification and regression using package <span class="pkg">deepnet</span> with tuning parameters:</p><ul>
<li><p>Hidden Layer 1 (<code>layer1</code>, numeric)</p></li>
<li><p>Hidden Layer 2 (<code>layer2</code>, numeric)</p></li>
<li><p>Hidden Layer 3 (<code>layer3</code>, numeric)</p></li>
<li><p>Hidden Dropouts (<code>hidden_dropout</code>, numeric)</p></li>
<li><p>Visible Dropout (<code>visible_dropout</code>, numeric)</p></li>
</ul>


<p><strong>Stochastic Gradient Boosting</strong> (<code>method = 'gbm'</code>)</p>
<p>For classification and regression using packages <span class="pkg">gbm</span> and <span class="pkg">plyr</span> with tuning parameters:</p><ul>
<li><p>Number of  Boosting Iterations (<code>n.trees</code>, numeric)</p></li>
<li><p>Max Tree Depth (<code>interaction.depth</code>, numeric)</p></li>
<li><p>Shrinkage (<code>shrinkage</code>, numeric)</p></li>
<li><p>Min. Terminal Node Size (<code>n.minobsinnode</code>, numeric)</p></li>
</ul>


<p><strong>Subtractive Clustering and Fuzzy c-Means Rules</strong> (<code>method = 'SBC'</code>)</p>
<p>For regression using package <span class="pkg">frbs</span> with tuning parameters:</p><ul>
<li><p>Radius (<code>r.a</code>, numeric)</p></li>
<li><p>Upper Threshold (<code>eps.high</code>, numeric)</p></li>
<li><p>Lower Threshold (<code>eps.low</code>, numeric)</p></li>
</ul>


<p><strong>Supervised Principal Component Analysis</strong> (<code>method = 'superpc'</code>)</p>
<p>For regression using package <span class="pkg">superpc</span> with tuning parameters:</p><ul>
<li><p>Threshold (<code>threshold</code>, numeric)</p></li>
<li><p>Number of Components (<code>n.components</code>, numeric)</p></li>
</ul>


<p><strong>Support Vector Machines with Boundrange String Kernel</strong> (<code>method = 'svmBoundrangeString'</code>)</p>
<p>For classification and regression using package <span class="pkg">kernlab</span> with tuning parameters:</p><ul>
<li><p>length (<code>length</code>, numeric)</p></li>
<li><p>Cost (<code>C</code>, numeric)</p></li>
</ul>


<p><strong>Support Vector Machines with Class Weights</strong> (<code>method = 'svmRadialWeights'</code>)</p>
<p>For classification using package <span class="pkg">kernlab</span> with tuning parameters:</p><ul>
<li><p>Sigma (<code>sigma</code>, numeric)</p></li>
<li><p>Cost (<code>C</code>, numeric)</p></li>
<li><p>Weight (<code>Weight</code>, numeric)</p></li>
</ul>


<p><strong>Support Vector Machines with Exponential String Kernel</strong> (<code>method = 'svmExpoString'</code>)</p>
<p>For classification and regression using package <span class="pkg">kernlab</span> with tuning parameters:</p><ul>
<li><p>lambda (<code>lambda</code>, numeric)</p></li>
<li><p>Cost (<code>C</code>, numeric)</p></li>
</ul>


<p><strong>Support Vector Machines with Linear Kernel</strong> (<code>method = 'svmLinear'</code>)</p>
<p>For classification and regression using package <span class="pkg">kernlab</span> with tuning parameters:</p><ul>
<li><p>Cost (<code>C</code>, numeric)</p></li>
</ul>


<p><strong>Support Vector Machines with Linear Kernel</strong> (<code>method = 'svmLinear2'</code>)</p>
<p>For classification and regression using package <span class="pkg">e1071</span> with tuning parameters:</p><ul>
<li><p>Cost (<code>cost</code>, numeric)</p></li>
</ul>


<p><strong>Support Vector Machines with Polynomial Kernel</strong> (<code>method = 'svmPoly'</code>)</p>
<p>For classification and regression using package <span class="pkg">kernlab</span> with tuning parameters:</p><ul>
<li><p>Polynomial Degree (<code>degree</code>, numeric)</p></li>
<li><p>Scale (<code>scale</code>, numeric)</p></li>
<li><p>Cost (<code>C</code>, numeric)</p></li>
</ul>


<p><strong>Support Vector Machines with Radial Basis Function Kernel</strong> (<code>method = 'svmRadial'</code>)</p>
<p>For classification and regression using package <span class="pkg">kernlab</span> with tuning parameters:</p><ul>
<li><p>Sigma (<code>sigma</code>, numeric)</p></li>
<li><p>Cost (<code>C</code>, numeric)</p></li>
</ul>


<p><strong>Support Vector Machines with Radial Basis Function Kernel</strong> (<code>method = 'svmRadialCost'</code>)</p>
<p>For classification and regression using package <span class="pkg">kernlab</span> with tuning parameters:</p><ul>
<li><p>Cost (<code>C</code>, numeric)</p></li>
</ul>


<p><strong>Support Vector Machines with Radial Basis Function Kernel</strong> (<code>method = 'svmRadialSigma'</code>)</p>
<p>For classification and regression using package <span class="pkg">kernlab</span> with tuning parameters:</p><ul>
<li><p>Sigma (<code>sigma</code>, numeric)</p></li>
<li><p>Cost (<code>C</code>, numeric)</p></li>
</ul>

<p>Note: This SVM model tunes over the cost parameter and the RBF kernel parameter sigma. In the latter case, using <code>tuneLength</code> will, at most, evaluate six values of the kernel parameter. This enables a broad search over the cost parameter and a relatively narrow search over <code>sigma</code></p>
<p><strong>Support Vector Machines with Spectrum String Kernel</strong> (<code>method = 'svmSpectrumString'</code>)</p>
<p>For classification and regression using package <span class="pkg">kernlab</span> with tuning parameters:</p><ul>
<li><p>length (<code>length</code>, numeric)</p></li>
<li><p>Cost (<code>C</code>, numeric)</p></li>
</ul>


<p><strong>The Bayesian lasso</strong> (<code>method = 'blasso'</code>)</p>
<p>For regression using package <span class="pkg">monomvn</span> with tuning parameters:</p><ul>
<li><p>Sparsity Threshold (<code>sparsity</code>, numeric)</p></li>
</ul>

<p>Note: This model creates predictions using the mean of the posterior distributions but sets some parameters specifically to zero based on the tuning parameter <code>sparsity</code>. For example, when <code>sparsity = .5</code>, only coefficients where at least half the posterior estimates are nonzero are used.</p>
<p><strong>The lasso</strong> (<code>method = 'lasso'</code>)</p>
<p>For regression using package <span class="pkg">elasticnet</span> with tuning parameters:</p><ul>
<li><p>Fraction of Full Solution (<code>fraction</code>, numeric)</p></li>
</ul>


<p><strong>Tree Augmented Naive Bayes Classifier</strong> (<code>method = 'tan'</code>)</p>
<p>For classification using package <span class="pkg">bnclassify</span> with tuning parameters:</p><ul>
<li><p>Score Function (<code>score</code>, character)</p></li>
<li><p>Smoothing Parameter (<code>smooth</code>, numeric)</p></li>
</ul>


<p><strong>Tree Augmented Naive Bayes Classifier Structure Learner Wrapper</strong> (<code>method = 'tanSearch'</code>)</p>
<p>For classification using package <span class="pkg">bnclassify</span> with tuning parameters:</p><ul>
<li><p>Number of Folds (<code>k</code>, numeric)</p></li>
<li><p>Minimum Absolute Improvement (<code>epsilon</code>, numeric)</p></li>
<li><p>Smoothing Parameter (<code>smooth</code>, numeric)</p></li>
<li><p>Final Smoothing Parameter (<code>final_smooth</code>, numeric)</p></li>
<li><p>Super-Parent (<code>sp</code>, logical)</p></li>
</ul>


<p><strong>Tree Augmented Naive Bayes Classifier with Attribute Weighting</strong> (<code>method = 'awtan'</code>)</p>
<p>For classification using package <span class="pkg">bnclassify</span> with tuning parameters:</p><ul>
<li><p>Score Function (<code>score</code>, character)</p></li>
<li><p>Smoothing Parameter (<code>smooth</code>, numeric)</p></li>
</ul>


<p><strong>Tree Models from Genetic Algorithms</strong> (<code>method = 'evtree'</code>)</p>
<p>For classification and regression using package <span class="pkg">evtree</span> with tuning parameters:</p><ul>
<li><p>Complexity Parameter (<code>alpha</code>, numeric)</p></li>
</ul>


<p><strong>Tree-Based Ensembles</strong> (<code>method = 'nodeHarvest'</code>)</p>
<p>For classification and regression using package <span class="pkg">nodeHarvest</span> with tuning parameters:</p><ul>
<li><p>Maximum Interaction Depth (<code>maxinter</code>, numeric)</p></li>
<li><p>Prediction Mode (<code>mode</code>, character)</p></li>
</ul>


<p><strong>Variational Bayesian Multinomial Probit Regression</strong> (<code>method = 'vbmpRadial'</code>)</p>
<p>For classification using package <span class="pkg">vbmp</span> with tuning parameters:</p><ul>
<li><p>Theta Estimated (<code>estimateTheta</code>, character)</p></li>
</ul>


<p><strong>Wang and Mendel Fuzzy Rules</strong> (<code>method = 'WM'</code>)</p>
<p>For regression using package <span class="pkg">frbs</span> with tuning parameters:</p><ul>
<li><p>Number of Fuzzy Terms (<code>num.labels</code>, numeric)</p></li>
<li><p>Membership Function (<code>type.mf</code>, character)</p></li>
</ul>


<p><strong>Weighted Subspace Random Forest</strong> (<code>method = 'wsrf'</code>)</p>
<p>For classification using package <span class="pkg">wsrf</span> with tuning parameters:</p><ul>
<li><p>Number of Randomly Selected Predictors (<code>mtry</code>, numeric)</p></li>
</ul>



    </div>



    <h2 class="hasAnchor" id="references"><a class="anchor" href="#references"></a>References</h2>

    <p>``Using your own model in <code><a href='train.html'>train</a></code>'' (<a href='https://topepo.github.io/caret/using-your-own-model-in-train.html'>https://topepo.github.io/caret/using-your-own-model-in-train.html</a>)</p>

  </div>
  <div class="col-md-3 hidden-xs hidden-sm" id="sidebar">
    <h2>Contents</h2>
    <ul class="nav nav-pills nav-stacked">
      <li><a href="#references">References</a></li>
    </ul>

  </div>
</div>


      <footer>
      <div class="copyright">
  <p>Developed by Max Kuhn.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="https://pkgdown.r-lib.org/">pkgdown</a> 1.4.1.</p>
</div>

      </footer>
   </div>

  


  </body>
</html>


